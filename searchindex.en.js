var relearn_searchindex = [
  {
    "breadcrumb": "Go Kure \u003e Architecture",
    "content": "Kure Architecture Documentation Version: 2.0.0\nDate: August 2025\nStatus: Complete\nExecutive Summary Kure is a Go library for programmatically building Kubernetes resources used by GitOps tools (Flux, cert-manager, MetalLB, External Secrets). The library emphasizes strongly-typed object construction over templating engines, providing a composable, type-safe approach to generating Kubernetes manifests.\nKey Architectural Achievements:\nDomain-Driven Design: Hierarchical cluster model with clear boundaries Interface Segregation: Split monolithic workflow interfaces into focused components Type Safety: Strong typing throughout with comprehensive validation GitOps Agnostic: Support for multiple GitOps tools through pluggable workflows Declarative Patching: JSONPath-based patching system with structure preservation The architecture supports complex Kubernetes cluster configurations while maintaining simplicity and extensibility through clean separation of concerns and well-defined interfaces.\nTable of Contents Architecture Overview Domain Model Architecture Workflow Architecture Error Handling Architecture Resource Builder Pattern Patch System Architecture Layout and Packaging Naming Conventions Developer Guidelines Performance Characteristics Security Model Testing Architecture Appendices Architecture Overview System Boundaries Kure operates within the Kubernetes ecosystem as a library for programmatic resource generation:\ngraph TB subgraph \"Kure Library\" DM[Domain Model] WF[Workflow Engines] RB[Resource Builders] PS[Patch System] LO[Layout Engine] end subgraph \"GitOps Tools\" FLUX[Flux] ARGO[ArgoCD] end subgraph \"Kubernetes\" K8S[Core Resources] CRD[Custom Resources] end USER[User Code] --\u003e DM DM --\u003e WF WF --\u003e RB RB --\u003e K8S RB --\u003e CRD WF --\u003e LO LO --\u003e FLUX LO --\u003e ARGO PS --\u003e K8S Core Components The system is organized around four primary architectural layers:\nDomain Model (pkg/stack/): Hierarchical abstractions for cluster configuration Workflow Engines (pkg/stack/workflow.go, pkg/stack/fluxcd/, pkg/stack/argocd/): GitOps-specific implementations Resource Builders (internal/): Strongly-typed Kubernetes resource factories Support Systems: Error handling, patching, layout, and I/O utilities Key Design Principles 1. Composition Over Inheritance\nDomain objects compose behavior through interfaces Workflow engines compose specialized generators Resources built through functional composition 2. Interface Segregation\nSmall, focused interfaces for specific concerns Workflow interfaces split by responsibility Clear separation between resource generation and layout 3. Immutable Constructs\nBuilder pattern creates immutable objects Patching creates new instances rather than modifying Functional approach to resource transformation 4. Type Safety\nStrong typing for all Kubernetes resources Compile-time validation of resource construction Custom error types with contextual information Domain Model Architecture Hierarchical Structure The domain model follows a four-tier hierarchy designed to mirror real-world Kubernetes cluster organization:\nCluster └── Node (Infrastructure/Applications) └── Bundle (Logical grouping) └── Application (Individual workloads) Cluster (pkg/stack/cluster.go) The root abstraction representing a complete Kubernetes cluster configuration:\ntype Cluster struct { Name string `yaml:\"name\"` Node *Node `yaml:\"node,omitempty\"` GitOps *GitOpsConfig `yaml:\"gitops,omitempty\"` } Design Decisions:\nSingle root node simplifies tree traversal GitOps configuration at cluster level for global policies Name field provides unique identification across environments Node (pkg/stack/cluster.go:47-64) Hierarchical containers for organizing related bundles:\ntype Node struct { Name string `yaml:\"name\"` ParentPath string `yaml:\"parentPath,omitempty\"` Children []*Node `yaml:\"children,omitempty\"` PackageRef *schema.GroupVersionKind `yaml:\"packageref,omitempty\"` Bundle *Bundle `yaml:\"bundle,omitempty\"` // Runtime fields (not serialized) parent *Node `yaml:\"-\"` pathMap map[string]*Node `yaml:\"-\"` } Anti-Circular Reference Design:\nParentPath string instead of direct parent pointer in serialized form Runtime parent field populated during InitializePathMap() Enables serialization while maintaining navigation efficiency Bundle (pkg/stack/bundle.go) Deployment units typically corresponding to single GitOps resources:\ntype Bundle struct { Name string `yaml:\"name\"` ParentPath string `yaml:\"parentPath,omitempty\"` DependsOn []*Bundle `yaml:\"dependsOn,omitempty\"` Applications []*Application `yaml:\"applications\"` SourceRef *SourceRef `yaml:\"sourceRef,omitempty\"` } Application (pkg/stack/application.go) Individual Kubernetes workloads or resource collections:\ntype Application struct { Name string `yaml:\"name\"` Resources []client.Object `yaml:\"resources\"` Labels map[string]string `yaml:\"labels,omitempty\"` } Hierarchy Navigation The domain model implements efficient tree traversal through a dual approach:\n1. Path-Based Navigation\nfunc (n *Node) GetPath() string { if n.ParentPath == \"\" { return n.Name } return n.ParentPath + \"/\" + n.Name } 2. Runtime Parent References\nfunc (n *Node) InitializePathMap() { pathMap := make(map[string]*Node) n.buildPathMap(pathMap, \"\") n.setPathMapRecursive(pathMap) } This design enables:\nEfficient serialization without circular references Fast runtime navigation through cached parent pointers Path-based lookups for configuration references Workflow Architecture Interface Segregation Pattern The workflow architecture implements Interface Segregation Principle by splitting monolithic interfaces into focused components:\n// pkg/stack/workflow.go type ResourceGenerator interface { GenerateFromCluster(*stack.Cluster) ([]client.Object, error) GenerateFromNode(*stack.Node) ([]client.Object, error) GenerateFromBundle(*stack.Bundle) ([]client.Object, error) } type LayoutIntegrator interface { IntegrateWithLayout(*layout.ManifestLayout, *stack.Cluster, layout.LayoutRules) error CreateLayoutWithResources(*stack.Cluster, layout.LayoutRules) (*layout.ManifestLayout, error) } type BootstrapGenerator interface { GenerateBootstrap(*stack.BootstrapConfig, *stack.Node) ([]client.Object, error) SupportedBootstrapModes() []string } type WorkflowEngine interface { ResourceGenerator LayoutIntegrator BootstrapGenerator GetName() string GetVersion() string } FluxCD Implementation The FluxCD workflow engine demonstrates the composition pattern:\n// pkg/stack/fluxcd/workflow_engine.go type WorkflowEngine struct { ResourceGen *ResourceGenerator // Pure resource generation LayoutInteg *LayoutIntegrator // Layout integration BootstrapGen *BootstrapGenerator // Bootstrap concerns } func NewWorkflowEngine() *WorkflowEngine { resourceGen := NewResourceGenerator() layoutInteg := NewLayoutIntegrator(resourceGen) bootstrapGen := NewBootstrapGenerator() return \u0026WorkflowEngine{ ResourceGen: resourceGen, LayoutInteg: layoutInteg, BootstrapGen: bootstrapGen, } } Component Responsibilities ResourceGenerator (pkg/stack/fluxcd/resource_generator.go)\nPure resource generation from domain objects Kustomization creation with proper source references Dependency management between bundles No layout or file system concerns LayoutIntegrator (pkg/stack/fluxcd/layout_integrator.go)\nIntegration with manifest layout system Directory structure generation File placement policies GitOps-specific layout requirements BootstrapGenerator (pkg/stack/fluxcd/bootstrap_generator.go)\nBootstrap resource generation GitOps system initialization Mode-specific configurations (gitops-toolkit vs flux-operator) Extensibility Pattern Adding new GitOps workflows follows a clear pattern:\nImplement Core Interfaces: ResourceGenerator, LayoutIntegrator, BootstrapGenerator Compose WorkflowEngine: Combine specialized generators Register with Layout: Add layout rules for the new workflow Provide Public API: Create user-facing convenience functions Error Handling Architecture KureError System Kure implements a sophisticated error handling system based on typed errors with contextual information:\n// pkg/errors/errors.go type KureError interface { error Type() ErrorType Suggestion() string Context() map[string]interface{} } type ErrorType string const ( ErrorTypeValidation ErrorType = \"validation\" ErrorTypeResource ErrorType = \"resource\" ErrorTypePatch ErrorType = \"patch\" ErrorTypeParse ErrorType = \"parse\" ErrorTypeFile ErrorType = \"file\" ErrorTypeConfiguration ErrorType = \"configuration\" ErrorTypeInternal ErrorType = \"internal\" ) Error Type Architecture ValidationError (pkg/errors/errors.go:155-185)\nField-level validation failures Provides valid value suggestions Component context for debugging ResourceError (pkg/errors/errors.go:188-250)\nResource-specific errors (not found, validation failed) Includes resource type, name, and namespace Lists available alternatives when applicable PatchError (pkg/errors/errors.go:253-294)\nPatch operation failures Path and operation context Graceful degradation suggestions ParseError (pkg/errors/errors.go:297-340)\nFile parsing errors with location information Line and column numbers Format-specific help suggestions Centralized Validation The validation system provides consistent error reporting across all resource builders:\n// internal/validation/validators.go type Validator struct{} func (v *Validator) ValidateDeployment(deployment *appsv1.Deployment) error { return v.validateNotNil(deployment, errors.ErrNilDeployment) } // Pre-defined error instances for common cases var ( ErrNilDeployment = ResourceValidationError(\"Deployment\", \"\", \"deployment\", \"deployment cannot be nil\", nil) ErrNilPod = ResourceValidationError(\"Pod\", \"\", \"pod\", \"pod cannot be nil\", nil) // ... more predefined errors ) Error Wrapping Strategy Kure follows Go’s error wrapping conventions while adding structured context:\nfunc (we *WorkflowEngine) GenerateFromCluster(c *stack.Cluster) ([]client.Object, error) { if c == nil { return nil, errors.ResourceValidationError(\"Cluster\", \"\", \"cluster\", \"cluster cannot be nil\", nil) } resources, err := we.ResourceGen.GenerateFromCluster(c) if err != nil { return nil, errors.Wrapf(err, \"failed to generate resources for cluster %s\", c.Name) } return resources, nil } Resource Builder Pattern Builder Architecture Resource builders follow a consistent functional pattern across all Kubernetes resource types:\n// Pattern: Create* functions for constructors func CreateDeployment(name, namespace string) *appsv1.Deployment // Pattern: Add* functions for collection modifications func AddDeploymentContainer(deployment *appsv1.Deployment, container *corev1.Container) error // Pattern: Set* functions for field assignments func SetDeploymentReplicas(deployment *appsv1.Deployment, replicas int32) error Implementation Structure Each resource builder package (internal/kubernetes/, internal/fluxcd/, etc.) follows consistent organization:\ninternal/kubernetes/ ├── doc.go # Package documentation ├── deployment.go # Deployment builders ├── deployment_test.go # Deployment tests ├── service.go # Service builders ├── service_test.go # Service tests └── ... Type Safety Guarantees Builders provide compile-time type safety through:\nStrong Return Types: All constructors return specific Kubernetes types Validation Integration: Automatic validation in constructor functions Error Propagation: Explicit error returns for validation failures Example implementation:\n// internal/kubernetes/deployment.go func CreateDeployment(name, namespace string) *appsv1.Deployment { return \u0026appsv1.Deployment{ ObjectMeta: metav1.ObjectMeta{ Name: name, Namespace: namespace, }, Spec: appsv1.DeploymentSpec{ Selector: \u0026metav1.LabelSelector{ MatchLabels: map[string]string{ \"app\": name, }, }, Template: corev1.PodTemplateSpec{ ObjectMeta: metav1.ObjectMeta{ Labels: map[string]string{ \"app\": name, }, }, Spec: corev1.PodSpec{ Containers: []corev1.Container{}, }, }, }, } } func AddDeploymentContainer(deployment *appsv1.Deployment, container *corev1.Container) error { validator := validation.NewValidator() if err := validator.ValidateDeployment(deployment); err != nil { return err } if err := validator.ValidateContainer(container); err != nil { return err } deployment.Spec.Template.Spec.Containers = append( deployment.Spec.Template.Spec.Containers, *container) return nil } Cross-Resource Consistency All builders maintain consistency through:\nCommon Validation: Centralized validator used across all builders Standard Error Types: Consistent error reporting patterns Naming Conventions: Uniform function naming across resource types Patch System Architecture Design Philosophy The patch system implements declarative, JSONPath-based patching with structure preservation:\nOriginal YAML + Patch Declarations → Modified YAML (preserving comments/formatting) Patch File Format Patches use a TOML-inspired format (.kpatch files) that’s optimized for Kubernetes resources:\n# examples/patches/resources.kpatch [deployment.app] replicas: 3 [deployment.app.containers.name=main] image.repository: ghcr.io/example/app image.tag: \"${values.version}\" resources.requests.cpu: 250m [service.app.ports.name=http] port: 80 Path Resolution Engine The patch engine implements sophisticated path resolution:\n// pkg/patch/apply.go type PatchEngine struct { preserveStructure bool variables map[string]interface{} } func (pe *PatchEngine) Apply(yamlContent []byte, patchContent []byte) ([]byte, error) { // 1. Parse YAML with structure preservation // 2. Parse patch declarations // 3. Resolve JSONPaths with type inference // 4. Apply modifications preserving formatting // 5. Return modified YAML } List Selector System Advanced list manipulation through selector syntax:\nSelector Type Example Operation By index spec.containers.0 Replace at index 0 By key-value spec.containers.name=main Replace item with name=main Insert before spec.containers.-3 Insert before index 3 Insert after spec.containers.+2 Insert after index 2 Append to list spec.containers.- Append to end Variable Substitution The patch system supports typed variable substitution:\n[deployment.app] enabled: ${features.web_enabled} # Boolean feature flags replicas: ${values.replica_count} # Numeric values [service.app] hostname: \"${values.name}.${values.domain}\" # String interpolation Type Inference Patches automatically infer Kubernetes field types:\nResource field types from OpenAPI schema List element types from existing content Scalar types from variable context Layout and Packaging Layout Architecture The layout system manages directory structure and manifest organization:\n// pkg/stack/layout/types.go type ManifestLayout struct { Root string // Repository root path Clusters map[string]*ClusterLayout // Per-cluster layouts Global *GlobalLayout // Shared resources } type LayoutRules struct { BundleGrouping GroupingStrategy // How to group bundles ApplicationGrouping GroupingStrategy // How to group applications KustomizationMode KustomizationMode // Kustomization generation } Grouping Strategies GroupFlat: Each item gets its own directory\nclusters/prod/ ├── bundles/ │ ├── monitoring/ │ ├── logging/ │ └── ingress/ └── apps/ ├── frontend/ ├── backend/ └── database/ GroupByParent: Items grouped under parent directories\nclusters/prod/ ├── infrastructure/ │ ├── monitoring/ │ ├── logging/ │ └── ingress/ └── applications/ ├── frontend/ ├── backend/ └── database/ GitOps Integration Layout integrates with GitOps tools through specialized placement:\nFlux Placement (pkg/stack/layout/config.go)\nKustomization resources placed in flux-system namespace Source references use relative paths (./clusters/prod/...) Automatic kustomization.yaml generation ArgoCD Placement\nApplication resources in argocd namespace Source paths without ./ prefix Manual kustomization.yaml required Directory Structure Generation // pkg/stack/layout/walker.go func WalkCluster(cluster *stack.Cluster, rules LayoutRules) (*ManifestLayout, error) { layout := \u0026ManifestLayout{ Root: \".\", Clusters: make(map[string]*ClusterLayout), } clusterLayout := \u0026ClusterLayout{ Name: cluster.Name, Path: filepath.Join(\"clusters\", cluster.Name), } // Walk node hierarchy if err := walkNode(cluster.Node, clusterLayout, rules); err != nil { return nil, err } layout.Clusters[cluster.Name] = clusterLayout return layout, nil } Naming Conventions Function Naming Standards Kure follows strict naming conventions based on function purpose:\nConstructor Functions // Go type constructors use New* prefix func NewCluster(name string, tree *Node) *Cluster func NewBundle(name string, resources []*Application, labels map[string]string) (*Bundle, error) // Kubernetes resource factories use descriptive names func CreateDeployment(name, namespace string) *appsv1.Deployment func CreateService(name, namespace string) *corev1.Service Helper Functions // Adders for collection modifications func AddDeploymentContainer(deployment *appsv1.Deployment, container *corev1.Container) error func AddServicePort(service *corev1.Service, port corev1.ServicePort) error // Setters for field assignments func SetDeploymentReplicas(deployment *appsv1.Deployment, replicas int32) error func SetServiceType(service *corev1.Service, serviceType corev1.ServiceType) error Workflow Functions // Engine constructors follow New* pattern func NewWorkflowEngine() *WorkflowEngine func NewResourceGenerator() *ResourceGenerator // Public APIs use descriptive names func Engine() *WorkflowEngine // Default engine func EngineWithMode(mode layout.KustomizationMode) *WorkflowEngine // Configured engine Package Organization Standards pkg/ # Public APIs and interfaces ├── stack/ # Domain model (public) │ ├── fluxcd/ # FluxCD workflow implementation │ ├── argocd/ # ArgoCD workflow implementation │ └── layout/ # Layout generation utilities ├── stack/workflow.go # Workflow interfaces (public) ├── errors/ # Error handling utilities (public) └── patch/ # Patch system (public) internal/ # Implementation packages (private) ├── kubernetes/ # Core Kubernetes builders ├── fluxcd/ # Flux resource builders ├── certmanager/ # cert-manager builders ├── metallb/ # MetalLB builders ├── externalsecrets/ # External Secrets builders └── validation/ # Centralized validation File Naming Patterns Implementation files: {resource_type}.go (e.g., deployment.go, service.go) Test files: {resource_type}_test.go (e.g., deployment_test.go) Documentation: doc.go for package documentation Design documents: DESIGN.md, README.md in relevant packages Developer Guidelines Adding New Resource Builders Follow this standardized process for adding Kubernetes resource support:\n1. Create Constructor Function // internal/kubernetes/newresource.go func CreateNewResource(name, namespace string, opts ...Option) *v1.NewResource { resource := \u0026v1.NewResource{ ObjectMeta: metav1.ObjectMeta{ Name: name, Namespace: namespace, }, Spec: v1.NewResourceSpec{ // Initialize required fields }, } // Apply options for _, opt := range opts { opt(resource) } return resource } 2. Add Helper Functions func AddNewResourceField(resource *v1.NewResource, field FieldType) error { validator := validation.NewValidator() if err := validator.ValidateNewResource(resource); err != nil { return err } if err := validator.ValidateField(field); err != nil { return err } // Add field to resource resource.Spec.Fields = append(resource.Spec.Fields, field) return nil } func SetNewResourceProperty(resource *v1.NewResource, value PropertyType) error { // Validation and assignment } 3. Add Validation Support // internal/validation/validators.go func (v *Validator) ValidateNewResource(resource *v1.NewResource) error { return v.validateNotNil(resource, errors.ErrNilNewResource) } // pkg/errors/errors.go - Add to predefined errors var ErrNilNewResource = ResourceValidationError(\"NewResource\", \"\", \"newresource\", \"new resource cannot be nil\", nil) 4. Comprehensive Testing // internal/kubernetes/newresource_test.go func TestCreateNewResource(t *testing.T) { resource := CreateNewResource(\"test\", \"default\") if resource == nil { t.Fatal(\"expected non-nil resource\") } // Validate required fields if resource.Name != \"test\" { t.Errorf(\"expected name 'test', got %s\", resource.Name) } if resource.Namespace != \"default\" { t.Errorf(\"expected namespace 'default', got %s\", resource.Namespace) } } func TestNewResourceHelpers(t *testing.T) { resource := CreateNewResource(\"test\", \"default\") // Test all helper functions field := FieldType{/* valid field */} if err := AddNewResourceField(resource, field); err != nil { t.Errorf(\"unexpected error: %v\", err) } // Validate field was added if len(resource.Spec.Fields) != 1 { t.Errorf(\"expected 1 field, got %d\", len(resource.Spec.Fields)) } } Extending Domain Model When extending the core domain model:\n1. Maintain Hierarchy Consistency // Add new domain types following existing patterns type NewDomainType struct { Name string `yaml:\"name\"` ParentPath string `yaml:\"parentPath,omitempty\"` // Domain-specific fields // Runtime navigation (not serialized) parent *ParentType `yaml:\"-\"` pathMap map[string]*NewType `yaml:\"-\"` } 2. Implement Navigation Methods func (n *NewDomainType) SetParent(parent *ParentType) { n.parent = parent if parent == nil { n.ParentPath = \"\" } else { n.ParentPath = parent.GetPath() } } func (n *NewDomainType) GetPath() string { if n.ParentPath == \"\" { return n.Name } return n.ParentPath + \"/\" + n.Name } 3. Update Workflow Implementations Ensure all workflow engines handle the new domain type appropriately.\nImplementing New GitOps Workflows To add support for new GitOps tools:\n1. Implement Core Interfaces // pkg/stack/newtool/resource_generator.go type ResourceGenerator struct { // Tool-specific configuration } func (rg *ResourceGenerator) GenerateFromCluster(c *stack.Cluster) ([]client.Object, error) { // Tool-specific resource generation } // Implement other ResourceGenerator methods 2. Create Layout Integration // pkg/stack/newtool/layout_integrator.go type LayoutIntegrator struct { ResourceGen *ResourceGenerator // Tool-specific layout configuration } func (li *LayoutIntegrator) IntegrateWithLayout(ml *layout.ManifestLayout, c *stack.Cluster, rules layout.LayoutRules) error { // Tool-specific layout integration } 3. Compose Workflow Engine // pkg/stack/newtool/workflow_engine.go type WorkflowEngine struct { ResourceGen *ResourceGenerator LayoutInteg *LayoutIntegrator BootstrapGen *BootstrapGenerator } func NewWorkflowEngine() *WorkflowEngine { // Compose components } // Implement workflow.WorkflowEngine interface 4. Add Public API // pkg/stack/newtool/newtool.go func Engine() *WorkflowEngine { return NewWorkflowEngine() } Testing Patterns Kure maintains comprehensive test coverage through consistent patterns:\nUnit Testing func TestResourceCreation(t *testing.T) { // Test constructor resource := CreateResource(\"test\", \"default\") // Validate required fields // Test error conditions // Verify helper functions } func TestResourceValidation(t *testing.T) { // Test validation logic // Test error cases // Verify error messages } Integration Testing func TestWorkflowGeneration(t *testing.T) { // Create domain model cluster := stack.NewCluster(\"test\", rootNode) // Generate with workflow engine := fluxcd.Engine() resources, err := engine.GenerateFromCluster(cluster) // Validate generated resources // Test layout integration } Error Testing func TestErrorHandling(t *testing.T) { // Test nil inputs err := AddResourceField(nil, field) if !errors.IsType(err, errors.ErrorTypeValidation) { t.Errorf(\"expected validation error, got %T\", err) } // Test error context kureErr := errors.GetKureError(err) if kureErr == nil { t.Error(\"expected KureError\") } } Performance Characteristics Resource Generation Performance Kure is optimized for batch resource generation rather than individual operations:\nBenchmarks (typical 100-node cluster):\nDomain model creation: ~1ms Resource generation: ~10ms Layout generation: ~5ms YAML serialization: ~15ms Memory Usage:\nDomain model: ~100KB per 100 resources Generated resources: ~1MB per 1000 resources Layout structures: ~50KB per cluster Optimization Strategies 1. Lazy Initialization func (n *Node) InitializePathMap() { // Only build path map when needed if n.pathMap == nil { pathMap := make(map[string]*Node) n.buildPathMap(pathMap, \"\") n.setPathMapRecursive(pathMap) } } 2. Resource Pooling // Reuse validation instances var validatorPool = sync.Pool{ New: func() interface{} { return validation.NewValidator() }, } 3. Batch Operations func (we *WorkflowEngine) GenerateFromCluster(c *stack.Cluster) ([]client.Object, error) { // Generate all resources in single pass // Minimize allocation overhead // Batch validation operations } Bottlenecks and Mitigations Known Bottlenecks:\nYAML serialization (mitigated by streaming output) Path resolution in complex hierarchies (mitigated by path caching) Validation overhead (mitigated by batch validation) Scaling Characteristics:\nLinear scaling with number of resources Logarithmic scaling with hierarchy depth Constant memory overhead per resource type Security Model Secret Management Kure follows Kubernetes security best practices for secret handling:\n1. No Hardcoded Secrets // NEVER do this func CreateSecretWithData(name, namespace, password string) *corev1.Secret { return \u0026corev1.Secret{ Data: map[string][]byte{ \"password\": []byte(password), // WRONG: hardcoded secret }, } } // CORRECT approach - reference existing secrets func CreateCertificateWithSecret(name, namespace string, secretRef cmmeta.SecretKeySelector) *cmv1.Certificate { return \u0026cmv1.Certificate{ Spec: cmv1.CertificateSpec{ SecretName: secretRef.Name, // Reference, don't embed }, } } 2. Secret Reference Pattern // Standard pattern for secret references key := cmmeta.SecretKeySelector{ LocalObjectReference: cmmeta.LocalObjectReference{Name: \"secret-name\"}, Key: \"key-name\", } // Use in resource builders cert := certmanager.CreateCertificate(\"tls-cert\", \"default\") certmanager.SetCertificateIssuerSecret(cert, key) RBAC Integration Resource builders provide granular RBAC control:\n// Create minimal privilege roles role := kubernetes.CreateRole(\"app-reader\", \"default\") kubernetes.AddRoleRule(role, rbacv1.PolicyRule{ APIGroups: []string{\"\"}, Resources: []string{\"pods\"}, Verbs: []string{\"get\", \"list\"}, }) // Bind to specific accounts binding := kubernetes.CreateRoleBinding(\"app-reader\", \"default\") kubernetes.SetRoleBindingRole(binding, \"app-reader\") kubernetes.AddRoleBindingSubject(binding, rbacv1.Subject{ Kind: \"ServiceAccount\", Name: \"app-sa\", }) Certificate Management cert-manager integration provides secure TLS:\n// ACME challenge configuration issuer := certmanager.CreateClusterIssuer(\"letsencrypt\") certmanager.SetClusterIssuerACME(issuer, \"https://acme-v02.api.letsencrypt.org/directory\") certmanager.AddClusterIssuerACMEDNS01Provider(issuer, \"cloudflare\", map[string]string{ \"email\": \"admin@example.com\", }) // Certificate with DNS validation cert := certmanager.CreateCertificate(\"api-tls\", \"default\") certmanager.SetCertificateIssuer(cert, cmmeta.ObjectReference{ Name: \"letsencrypt\", Kind: \"ClusterIssuer\", }) certmanager.AddCertificateDNSName(cert, \"api.example.com\") Input Validation All user inputs undergo strict validation:\nfunc CreateResource(name, namespace string) (*Resource, error) { // Validate Kubernetes naming conventions if !isValidKubernetesName(name) { return nil, errors.NewValidationError(\"name\", name, \"Resource\", []string{\"lowercase\", \"alphanumeric\", \"hyphens-only\"}) } // Validate namespace format if namespace != \"\" \u0026\u0026 !isValidNamespace(namespace) { return nil, errors.NewValidationError(\"namespace\", namespace, \"Resource\", []string{\"valid-namespace-name\"}) } return \u0026Resource{Name: name, Namespace: namespace}, nil } Testing Architecture Test Organization Kure maintains 105 test files with comprehensive coverage:\ninternal/ ├── kubernetes/ │ ├── deployment_test.go │ ├── service_test.go │ └── ... ├── fluxcd/ │ ├── kustomize_test.go │ ├── source_test.go │ └── ... └── ... pkg/ ├── stack/ │ ├── application_test.go │ ├── bundle_test.go │ └── ... ├── patch/ │ ├── apply_test.go │ ├── set_test.go │ └── ... └── ... Testing Patterns Constructor Testing func TestCreateDeployment(t *testing.T) { deployment := CreateDeployment(\"test-app\", \"default\") // Validate non-nil result if deployment == nil { t.Fatal(\"expected non-nil deployment\") } // Validate required fields if deployment.Name != \"test-app\" { t.Errorf(\"expected name 'test-app', got %s\", deployment.Name) } if deployment.Namespace != \"default\" { t.Errorf(\"expected namespace 'default', got %s\", deployment.Namespace) } // Validate default values if deployment.Spec.Replicas == nil || *deployment.Spec.Replicas != 1 { t.Error(\"expected default replicas to be 1\") } } Helper Function Testing func TestAddDeploymentContainer(t *testing.T) { deployment := CreateDeployment(\"test-app\", \"default\") container := \u0026corev1.Container{ Name: \"main\", Image: \"nginx:latest\", } // Test successful addition err := AddDeploymentContainer(deployment, container) if err != nil { t.Fatalf(\"unexpected error: %v\", err) } // Validate container was added if len(deployment.Spec.Template.Spec.Containers) != 1 { t.Errorf(\"expected 1 container, got %d\", len(deployment.Spec.Template.Spec.Containers)) } // Test error conditions err = AddDeploymentContainer(nil, container) if err == nil { t.Error(\"expected error for nil deployment\") } err = AddDeploymentContainer(deployment, nil) if err == nil { t.Error(\"expected error for nil container\") } } Workflow Testing func TestFluxWorkflowGeneration(t *testing.T) { // Create test cluster app := \u0026stack.Application{ Name: \"test-app\", Resources: []client.Object{ kubernetes.CreateDeployment(\"app\", \"default\"), }, } bundle := \u0026stack.Bundle{ Name: \"test-bundle\", Applications: []*stack.Application{app}, } node := \u0026stack.Node{ Name: \"test-node\", Bundle: bundle, } cluster := stack.NewCluster(\"test-cluster\", node) // Test resource generation engine := fluxcd.Engine() resources, err := engine.GenerateFromCluster(cluster) if err != nil { t.Fatalf(\"unexpected error: %v\", err) } if len(resources) == 0 { t.Error(\"expected generated resources\") } // Validate resource types hasKustomization := false for _, resource := range resources { if resource.GetObjectKind().GroupVersionKind().Kind == \"Kustomization\" { hasKustomization = true break } } if !hasKustomization { t.Error(\"expected Kustomization resource\") } } Error Testing func TestValidationErrors(t *testing.T) { // Test validation error structure err := validation.NewValidator().ValidateDeployment(nil) if err == nil { t.Fatal(\"expected validation error\") } // Test KureError interface kureErr := errors.GetKureError(err) if kureErr == nil { t.Fatal(\"expected KureError\") } // Validate error properties if kureErr.Type() != errors.ErrorTypeValidation { t.Errorf(\"expected validation error type, got %s\", kureErr.Type()) } suggestion := kureErr.Suggestion() if suggestion == \"\" { t.Error(\"expected non-empty suggestion\") } context := kureErr.Context() if context == nil { t.Error(\"expected error context\") } } Test Utilities Common test utilities for consistent testing:\n// Test helper functions func createTestCluster(name string) *stack.Cluster { // Standard test cluster creation } func validateResource(t *testing.T, resource client.Object, expectedKind string) { // Standard resource validation } func assertNoError(t *testing.T, err error) { if err != nil { t.Fatalf(\"unexpected error: %v\", err) } } func assertError(t *testing.T, err error, expectedType errors.ErrorType) { if err == nil { t.Fatal(\"expected error\") } if !errors.IsType(err, expectedType) { t.Errorf(\"expected error type %s, got %T\", expectedType, err) } } Appendices Appendix A: Glossary Application: Individual Kubernetes workload or resource collection within a Bundle.\nBundle: Deployment unit typically corresponding to a single GitOps resource (e.g., Flux Kustomization).\nCluster: Root abstraction representing a complete Kubernetes cluster configuration.\nDomain Model: The hierarchical structure (Cluster → Node → Bundle → Application) representing cluster organization.\nGitOps Engine: Implementation of GitOps-specific resource generation and layout integration.\nKureError: Structured error type providing contextual information and suggestions.\nLayout: Directory structure and manifest organization for GitOps repositories.\nNode: Hierarchical container for organizing related Bundles (e.g., infrastructure vs applications).\nPatch: Declarative modification of Kubernetes resources using JSONPath-based operations.\nResource Builder: Strongly-typed factory function for creating Kubernetes resources.\nWorkflow Engine: Complete GitOps workflow implementation combining resource generation, layout integration, and bootstrap capabilities.\nAppendix B: References Kubernetes API Reference Flux Documentation ArgoCD Documentation cert-manager Documentation MetalLB Documentation External Secrets Operator Appendix C: Design Documents Additional design documentation available in the repository:\npkg/patch/DESIGN.md: Detailed patch system specification pkg/patch/PATCH_ENGINE_DESIGN.md: Patch engine implementation details pkg/patch/PATH_RESOLUTION.md: JSONPath resolution algorithms pkg/stack/layout/README.md: Layout system overview pkg/stack/workflow.go: Workflow interface definitions Appendix D: Migration Guide For migrating from previous versions:\nV1 to V2 Migration Domain Model Changes:\nNode hierarchy now uses ParentPath strings instead of direct parent pointers Call InitializePathMap() on root nodes after construction Bundle hierarchy follows same pattern Workflow Interface Changes:\nSplit monolithic workflow interfaces into specialized components Update implementations to use ResourceGenerator, LayoutIntegrator, BootstrapGenerator Compose WorkflowEngine from specialized generators Error Handling Changes:\nReplace generic errors with typed KureError instances Use centralized validation from internal/validation package Handle error context and suggestions in error reporting Function Naming Changes:\nConstructor functions now follow New* vs Create* patterns consistently Update imports to use new package structure Helper function signatures remain compatible This comprehensive architecture serves as the foundation for Kure’s continued evolution while maintaining backward compatibility and extensibility.",
    "description": "Kure Architecture Documentation Version: 2.0.0\nDate: August 2025\nStatus: Complete\nExecutive Summary Kure is a Go library for programmatically building Kubernetes resources used by GitOps tools (Flux, cert-manager, MetalLB, External Secrets). The library emphasizes strongly-typed object construction over templating engines, providing a composable, type-safe approach to generating Kubernetes manifests.\nKey Architectural Achievements:\nDomain-Driven Design: Hierarchical cluster model with clear boundaries Interface Segregation: Split monolithic workflow interfaces into focused components Type Safety: Strong typing throughout with comprehensive validation GitOps Agnostic: Support for multiple GitOps tools through pluggable workflows Declarative Patching: JSONPath-based patching system with structure preservation The architecture supports complex Kubernetes cluster configurations while maintaining simplicity and extensibility through clean separation of concerns and well-defined interfaces.",
    "tags": [],
    "title": "Architecture Details",
    "uri": "/architecture/details/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Reference",
    "content": "Kure Compatibility Matrix This document describes the versions of infrastructure tools that Kure supports.\nVersion Philosophy Kure maintains two version concepts for each dependency:\nBuild Version (current in versions.yaml): The exact library version Kure imports in go.mod Deployment Compatibility (supported_range): The range of deployed tool versions that Kure can generate YAML for Go Version Current: Go 1.24.12\nInfrastructure Dependencies Tool Build Version Deployment Compatibility Notes cert-manager 1.16.5 1.14 - 1.16 1.17+ requires Go 1.25 fluxcd 2.6.4 2.4 - 2.6 2.7+ requires Go 1.25, tracked in #128 image-automation-controller 1.0+ requires Go 1.25 (#171) All github.com/fluxcd/* packages blocked from minor/major updates flux-operator 0.24.1 0.23 - 0.24 0.25+ requires Go 1.25 metallb 0.15.2 0.14 - 0.15 0.15.3+ requires Go 1.25 and triggers k8s.io upgrade to 0.34+ (#169) external-secrets 0.19.2 0.18 - 0.19 Compatible with current Go version controller-runtime 0.21.0 0.19 - 0.21 0.22+ requires Go 1.25 kubernetes 0.33.2 1.28 - 1.33 Tested in CI matrix Understanding the Matrix Build Version (go.mod) The version Kure imports and builds against. This is validated by CI to match versions.yaml.\nDeployment Compatibility The range of versions that Kure can generate valid YAML for. Kure may generate YAML compatible with older or newer versions than it builds against.\nFor example:\nKure builds against cert-manager 1.16.2 But generates YAML compatible with cert-manager 1.14.x, 1.15.x, and 1.16.x Upgrading Dependencies When upgrading a dependency:\nUpdate versions.yaml with new current and supported_range Run go get \u003cmodule\u003e@\u003cversion\u003e to update go.mod Update code for any API changes Run ./scripts/sync-versions.sh generate to update docs Run ./scripts/sync-versions.sh check to validate consistency Related Issues #133 - Go 1.25 upgrade tracking #128 - FluxCD ecosystem upgrade (blocked by Go 1.25)",
    "description": "Kure Compatibility Matrix This document describes the versions of infrastructure tools that Kure supports.\nVersion Philosophy Kure maintains two version concepts for each dependency:\nBuild Version (current in versions.yaml): The exact library version Kure imports in go.mod Deployment Compatibility (supported_range): The range of deployed tool versions that Kure can generate YAML for Go Version Current: Go 1.24.12\nInfrastructure Dependencies Tool Build Version Deployment Compatibility Notes cert-manager 1.16.5 1.14 - 1.16 1.17+ requires Go 1.25 fluxcd 2.6.4 2.4 - 2.6 2.7+ requires Go 1.25, tracked in #128 image-automation-controller 1.0+ requires Go 1.25 (#171) All github.com/fluxcd/* packages blocked from minor/major updates flux-operator 0.24.1 0.23 - 0.24 0.25+ requires Go 1.25 metallb 0.15.2 0.14 - 0.15 0.15.3+ requires Go 1.25 and triggers k8s.io upgrade to 0.34+ (#169) external-secrets 0.19.2 0.18 - 0.19 Compatible with current Go version controller-runtime 0.21.0 0.19 - 0.21 0.22+ requires Go 1.25 kubernetes 0.33.2 1.28 - 1.33 Tested in CI matrix Understanding the Matrix Build Version (go.mod) The version Kure imports and builds against. This is validated by CI to match versions.yaml.",
    "tags": [],
    "title": "Compatibility Matrix",
    "uri": "/reference/compatibility/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Packages \u003e Patch",
    "content": "Kure Patch File Format — Specification This document describes the complete structure and semantics of .kpatch files used in Kure to define Kubernetes resource overrides.\nKure patches are:\nFlat, line-based TOML-inspired, but not valid TOML Declarative (no conditionals or logic) Scoped to specific resource kinds and instances 1. File Extension All patch files must use the .kpatch extension. These are plain text files with Kure’s custom patch format.\nExamples:\ndeployment.app.kpatch service.backend.kpatch config.kpatch (merged aggregate) 2. Patch Header Syntax Each patch file is divided into sections. Each section is introduced by a header:\n[kind.name] [kind.name.section] [kind.name.section.key=value] [kind.name.section.index] 2.1 Header Grammar [kind.name[.section[.subsection[.selector or .index]]]] Simplified Selector Rule List selectors using key=value pairs can omit brackets unless the key or value contains special characters (e.g., ., =, [, ], +, -).\nPreferred syntax:\n[deployment.app.containers.name=main] Bracketed fallback (required when ambiguous):\n[deployment.app.containers[image.name=main]] 2.2 Examples [deployment.app] # Top-level fields [deployment.app.containers] # Applies to all containers [deployment.app.containers.name=main] # Replace item with name == \"main\" [deployment.app.ports.0] # First port entry 3. Patch Keys Within each header block, individual settings are expressed as dotpaths, referencing fields to override.\nSyntax:\nkey.subkey.subsubkey: value 3.1 Values Must be scalar (string, int, float, boolean) Strings may be quoted or unquoted (if YAML-compliant) Variables are allowed (see below) 3.2 Examples replicas: 3 image.repository: ghcr.io/example/myapp resources.limits.cpu: 500m host: \"${values.domain}\" 4. Variables Kure patch files support scalar substitution using instance-level variables.\nSyntax ${features.myflag} ${values.domain} Scope features.*: booleans provided programmatically values.*: strings or numbers provided programmatically Variables must resolve to scalars. No objects or arrays allowed.\nExample [deployment.app] enabled: ${features.web_enabled} replicas: 2 [service.app] hostname: \"${values.name}.${values.domain}\" 5. Lists and Selectors Kure supports patching into Kubernetes lists like containers, env, ports, volumes, volumeMounts, etc.\n5.1 List Selector Syntax List selectors allow addressing or inserting elements within Kubernetes lists.\nSelector Type Example Meaning By index spec.containers[0] / spec.containers.0 Replace at index 0 By key-value spec.containers[name=web] / ...name=web Replace item with name=web Insert before index spec.containers[-3] Insert before index 3 Insert before match spec.containers[-name=sidecar] Insert before item matching name=sidecar Insert after index spec.containers[+2] Insert after index 2 Insert after match spec.containers[+name=main] Insert after item matching name=main Append to list spec.containers[-] Append item to end of list Note: You may omit brackets around key=value unless the key or value contains special characters (e.g. ., [, ]).\n6. Implementation Status Supported Features Dual format support (YAML and TOML) with automatic detection Advanced structure preservation maintaining comments and formatting Intelligent path resolution with disambiguation Variable substitution with ${values.key} and ${features.flag} syntax Automatic type inference for Kubernetes compatibility Comprehensive debug logging with KURE_DEBUG=1 Graceful error handling with warnings for missing targets Current Limitations No logic, conditionals, or templating expressions No map merging — field values are completely replaced Only scalar values supported (arrays/objects not allowed in patch values) ✅ Pure index-based insertion ([-3], [+2]) now implemented Variable context must be provided programmatically No OpenAPI schema validation (planned for future implementation) Future Enhancements OpenAPI schema validation for patch target verification 7. Purpose Kure patches are designed to:\nOverride Kubernetes manifests without templates Enable reusable, modular package definitions Support clean schema validation via OpenAPI Allow editing via CLI and JSONSchema-aware UIs 8. Example [deployment.app] replicas: 3 [deployment.app.containers.name=main] image.repository: ghcr.io/example/app image.tag: \"${values.version}\" resources.requests.cpu: 250m [service.app.ports.name=http] port: 80 [ingress.web.tls.0] hosts.0: \"${values.name}.${values.domain}\" This file:\nUpdates the replica count Modifies the main container image and CPU request Sets the service port Configures the first TLS entry of the ingress This format is the foundation for declarative, schema-validated Kubernetes customization in Kure.",
    "description": "Kure Patch File Format — Specification This document describes the complete structure and semantics of .kpatch files used in Kure to define Kubernetes resource overrides.\nKure patches are:\nFlat, line-based TOML-inspired, but not valid TOML Declarative (no conditionals or logic) Scoped to specific resource kinds and instances 1. File Extension All patch files must use the .kpatch extension. These are plain text files with Kure’s custom patch format.\nExamples:\ndeployment.app.kpatch service.backend.kpatch config.kpatch (merged aggregate) 2. Patch Header Syntax Each patch file is divided into sections. Each section is introduced by a header:",
    "tags": [],
    "title": "Design",
    "uri": "/packages/patch/design/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Packages \u003e Stack",
    "content": "Stack Module Design Document Last Updated: 2025-08-08\nStatus: Implemented - Phase 2 Complete\nOverview The Stack module provides the core domain model for Kure’s hierarchical configuration system. It defines a tree structure (Cluster → Node → Bundle → Application) that represents complete Kubernetes deployments organized for GitOps workflows.\nCurrent Architecture Hierarchy Structure The stack follows a four-level hierarchy:\nCluster # Top-level configuration └── Node # Hierarchical packaging unit └── Bundle # Deployment unit (Flux Kustomization) └── Application # Single deployable app Core Components 1. Cluster (pkg/stack/cluster.go) Purpose: Represents complete cluster configuration Contains: Root Node and GitOps bootstrap configuration Current Structure: type Cluster struct { Name string `yaml:\"name\"` Node *Node `yaml:\"node,omitempty\"` GitOps *GitOpsConfig `yaml:\"gitops,omitempty\"` } 2. Node (pkg/stack/cluster.go) Purpose: Hierarchical tree structure for packaging Contains: Bundles and child Nodes Features: Package references for OCI/Git artifacts Hierarchical path management Dependency relationships 3. Bundle (pkg/stack/bundle.go) Purpose: Unit of deployment (maps to Flux Kustomization) Contains: Multiple Applications Features: Flux reconciliation settings (interval, timeout, retryInterval, source) Interval validation (1s to 24h, Go duration format) Dependency management Label propagation 4. Application (pkg/stack/application.go) Purpose: Single deployable application Contains: ApplicationConfig interface Current GVK Support: ✅ Already implemented ApplicationConfig System The ApplicationConfig interface provides pluggable resource generation:\ntype ApplicationConfig interface { Generate(*Application) ([]*client.Object, error) } Current Generators (with GVK support):\ngenerators.gokure.dev/v1alpha1/AppWorkload - Standard Kubernetes workloads generators.gokure.dev/v1alpha1/FluxHelm - Flux HelmRelease resources Planned GVK Integration Motivation Currently, only the ApplicationConfig layer has GVK versioning. The upper layers (Cluster, Node, Bundle) are unversioned structs, which limits:\nAPI Evolution: No clear versioning for schema changes Multiple Formats: Different stack representation approaches Tooling Integration: Different tools may prefer different formats Schema Validation: No OpenAPI schema generation capability Proposed GVK Design API Group Structure Domain: stack.gokure.dev\n# Cluster Configuration apiVersion: stack.gokure.dev/v1alpha1 kind: Cluster metadata: name: production-cluster spec: gitops: type: flux bootstrap: enabled: true fluxVersion: v2.2.0 node: name: root # ... node specification --- # Node Configuration (can be separate artifact) apiVersion: stack.gokure.dev/v1alpha1 kind: Node metadata: name: infrastructure namespace: flux-system spec: packageRef: name: infra-packages version: v1.0.0 bundles: - name: cert-manager - name: ingress-nginx children: - name: monitoring packageRef: name: monitoring-packages version: v2.1.0 --- # Bundle Configuration apiVersion: stack.gokure.dev/v1alpha1 kind: Bundle metadata: name: cert-manager namespace: cert-manager spec: interval: 10m sourceRef: kind: GitRepository name: fleet-infra dependsOn: - name: crds applications: - apiVersion: generators.gokure.dev/v1alpha1 kind: AppWorkload metadata: name: cert-manager spec: # ... application config Bundle Validation The Stack module includes comprehensive validation for Bundle configurations to ensure GitOps best practices and prevent deployment issues.\nInterval Field Validation All time-based fields in Bundle configurations are automatically validated:\ninterval: GitOps reconciliation frequency timeout: Maximum wait time for resources to be ready retryInterval: Frequency for retrying failed reconciliations Validation Rules:\nFormat: Must follow Go time.Duration syntax Range: 1 second minimum, 24 hours maximum Examples: \"1s\", \"5m\", \"1h\", \"1h30m45s\", \"2.5m\" Validation Examples:\n# ✅ Valid Bundle with proper intervals apiVersion: stack.gokure.dev/v1alpha1 kind: Bundle metadata: name: web-app spec: interval: \"10m\" # Reconcile every 10 minutes timeout: \"15m\" # Wait up to 15 minutes retryInterval: \"2m\" # Retry every 2 minutes # ❌ Invalid Bundle - validation errors apiVersion: stack.gokure.dev/v1alpha1 kind: Bundle metadata: name: invalid-bundle spec: interval: \"5 minutes\" # Error: spaces not allowed timeout: \"500ms\" # Error: too short (minimum 1s) retryInterval: \"48h\" # Error: too long (maximum 24h) Error Messages:\nValidation failures provide clear, actionable error messages:\nvalidation error in Bundle \"web-app\" at spec.interval: invalid interval format: \"5 minutes\", expected format like '5m', '1h', '30s' validation error in Bundle \"web-app\" at spec.timeout: interval \"500ms\" is too short, minimum is 1s Version Evolution Strategy v1alpha1: Initial implementation, API may change v1beta1: API stabilizing, backward compatibility within beta v1: Stable API, backward compatibility guaranteed Implementation Architecture Shared GVK Infrastructure Create internal/gvk package with reusable components:\n// Generic GVK representation type GVK struct { Group string Version string Kind string } // Generic registry for any GVK-enabled type type Registry[T any] struct { factories map[GVK]func() T mu sync.RWMutex } // Generic wrapper for type-aware unmarshaling type TypedWrapper[T any] struct { APIVersion string `yaml:\"apiVersion\"` Kind string `yaml:\"kind\"` Metadata map[string]any `yaml:\"metadata\"` Spec T `yaml:\"spec\"` } // Common interfaces type VersionedType interface { GetAPIVersion() string GetKind() string } Migration Plan Phase 1: Create Shared Infrastructure\nExtract generic GVK components to internal/gvk Update generators to use shared infrastructure Add comprehensive tests Phase 2: Add GVK to Stack Structs\nCreate versioned wrappers for Cluster/Node/Bundle Implement custom YAML marshaling/unmarshaling Maintain backward compatibility during transition Phase 3: Migration \u0026 Validation\nAdd schema validation for each version Create migration utilities for existing configs Update documentation and examples Current Workflow Integration Flux Workflow (pkg/stack/fluxcd/) The Flux workflow engine generates:\nBootstrap resources: Flux system components Source resources: GitRepository, OCIRepository, etc. Kustomization resources: From Bundle configurations ArgoCD Workflow (pkg/stack/argocd/) The ArgoCD workflow engine generates:\nApplication resources: From Bundle configurations AppProject resources: For grouping and RBAC Layout System (pkg/stack/layout/) The layout system handles manifest organization:\nDirectory structure: Hierarchical file layout Resource grouping: By namespace, type, or bundle Dependency ordering: Ensuring proper apply sequence Benefits of GVK Integration 1. Versioned APIs Clear schema evolution path Backward compatibility guarantees Migration tooling support 2. Multi-Format Support # Hierarchical format (current) apiVersion: stack.gokure.dev/v1alpha1 kind: Cluster spec: node: bundles: [...] # Flat format (future) apiVersion: stack.gokure.dev/v1beta1 kind: ClusterFlat spec: bundles: [...] # Direct bundle list 3. Schema Validation OpenAPI schemas for each version IDE support with autocompletion Runtime validation 4. Tooling Integration Different tools can support different versions Clear API contracts Automated conversion between versions 5. Future Extensibility Plugin system for custom stack types Alternative hierarchy models Integration with external tools Compatibility Considerations Backward Compatibility During transition, support both formats:\n// Current direct struct usage cluster := \u0026stack.Cluster{ Name: \"prod\", Node: \u0026stack.Node{...}, } // New GVK-based usage var wrapper stack.ClusterWrapper yaml.Unmarshal(data, \u0026wrapper) cluster := wrapper.ToCluster() Migration Path Phase 1: Internal infrastructure (no breaking changes) Phase 2: Add GVK support alongside existing APIs Phase 3: Deprecate old APIs (with migration period) Phase 4: Remove old APIs in next major version Testing Strategy Unit Tests GVK parsing and generation Registry functionality Wrapper marshaling/unmarshaling Version compatibility Integration Tests Full stack configuration parsing Workflow engine compatibility Layout generation with GVK structs Migration Tests Conversion between formats Backward compatibility Schema validation Future Enhancements 1. Advanced Versioning Conversion webhooks for Kubernetes-style migration Automatic version detection and upgrade Version-specific optimizations 2. Schema Management OpenAPI schema generation JSON Schema validation Documentation generation from schemas 3. Tooling CLI commands for version management Migration utilities Validation tools 4. Alternative Stack Models # GitOps-native stack (future) apiVersion: stack.gokure.dev/v2alpha1 kind: GitOpsStack spec: repositories: - url: github.com/org/infra path: clusters/prod kustomizations: [...] helmReleases: [...] Implementation Status Phase 1: Shared GVK Infrastructure ✅ COMPLETE Successfully created internal/gvk package with:\nGeneric registry using Go generics Type-safe factory patterns YAML unmarshaling with automatic type detection Version conversion framework Comprehensive test coverage Files Created:\ninternal/gvk/types.go - Core GVK types and interfaces internal/gvk/registry.go - Generic registry implementation internal/gvk/wrapper.go - TypedWrapper for YAML unmarshaling internal/gvk/parsing.go - YAML parsing utilities internal/gvk/conversion.go - Version conversion infrastructure Phase 2: Stack Struct GVK ✅ COMPLETE Implemented versioned stack types in pkg/stack/v1alpha1:\nClusterV1Alpha1: Versioned cluster configuration with GitOps support NodeV1Alpha1: Hierarchical node structure with bundles BundleV1Alpha1: Application bundle with dependencies Key Features:\nFull GVK support (apiVersion: stack.gokure.dev/v1alpha1) Conversion to unversioned types for backward compatibility Multi-document YAML parsing Nested resource support (inline and references) Comprehensive test coverage Files Created:\npkg/stack/v1alpha1/types.go - Versioned type definitions pkg/stack/v1alpha1/register.go - Registration and factory pkg/stack/v1alpha1/parser.go - YAML parsing utilities pkg/stack/v1alpha1/types_test.go - Complete test suite Phase 3: ApplicationConfig Refactoring ✅ COMPLETE Refactored generators to use internal/gvk infrastructure Updated ApplicationWrapper to eliminate circular dependencies Maintained full backward compatibility All tests passing Related Documentation generators/DESIGN.md - ApplicationConfig generator system layout/README.md - Manifest layout and organization fluxcd/README.md - Flux workflow integration argocd/README.md - ArgoCD workflow integration This design document is a living document that will be updated as the implementation progresses.",
    "description": "Stack Module Design Document Last Updated: 2025-08-08\nStatus: Implemented - Phase 2 Complete\nOverview The Stack module provides the core domain model for Kure’s hierarchical configuration system. It defines a tree structure (Cluster → Node → Bundle → Application) that represents complete Kubernetes deployments organized for GitOps workflows.\nCurrent Architecture Hierarchy Structure The stack follows a four-level hierarchy:\nCluster # Top-level configuration └── Node # Hierarchical packaging unit └── Bundle # Deployment unit (Flux Kustomization) └── Application # Single deployable app Core Components 1. Cluster (pkg/stack/cluster.go) Purpose: Represents complete cluster configuration Contains: Root Node and GitOps bootstrap configuration Current Structure: type Cluster struct { Name string `yaml:\"name\"` Node *Node `yaml:\"node,omitempty\"` GitOps *GitOpsConfig `yaml:\"gitops,omitempty\"` } 2. Node (pkg/stack/cluster.go) Purpose: Hierarchical tree structure for packaging Contains: Bundles and child Nodes Features: Package references for OCI/Git artifacts Hierarchical path management Dependency relationships 3. Bundle (pkg/stack/bundle.go) Purpose: Unit of deployment (maps to Flux Kustomization) Contains: Multiple Applications Features: Flux reconciliation settings (interval, timeout, retryInterval, source) Interval validation (1s to 24h, Go duration format) Dependency management Label propagation 4. Application (pkg/stack/application.go) Purpose: Single deployable application Contains: ApplicationConfig interface Current GVK Support: ✅ Already implemented ApplicationConfig System The ApplicationConfig interface provides pluggable resource generation:",
    "tags": [],
    "title": "Design",
    "uri": "/packages/stack/design/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Packages \u003e Stack \u003e Generators",
    "content": "ApplicationConfig Generators Design Document Note: For implementation patterns and architectural conventions, see ARCHITECTURE.md Overview The ApplicationConfig system provides a pluggable architecture for generating Kubernetes resources from different configuration formats. This document describes the design for supporting multiple ApplicationConfig implementations with automatic type detection and versioning.\nBackground The ApplicationConfig interface is embedded in the hierarchical structure:\nCluster → contains Nodes (tree structure) Node → contains a Bundle and child Nodes Bundle → contains multiple Applications Application → contains an ApplicationConfig implementation Each ApplicationConfig implementation generates Kubernetes resources specific to its type (e.g., AppWorkload, HelmChart, Kustomize).\nDesign Goals Type Safety: Compile-time checking for each generator type Extensibility: Easy addition of new generator types without modifying core interfaces Version Management: Support for evolving generator schemas over time API Readiness: Dynamic type selection and validation for API consumers Clean Separation: Self-contained generator implementations Architecture GVK Convention Following Kubernetes’ Group, Version, Kind (GVK) pattern for type identification:\napiVersion: generators.gokure.dev/v1alpha1 kind: AppWorkload metadata: name: web-app namespace: default spec: # ... generator-specific configuration This provides:\nGroup: generators.gokure.dev - namespace for all generators Version: v1alpha1, v1beta1, v1 - schema evolution Kind: AppWorkload, HelmChart, Kustomize - generator type Type Registry A central registry manages all available ApplicationConfig implementations:\ntype GVK struct { Group string Version string Kind string } type ApplicationConfigFactory func() ApplicationConfig type Registry struct { factories map[GVK]ApplicationConfigFactory mu sync.RWMutex } Generator Interface Hierarchy // Core interface - unchanged type ApplicationConfig interface { Generate(*Application) ([]*client.Object, error) } // Versioned interface for GVK support type VersionedConfig interface { ApplicationConfig GetAPIVersion() string // Returns \"group/version\" GetKind() string } // Metadata interfaces (optional) type NamedConfig interface { GetName() string SetName(string) } type NamespacedConfig interface { GetNamespace() string SetNamespace(string) } Implementation Registry Implementation Located in pkg/stack/generators/registry.go:\npackage generators import ( \"fmt\" \"strings\" \"sync\" ) type GVK struct { Group string Version string Kind string } func (g GVK) String() string { return fmt.Sprintf(\"%s/%s, Kind=%s\", g.Group, g.Version, g.Kind) } func ParseAPIVersion(apiVersion, kind string) GVK { parts := strings.Split(apiVersion, \"/\") if len(parts) == 2 { return GVK{ Group: parts[0], Version: parts[1], Kind: kind, } } // Handle core/v1 style return GVK{ Group: \"\", Version: parts[0], Kind: kind, } } type ApplicationConfigFactory func() ApplicationConfig var ( registry = \u0026Registry{ factories: make(map[GVK]ApplicationConfigFactory), } ) type Registry struct { factories map[GVK]ApplicationConfigFactory mu sync.RWMutex } func (r *Registry) Register(gvk GVK, factory ApplicationConfigFactory) { r.mu.Lock() defer r.mu.Unlock() r.factories[gvk] = factory } func (r *Registry) Create(gvk GVK) (ApplicationConfig, error) { r.mu.RLock() defer r.mu.RUnlock() factory, exists := r.factories[gvk] if !exists { return nil, fmt.Errorf(\"unknown application config type: %s\", gvk) } return factory(), nil } func (r *Registry) ListKinds() []GVK { r.mu.RLock() defer r.mu.RUnlock() kinds := make([]GVK, 0, len(r.factories)) for gvk := range r.factories { kinds = append(kinds, gvk) } return kinds } // Global registry functions func Register(gvk GVK, factory ApplicationConfigFactory) { registry.Register(gvk, factory) } func Create(apiVersion, kind string) (ApplicationConfig, error) { gvk := ParseAPIVersion(apiVersion, kind) return registry.Create(gvk) } Type-Aware Wrapper Located in pkg/stack/application_wrapper.go:\npackage stack import ( \"fmt\" \"gopkg.in/yaml.v3\" \"github.com/go-kure/kure/pkg/stack/generators\" ) // ApplicationWrapper provides type detection and unmarshaling type ApplicationWrapper struct { APIVersion string `yaml:\"apiVersion\"` Kind string `yaml:\"kind\"` Metadata ApplicationMetadata `yaml:\"metadata\"` Spec generators.ApplicationConfig `yaml:\"spec\"` } type ApplicationMetadata struct { Name string `yaml:\"name\"` Namespace string `yaml:\"namespace,omitempty\"` Labels map[string]string `yaml:\"labels,omitempty\"` } func (w *ApplicationWrapper) UnmarshalYAML(node *yaml.Node) error { // First pass: extract GVK var gvkDetect struct { APIVersion string `yaml:\"apiVersion\"` Kind string `yaml:\"kind\"` } if err := node.Decode(\u0026gvkDetect); err != nil { return fmt.Errorf(\"failed to detect GVK: %w\", err) } if gvkDetect.APIVersion == \"\" || gvkDetect.Kind == \"\" { return fmt.Errorf(\"apiVersion and kind are required fields\") } // Create appropriate config instance config, err := generators.Create(gvkDetect.APIVersion, gvkDetect.Kind) if err != nil { return fmt.Errorf(\"failed to create config for %s/%s: %w\", gvkDetect.APIVersion, gvkDetect.Kind, err) } // Decode full content var raw struct { APIVersion string `yaml:\"apiVersion\"` Kind string `yaml:\"kind\"` Metadata ApplicationMetadata `yaml:\"metadata\"` Spec yaml.Node `yaml:\"spec\"` } if err := node.Decode(\u0026raw); err != nil { return fmt.Errorf(\"failed to decode wrapper: %w\", err) } // Decode spec into the specific config type if err := raw.Spec.Decode(config); err != nil { return fmt.Errorf(\"failed to decode spec: %w\", err) } w.APIVersion = raw.APIVersion w.Kind = raw.Kind w.Metadata = raw.Metadata w.Spec = config return nil } func (w *ApplicationWrapper) ToApplication() *Application { app := NewApplication(w.Metadata.Name, w.Metadata.Namespace, w.Spec) // If the config supports metadata injection, apply it if named, ok := w.Spec.(generators.NamedConfig); ok { named.SetName(w.Metadata.Name) } if namespaced, ok := w.Spec.(generators.NamespacedConfig); ok { namespaced.SetNamespace(w.Metadata.Namespace) } return app } Example Generator Implementations AppWorkload v1alpha1 Located in pkg/stack/generators/appworkload.go:\npackage generators func init() { Register(GVK{ Group: \"generators.gokure.dev\", Version: \"v1alpha1\", Kind: \"AppWorkload\", }, func() ApplicationConfig { return \u0026AppWorkloadConfig{} }) } // AppWorkloadConfig with GVK support type AppWorkloadConfig struct { // Existing fields... Name string `yaml:\"name\"` Namespace string `yaml:\"namespace,omitempty\"` // ... rest of existing implementation } func (c *AppWorkloadConfig) GetAPIVersion() string { return \"generators.gokure.dev/v1alpha1\" } func (c *AppWorkloadConfig) GetKind() string { return \"AppWorkload\" } func (c *AppWorkloadConfig) GetName() string { return c.Name } func (c *AppWorkloadConfig) SetName(name string) { c.Name = name } func (c *AppWorkloadConfig) GetNamespace() string { return c.Namespace } func (c *AppWorkloadConfig) SetNamespace(namespace string) { c.Namespace = namespace } HelmChart v1alpha1 Located in pkg/stack/generators/helmchart.go:\npackage generators import ( fluxhelm \"github.com/fluxcd/helm-controller/api/v2beta1\" ) func init() { Register(GVK{ Group: \"generators.gokure.dev\", Version: \"v1alpha1\", Kind: \"HelmChart\", }, func() ApplicationConfig { return \u0026HelmChartConfig{} }) } type HelmChartConfig struct { Name string `yaml:\"name\"` Namespace string `yaml:\"namespace,omitempty\"` // Helm-specific fields Chart string `yaml:\"chart\"` Version string `yaml:\"version\"` Repository string `yaml:\"repository,omitempty\"` Values map[string]interface{} `yaml:\"values,omitempty\"` // Advanced options CreateNamespace bool `yaml:\"createNamespace,omitempty\"` Wait bool `yaml:\"wait,omitempty\"` Timeout string `yaml:\"timeout,omitempty\"` DependsOn []string `yaml:\"dependsOn,omitempty\"` } func (h *HelmChartConfig) Generate(app *Application) ([]*client.Object, error) { // Generate HelmRelease for Flux or Application for ArgoCD // based on the workflow context } // Implement VersionedConfig func (h *HelmChartConfig) GetAPIVersion() string { return \"generators.gokure.dev/v1alpha1\" } func (h *HelmChartConfig) GetKind() string { return \"HelmChart\" } // Implement NamedConfig and NamespacedConfig func (h *HelmChartConfig) GetName() string { return h.Name } func (h *HelmChartConfig) SetName(name string) { h.Name = name } func (h *HelmChartConfig) GetNamespace() string { return h.Namespace } func (h *HelmChartConfig) SetNamespace(ns string) { h.Namespace = ns } Configuration Examples AppWorkload Configuration apiVersion: generators.gokure.dev/v1alpha1 kind: AppWorkload metadata: name: web-app namespace: production spec: workload: type: Deployment replicas: 3 container: image: nginx:1.21 ports: - containerPort: 80 name: http service: enabled: true type: LoadBalancer HelmChart Configuration apiVersion: generators.gokure.dev/v1alpha1 kind: HelmChart metadata: name: postgresql namespace: database spec: chart: postgresql version: 12.0.0 repository: https://charts.bitnami.com/bitnami values: auth: database: myapp persistence: size: 10Gi Kustomize Configuration apiVersion: generators.gokure.dev/v1alpha1 kind: Kustomize metadata: name: config-app namespace: default spec: path: ./overlays/production prune: true patches: - target: kind: Deployment name: app patch: | - op: replace path: /spec/replicas value: 5 Version Evolution Schema Versioning Strategy v1alpha1: Initial implementation, API may change v1beta1: API stabilizing, backward compatibility within beta v1: Stable API, backward compatibility guaranteed Version Migration When updating generator versions:\n// Conversion interface for version upgrades type Convertible interface { ConvertTo(version string) (ApplicationConfig, error) ConvertFrom(from ApplicationConfig) error } // Example: AppWorkloadConfig v1alpha1 to v1beta1 func (c *AppWorkloadConfigV1Alpha1) ConvertTo(version string) (ApplicationConfig, error) { switch version { case \"v1beta1\": return \u0026AppWorkloadConfigV1Beta1{ // Map fields from v1alpha1 to v1beta1 }, nil default: return nil, fmt.Errorf(\"unsupported version: %s\", version) } } Benefits Version Management: Clear versioning strategy following Kubernetes conventions Type Discovery: Automatic detection of generator types from configuration API Evolution: Support for backward compatibility and migrations Extensibility: New generators can be added without modifying core code Validation: Type-specific validation at unmarshal time Tooling Support: GVK pattern enables better IDE support and schema validation Future Enhancements Schema Validation: OpenAPI schema generation for each GVK Webhook Validation: Admission webhooks for API validation CRD Generation: Generate CRDs for each ApplicationConfig type Conversion Webhooks: Automatic version conversion support Discovery API: Runtime discovery of available generator types Testing Strategy Unit Tests: Each generator implementation with its own test suite Integration Tests: Cross-generator workflow tests Version Migration Tests: Ensure conversions work correctly Registry Tests: Validate registration and factory patterns YAML Parsing Tests: Comprehensive unmarshal testing Migration Path Since the project is still in development with no releases:\nDirect Migration: Update all existing configurations to use GVK format Update Examples: Modify demo and example code to use new format Generator Updates: Retrofit existing AppWorkloadConfig with GVK support Documentation: Update all documentation with new examples No Backward Compatibility: Clean break from old format Conclusion This design provides a robust, extensible system for managing multiple ApplicationConfig implementations with proper versioning and type detection. The GVK convention ensures compatibility with Kubernetes patterns and enables future API evolution.",
    "description": "ApplicationConfig Generators Design Document Note: For implementation patterns and architectural conventions, see ARCHITECTURE.md Overview The ApplicationConfig system provides a pluggable architecture for generating Kubernetes resources from different configuration formats. This document describes the design for supporting multiple ApplicationConfig implementations with automatic type detection and versioning.\nBackground The ApplicationConfig interface is embedded in the hierarchical structure:\nCluster → contains Nodes (tree structure) Node → contains a Bundle and child Nodes Bundle → contains multiple Applications Application → contains an ApplicationConfig implementation Each ApplicationConfig implementation generates Kubernetes resources specific to its type (e.g., AppWorkload, HelmChart, Kustomize).",
    "tags": [],
    "title": "Design",
    "uri": "/packages/stack/generators/design/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Development",
    "content": "Development Guide This guide covers development workflows and tooling for the Kure project.\nQuick Start # Get help with all available commands make help # Run all standard development tasks make all # Quick development cycle make check Contributing Workflow The main branch is protected — all changes must go through pull requests.\nBranch Workflow Create a feature branch from main:\ngit checkout -b feat/my-feature main Use branch prefixes: feat/, fix/, docs/, chore/\nDevelop and test locally:\nmake check # Quick validation make precommit # Full pre-commit checks Push and create a pull request:\ngit push -u origin feat/my-feature gh pr create Fill out the PR template (.github/PULL_REQUEST_TEMPLATE.md).\nPass required CI checks: lint, test, build\nGet 1 approving review, resolve all conversations\nMerge (linear history required — rebase, no merge commits)\nBranch Protection Rules Required status checks (strict): lint, test, build, rebase-check Auto-rebase: open PRs are automatically rebased when main is updated (via auto-rebase.yml) Required reviews: 1 approving review Linear history: enforced (rebase only, no merge commits) Conversation resolution: all conversations must be resolved Force pushes: disabled Branch deletion: disabled Enforced for admins: yes Development Workflow 1. Initial Setup # Install dependencies make deps # Install development tools make tools 2. Development Cycle # Format code make fmt # Run quick checks (lint, vet, short tests) make check # Run all tests make test # Run tests with coverage make test-coverage 3. Building # Build all executables make build # Build specific executable make build-kure make build-kurel make build-demo # Build with race detection for debugging make build-race 4. Testing # Run all tests make test # Run tests with verbose output make test-verbose # Run tests with race detection make test-race # Run only short tests (good for quick feedback) make test-short # Run tests with coverage report make test-coverage # Run benchmark tests make test-benchmark # Run integration tests (when available) make test-integration 5. Code Quality # Run all linting make lint # Format code make fmt # Run go vet make vet # Tidy modules make tidy # Run Qodana static analysis (requires Docker) make qodana 6. Demo and Examples # Run comprehensive demo make demo # Run GVK generators demo make demo-gvk # Generate all examples (alias for demo) make examples 7. Package Operations # Build a kurel package make kurel-build PACKAGE_PATH=path/to/package # Show package information make kurel-info PACKAGE_PATH=path/to/package Pre-commit Workflow Before committing changes, run:\nmake precommit This will:\nFormat code with go fmt Tidy modules Run linters Run go vet Run all tests CI/CD Pipeline The project uses several GitHub Actions workflows:\nMain CI Pipeline (.github/workflows/ci.yml) Triggers: Push to main/develop, PRs Jobs: Test (unit, race, coverage) Lint and format check Build executables Generate demo outputs Integration tests (main branch only) Cross-platform builds Security scanning Dependency vulnerability checks Qodana Code Quality (.github/workflows/code_quality.yml) Triggers: Push, PRs Purpose: Static analysis with JetBrains Qodana Uses: make deps for setup Auto-Rebase (.github/workflows/auto-rebase.yml) Triggers: Push to main Purpose: Automatically rebases all open PRs targeting main Uses: peter-evans/rebase@v4 Excludes: Dependabot PRs (dependencies label), draft PRs Auth: Requires AUTO_REBASE_PAT secret (PAT needed to trigger CI on rebased branches) Release Pipeline (.github/workflows/release.yml) Triggers: Version tags (v*.*.*) Jobs: Pre-release validation with make ci-coverage Release readiness check with make release-check Multi-platform build with make release-build GitHub release creation Go proxy refresh PR Checks (.github/workflows/pr-checks.yml) Triggers: PR events Jobs: Quick validation with make check Security and dependency checks Test coverage validation Changed files analysis Performance benchmarks (when labeled) Documentation validation Dependabot Management Handling PRs Use @dependabot commands in PR comments (not gh pr close):\nCommand Effect @dependabot close Close PR, prevent recreation @dependabot ignore this dependency Close PR, ignore dependency permanently @dependabot ignore this major version Ignore major version updates @dependabot ignore this minor version Ignore minor version updates @dependabot rebase Rebase the PR @dependabot recreate Recreate the PR from scratch Deferring Updates When an update requires a blocked dependency (e.g., newer Go version):\nComment @dependabot close with explanation and link to blocking issue Do not use gh pr close directly - Dependabot will recreate the PR Reference: GitHub Docs - Dependabot PR Commands Makefile Targets Reference Development help - Display help message all - Run all standard development tasks info - Display project information clean - Clean build artifacts and caches Dependencies deps - Download and tidy Go modules deps-upgrade - Upgrade all dependencies tools - Install development tools outdated - Check for outdated dependencies Building build - Build all executables build-kure - Build kure executable build-kurel - Build kurel executable build-demo - Build demo executable build-race - Build with race detection Testing test - Run all tests test-verbose - Run tests with verbose output test-race - Run tests with race detection test-short - Run short tests only test-coverage - Run tests with coverage report test-benchmark - Run benchmark tests test-integration - Run integration tests Code Quality lint - Run all linters lint-go - Run golangci-lint fmt - Format Go code vet - Run go vet tidy - Tidy modules qodana - Run Qodana static analysis CI/CD ci - Run CI pipeline tasks ci-coverage - Run CI with coverage ci-integration - Run CI with integration tests check - Quick code quality check precommit - Run all pre-commit checks Release release-check - Check if ready for release release-build - Build release artifacts for multiple platforms Utilities generate - Run go generate mod-graph - Display module dependency graph list-packages - List all packages demo* - Various demo commands Environment Variables Key environment variables the Makefile respects:\nGO - Go command (default: go) GOROOT - Go root directory VERSION - Version string for builds BUILD_DIR - Build output directory (default: bin) OUTPUT_DIR - Demo output directory (default: out) TEST_TIMEOUT - Test timeout (default: 30s) PACKAGE_PATH - Package path for kurel operations Development Tips Running Demos The demo system generates example YAML files showing Kure’s capabilities:\n# Run all demos make demo # Generated files appear in out/ directory ls -la out/ Testing Strategy Use make test-short for quick feedback during development Use make test-coverage to check coverage before PRs Use make test-race to catch concurrency issues Use make check for quick pre-commit validation Code Quality The CI pipeline enforces 80% test coverage All code must pass golangci-lint checks Code must be properly formatted with go fmt Modules must be tidy Performance Benchmark tests can be run with make test-benchmark PR checks include performance benchmarks when labeled with performance Build targets include optimized release builds with -s -w flags Troubleshooting Build Issues # Clean everything and rebuild make clean all # Check Go installation and environment make info Test Failures # Run tests with verbose output for more details make test-verbose # Run specific test go test -v ./pkg/specific/package -run TestSpecific Dependency Issues # Update dependencies make deps-upgrade # Check for outdated or vulnerable dependencies make outdated This development guide provides a comprehensive overview of the development workflow using the Makefile and CI/CD pipeline.\nCrane Integration Kure is a dependency of the Crane project (~/src/autops/wharf/crane).\nRelationship Crane transforms OAM → Kure domain model → Kubernetes manifests Kure provides the domain model and manifest generation engine Both repos are co-developed with local replace directives Key Files Crane’s requirements: ~/src/autops/wharf/crane/PLAN.md Crane’s agent guide: ~/src/autops/wharf/crane/AGENTS.md When Making Changes Check if change affects Crane’s integration Keep public API (pkg/stack/) stable when possible Update Crane if breaking changes are necessary Test with go mod tidy in Crane to verify compatibility Go Workspaces Crane uses Go workspaces for local development. The workspace file lives in the parent directory:\n# From wharf/ directory go work init go work use ./crane ./kure This allows Crane to use your local Kure changes without pushing.\nBefore pushing Kure changes that Crane depends on:\nPush Kure changes first In Crane: GOWORK=off go get github.com/go-kure/kure@main Commit the updated go.mod/go.sum in Crane",
    "description": "Development Guide This guide covers development workflows and tooling for the Kure project.\nQuick Start # Get help with all available commands make help # Run all standard development tasks make all # Quick development cycle make check Contributing Workflow The main branch is protected — all changes must go through pull requests.\nBranch Workflow Create a feature branch from main:\ngit checkout -b feat/my-feature main Use branch prefixes: feat/, fix/, docs/, chore/",
    "tags": [],
    "title": "Development Guide",
    "uri": "/development/guide/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Packages \u003e Stack",
    "content": "Stack Generators The generators sub-package provides strategies for generating Kubernetes resources within the stack hierarchy.\nDocumentation Design - Generator system design Architecture - Generator architecture API Reference pkg.go.dev/github.com/go-kure/kure/pkg/stack/generators",
    "description": "Stack Generators The generators sub-package provides strategies for generating Kubernetes resources within the stack hierarchy.\nDocumentation Design - Generator system design Architecture - Generator architecture API Reference pkg.go.dev/github.com/go-kure/kure/pkg/stack/generators",
    "tags": [],
    "title": "Generators",
    "uri": "/packages/stack/generators/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Packages",
    "content": "Launcher Package The launcher package (formerly kurel) provides a package system for creating reusable, customizable Kubernetes applications without the complexity of templating engines.\nIt uses a declarative patch-based approach to customize base Kubernetes manifests, enabling composable and type-safe application definitions.\nDocumentation Overview - Package overview and usage Design - High-level design document Design Details - Detailed design decisions Code Design - Code-level architecture Implementation Plan - Implementation roadmap Architecture - Package architecture API Reference pkg.go.dev/github.com/go-kure/kure/pkg/launcher",
    "description": "Launcher Package The launcher package (formerly kurel) provides a package system for creating reusable, customizable Kubernetes applications without the complexity of templating engines.\nIt uses a declarative patch-based approach to customize base Kubernetes manifests, enabling composable and type-safe application definitions.\nDocumentation Overview - Package overview and usage Design - High-level design document Design Details - Detailed design decisions Code Design - Code-level architecture Implementation Plan - Implementation roadmap Architecture - Package architecture API Reference pkg.go.dev/github.com/go-kure/kure/pkg/launcher",
    "tags": [],
    "title": "Launcher",
    "uri": "/packages/launcher/index.html"
  },
  {
    "breadcrumb": "Go Kure",
    "content": "Overview Kure is a Go library and CLI toolset for programmatic Kubernetes resource generation, designed for GitOps workflows with Flux and ArgoCD.\nExplore the Project README for a comprehensive introduction.",
    "description": "Overview Kure is a Go library and CLI toolset for programmatic Kubernetes resource generation, designed for GitOps workflows with Flux and ArgoCD.\nExplore the Project README for a comprehensive introduction.",
    "tags": [],
    "title": "Overview",
    "uri": "/overview/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Packages \u003e Launcher",
    "content": "Launcher - Kubernetes Resources Launcher This package provides the core functionality for Kurel, the Kubernetes Resources Launcher CLI tool.\nKurel is a package system for creating reusable, customizable Kubernetes applications without the complexity of templating engines. It uses a declarative patch-based approach to customize base Kubernetes manifests, making it perfect for GitOps workflows.\n✨ Key Features 📦 Package-based - Encapsulate applications in reusable .kurel packages 🎯 No Templating - Use patches instead of complex template syntax 🔧 Declarative Customization - Simple parameter-driven configuration 🚀 GitOps Native - Generate clean Kubernetes manifests for Flux/ArgoCD 📊 Schema Validation - Auto-generated schemas with Kubernetes API integration 🏗️ Multi-Phase Deployment - Support for ordered deployment phases 🌐 Multi-Namespace - Deploy across multiple namespaces seamlessly 🎨 User Extensions - Extend packages without modifying originals 🚀 Quick Start Installing a Package # Download a kurel package (example) git clone https://github.com/example/prometheus-operator.kurel # Validate the package kurel validate prometheus-operator.kurel/ # Customize with your parameters cat \u003e my-values.yaml \u003c\u003c EOF monitoring: enabled: true retention: 7d persistence: enabled: true size: 50Gi resources: requests: cpu: 200m memory: 512Mi EOF # Generate manifests kurel build prometheus-operator.kurel/ \\ --values my-values.yaml \\ --output ./manifests/ Using with GitOps # Generated structure is GitOps-ready ls manifests/ # pre-install/ - CRDs, namespaces, RBAC # main/ - Main application (depends on pre-install) # post-install/ - Monitoring, backups (depends on main) # Each phase includes kustomization.yaml with proper dependencies cat manifests/main/kustomization.yaml # apiVersion: kustomize.config.k8s.io/v1beta1 # kind: Kustomization # dependsOn: # - name: prometheus-pre-install # resources: [...] 📁 Package Structure A kurel package is a directory with this structure:\nmy-app.kurel/ ├── parameters.yaml # Variables and package metadata ├── resources/ # Base Kubernetes manifests │ ├── deployment.yaml │ ├── service.yaml │ └── namespace.yaml ├── patches/ # Modular customization patches │ ├── 00-base.kpatch # Global settings │ ├── features/ │ │ ├── 10-monitoring.kpatch │ │ ├── 10-monitoring.yaml # Patch conditions │ │ └── 20-ingress.kpatch │ └── profiles/ │ ├── 10-dev.kpatch │ └── 20-prod.kpatch └── README.md # Package documentation ⚙️ Configuration Parameters File The parameters.yaml file contains all configurable options:\n# Package metadata kurel: name: my-application version: 1.0.0 description: \"A sample application package\" # Global defaults applied to all resources global: labels: app.kubernetes.io/managed-by: kurel resources: requests: cpu: 100m memory: 128Mi # Feature flags monitoring: enabled: false # Enable monitoring patches retention: 30d # Application settings app: replicas: 3 image: registry: docker.io repository: myapp tag: v1.0.0 Patch System Patches use simple TOML syntax to modify resources:\n# patches/features/10-monitoring.kpatch [deployment.myapp.spec.template.spec] securityContext.runAsNonRoot: true [deployment.myapp.spec.template.spec.containers.0] resources: \"${global.resources}\" image: \"${app.image.registry}/${app.image.repository}:${app.image.tag}\" # Add monitoring sidecar [deployment.myapp.spec.template.spec.containers.-] name: \"metrics-exporter\" image: \"prom/node-exporter:latest\" ports: - containerPort: 9100 name: \"metrics\" Conditional Patches Control when patches are applied:\n# patches/features/10-monitoring.yaml enabled: \"${monitoring.enabled}\" # Only apply if monitoring enabled description: \"Adds Prometheus monitoring\" requires: # Auto-enable these patches - \"features/05-metrics-base.kpatch\" conflicts: # Cannot be used with these - \"features/20-simple-monitoring.kpatch\" 🔧 User Customization Local Extensions Extend packages without modifying them using .local.kurel:\nmy-app.local.kurel/ ├── parameters.yaml # Override parameters └── patches/ └── 50-custom.kpatch # Add custom patches Example Local Override # my-app.local.kurel/parameters.yaml monitoring: enabled: true # Enable monitoring retention: 7d # Shorter retention app: replicas: 5 # More replicas for production 🏗️ Multi-Phase Deployment Support complex applications that need ordered deployment:\n# In your Kubernetes resources apiVersion: v1 kind: Namespace metadata: name: myapp annotations: kurel.gokure.dev/install-phase: \"pre-install\" --- apiVersion: apps/v1 kind: Deployment metadata: name: myapp-server annotations: kurel.gokure.dev/install-phase: \"main\" --- apiVersion: v1 kind: Service metadata: name: myapp-monitor annotations: kurel.gokure.dev/install-phase: \"post-install\" This generates three separate phases with proper dependencies for GitOps deployment.\n🛠️ CLI Commands Validation # Validate package structure and parameters kurel validate my-app.kurel/ --values my-values.yaml # Output shows enabled patches and validation results # ✓ Package structure valid # ✓ All variables resolved # Enabled patches: # ✓ patches/00-base.kpatch # ✓ patches/features/10-monitoring.kpatch (monitoring.enabled=true) # → patches/features/05-metrics-base.kpatch (auto-enabled) Schema Generation # Generate validation schemas kurel schema generate my-app.kurel/ # Creates schemas/parameters.schema.json with: # - Type information from parameter values # - Kubernetes validation rules where traceable # - Custom validation patterns Building Manifests # Generate final Kubernetes manifests kurel build my-app.kurel/ \\ --values production.yaml \\ --output ./deploy/ # Generates phase-based directory structure: # deploy/pre-install/ - CRDs, namespaces, RBAC # deploy/main/ - Main application # deploy/post-install/ - Monitoring, cleanup Package Information # Show package details kurel info my-app.kurel/ # Package: my-application v1.0.0 # Description: A sample application package # Patches: 8 total, 3 conditional # Variables: 12 configurable parameters # Phases: pre-install, main, post-install 🌟 Common Use Cases Platform Teams Create standardized application packages with defined customization boundaries:\n# Standard web app package kurel: name: webapp-standard version: 2.1.0 global: securityContext: runAsNonRoot: true fsGroup: 1000 # Teams can only customize these parameters app: replicas: 3 domain: \"\" # Required override monitoring: enabled: true # Always enabled for compliance Multi-Environment Deployments Same package, different configurations:\n# Development kurel build webapp.kurel/ --values environments/dev.yaml # Staging kurel build webapp.kurel/ --values environments/staging.yaml # Production kurel build webapp.kurel/ --values environments/prod.yaml Complex Applications Multi-namespace applications with dependencies:\n# Database in its own namespace (pre-install) # Application in app namespace (main) # Monitoring in monitoring namespace (post-install) global: namespaces: create: true database: namespace: database app: namespace: application monitoring: namespace: monitoring 🤝 Contributing Kurel is part of the Kure project. See the main repository for contribution guidelines.\n📚 Documentation Design Specification - Technical design and architecture Detailed Design Document - Complete design discussion and decisions Kure Documentation - Main project documentation 🎯 Philosophy Kurel follows the principle “kurel just generates YAML”. It’s not a runtime system or complex orchestrator - it’s a focused tool that generates clean, validated Kubernetes manifests for your GitOps workflows.\nThis approach gives you:\nPredictable output - Same inputs always generate same manifests GitOps compatibility - Standard Kubernetes YAML that any tool can deploy Debugging simplicity - Generated manifests are human-readable Tool independence - Not locked into specific deployment tools",
    "description": "Launcher - Kubernetes Resources Launcher This package provides the core functionality for Kurel, the Kubernetes Resources Launcher CLI tool.\nKurel is a package system for creating reusable, customizable Kubernetes applications without the complexity of templating engines. It uses a declarative patch-based approach to customize base Kubernetes manifests, making it perfect for GitOps workflows.\n✨ Key Features 📦 Package-based - Encapsulate applications in reusable .kurel packages 🎯 No Templating - Use patches instead of complex template syntax 🔧 Declarative Customization - Simple parameter-driven configuration 🚀 GitOps Native - Generate clean Kubernetes manifests for Flux/ArgoCD 📊 Schema Validation - Auto-generated schemas with Kubernetes API integration 🏗️ Multi-Phase Deployment - Support for ordered deployment phases 🌐 Multi-Namespace - Deploy across multiple namespaces seamlessly 🎨 User Extensions - Extend packages without modifying originals 🚀 Quick Start Installing a Package # Download a kurel package (example) git clone https://github.com/example/prometheus-operator.kurel # Validate the package kurel validate prometheus-operator.kurel/ # Customize with your parameters cat \u003e my-values.yaml \u003c\u003c EOF monitoring: enabled: true retention: 7d persistence: enabled: true size: 50Gi resources: requests: cpu: 200m memory: 512Mi EOF # Generate manifests kurel build prometheus-operator.kurel/ \\ --values my-values.yaml \\ --output ./manifests/ Using with GitOps # Generated structure is GitOps-ready ls manifests/ # pre-install/ - CRDs, namespaces, RBAC # main/ - Main application (depends on pre-install) # post-install/ - Monitoring, backups (depends on main) # Each phase includes kustomization.yaml with proper dependencies cat manifests/main/kustomization.yaml # apiVersion: kustomize.config.k8s.io/v1beta1 # kind: Kustomization # dependsOn: # - name: prometheus-pre-install # resources: [...] 📁 Package Structure A kurel package is a directory with this structure:",
    "tags": [],
    "title": "Overview",
    "uri": "/packages/launcher/readme/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Packages \u003e Layout",
    "content": "Layout Module The layout module is a sophisticated system for organizing and writing Kubernetes manifests to disk in directory structures that work with GitOps tools like Flux and ArgoCD.\nCore Purpose The layout module transforms Kure’s in-memory stack representation (Clusters → Nodes → Bundles → Applications) into organized directory structures with proper kustomization.yaml files that GitOps tools can consume.\nKey Components 1. ManifestLayout Structure Central data structure representing a directory with its resources and children Contains: Name, Namespace, Resources (K8s objects), Children (subdirectories) Supports package-aware layouts for multi-OCI/Git scenarios 2. LayoutRules Configuration NodeGrouping: How nodes are organized (GroupByName creates dirs, GroupFlat flattens) BundleGrouping: How bundles within nodes are organized ApplicationGrouping: How applications within bundles are organized FilePer: How resources are written (FilePerResource vs FilePerKind) FluxPlacement: Where Flux Kustomizations go (FluxSeparate vs FluxIntegrated) 3. Two Main Walker Functions WalkCluster(): Standard hierarchical layout (Node → Bundle → App structure) WalkClusterByPackage(): Groups by PackageRef for multi-source scenarios 4. Writing System WriteManifest(): Standard hierarchical writing WritePackagesToDisk(): Package-based writing with sanitized directory names Auto-generates kustomization.yaml files with proper resource references Directory Structure Patterns Standard Layout (WalkCluster) clusters/ cluster-name/ node1/ bundle1/ app1/ manifest-files.yaml kustomization.yaml app2/... bundle2/... node2/... Package-Based Layout (WalkClusterByPackage) oci-packages/ cluster/ web/ app-manifests.yaml git-packages/ cluster/ monitoring/ app-manifests.yaml Flat Layout (GroupFlat rules) clusters/ cluster-name/ all-manifests-together.yaml kustomization.yaml GitOps Tool Compatibility Flux Integration Uses spec.path: ./clusters/cluster-name/node format Auto-generates kustomization.yaml files Supports recursive discovery of manifests Handles FluxSeparate vs FluxIntegrated placement modes ArgoCD Integration Uses spec.source.path: clusters/cluster-name/node format Requires explicit kustomization.yaml files (no auto-discovery) Each target directory needs its own Application Advanced Features Package Reference Support Tracks different source types (OCIRepository, GitRepository, Bucket) Enables multi-source deployments with proper isolation Sanitizes package keys into valid directory names Flexible File Organization FilePerResource: Each K8s object gets its own file FilePerKind: Group objects by Kind (all Services together, etc.) AppFileSingle: All app resources in one file Kustomization Generation KustomizationExplicit: Lists all manifest files explicitly KustomizationRecursive: References subdirectories only Smart handling of cross-references and child relationships Real-World Use Cases Simple Cluster: Single source, hierarchical structure Multi-OCI Deployment: Different services from different OCI registries Monorepo: Everything flattened into minimal directory structure Bootstrap Scenarios: Special handling for Flux/ArgoCD system components Example Usage // Create layout rules rules := layout.DefaultLayoutRules() rules.BundleGrouping = layout.GroupFlat rules.ApplicationGrouping = layout.GroupFlat // Walk cluster to create layout ml, err := layout.WalkCluster(cluster, rules) if err != nil { return err } // Write to disk cfg := layout.DefaultLayoutConfig() err = layout.WriteManifest(\"out/manifests\", cfg, ml) Key Files types.go: Core types and configuration options walker.go: Tree traversal algorithms (WalkCluster, WalkClusterByPackage) manifest.go: ManifestLayout structure and package-based writing write.go: Standard manifest writing with kustomization generation config.go: Configuration and file naming conventions The layout module essentially bridges the gap between Kure’s programmatic resource construction and the file-based expectations of GitOps workflows, with extensive configurability for different organizational preferences and tool requirements.",
    "description": "Layout Module The layout module is a sophisticated system for organizing and writing Kubernetes manifests to disk in directory structures that work with GitOps tools like Flux and ArgoCD.\nCore Purpose The layout module transforms Kure’s in-memory stack representation (Clusters → Nodes → Bundles → Applications) into organized directory structures with proper kustomization.yaml files that GitOps tools can consume.\nKey Components 1. ManifestLayout Structure Central data structure representing a directory with its resources and children Contains: Name, Namespace, Resources (K8s objects), Children (subdirectories) Supports package-aware layouts for multi-OCI/Git scenarios 2. LayoutRules Configuration NodeGrouping: How nodes are organized (GroupByName creates dirs, GroupFlat flattens) BundleGrouping: How bundles within nodes are organized ApplicationGrouping: How applications within bundles are organized FilePer: How resources are written (FilePerResource vs FilePerKind) FluxPlacement: Where Flux Kustomizations go (FluxSeparate vs FluxIntegrated) 3. Two Main Walker Functions WalkCluster(): Standard hierarchical layout (Node → Bundle → App structure) WalkClusterByPackage(): Groups by PackageRef for multi-source scenarios 4. Writing System WriteManifest(): Standard hierarchical writing WritePackagesToDisk(): Package-based writing with sanitized directory names Auto-generates kustomization.yaml files with proper resource references Directory Structure Patterns Standard Layout (WalkCluster) clusters/ cluster-name/ node1/ bundle1/ app1/ manifest-files.yaml kustomization.yaml app2/... bundle2/... node2/... Package-Based Layout (WalkClusterByPackage) oci-packages/ cluster/ web/ app-manifests.yaml git-packages/ cluster/ monitoring/ app-manifests.yaml Flat Layout (GroupFlat rules) clusters/ cluster-name/ all-manifests-together.yaml kustomization.yaml GitOps Tool Compatibility Flux Integration Uses spec.path: ./clusters/cluster-name/node format Auto-generates kustomization.yaml files Supports recursive discovery of manifests Handles FluxSeparate vs FluxIntegrated placement modes ArgoCD Integration Uses spec.source.path: clusters/cluster-name/node format Requires explicit kustomization.yaml files (no auto-discovery) Each target directory needs its own Application Advanced Features Package Reference Support Tracks different source types (OCIRepository, GitRepository, Bucket) Enables multi-source deployments with proper isolation Sanitizes package keys into valid directory names Flexible File Organization FilePerResource: Each K8s object gets its own file FilePerKind: Group objects by Kind (all Services together, etc.) AppFileSingle: All app resources in one file Kustomization Generation KustomizationExplicit: Lists all manifest files explicitly KustomizationRecursive: References subdirectories only Smart handling of cross-references and child relationships Real-World Use Cases Simple Cluster: Single source, hierarchical structure Multi-OCI Deployment: Different services from different OCI registries Monorepo: Everything flattened into minimal directory structure Bootstrap Scenarios: Special handling for Flux/ArgoCD system components Example Usage // Create layout rules rules := layout.DefaultLayoutRules() rules.BundleGrouping = layout.GroupFlat rules.ApplicationGrouping = layout.GroupFlat // Walk cluster to create layout ml, err := layout.WalkCluster(cluster, rules) if err != nil { return err } // Write to disk cfg := layout.DefaultLayoutConfig() err = layout.WriteManifest(\"out/manifests\", cfg, ml) Key Files types.go: Core types and configuration options walker.go: Tree traversal algorithms (WalkCluster, WalkClusterByPackage) manifest.go: ManifestLayout structure and package-based writing write.go: Standard manifest writing with kustomization generation config.go: Configuration and file naming conventions The layout module essentially bridges the gap between Kure’s programmatic resource construction and the file-based expectations of GitOps workflows, with extensive configurability for different organizational preferences and tool requirements.",
    "tags": [],
    "title": "Overview",
    "uri": "/packages/layout/readme/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Examples",
    "content": "Kure Patch Module Examples (TOML Format) This directory demonstrates the new TOML-style patch system with cert-manager as an example.\nFiles cert-manager-simple.yaml - Base cert-manager resources (simplified) resources.kpatch - Resource limits using TOML container selectors ingress.kpatch - Service configuration with port selectors security.kpatch - Security contexts with deployment targeting advanced.kpatch - Complex selectors and variable substitution Running the Demo go run ./cmd/demo -patches TOML Patch Format The patch files now use TOML-style headers for precise resource targeting:\n# Basic resource targeting [deployment.app] spec.replicas: 3 metadata.labels.env: production # Container-specific patches [deployment.app.containers.name=main] resources.requests.cpu: 100m resources.limits.memory: 512Mi # Service port configuration [service.app.ports.name=https] port: 443 nodePort: 30443 # Array index targeting [ingress.web.rules.0.paths.0] path: /api pathType: Prefix Header Grammar The TOML header format follows this grammar:\n[kind.name[.section[.subsection[.selector]]]] Selector Types Key-value selectors: containers.name=main Index selectors: ports.0, rules.1 Bracketed selectors: containers[image=nginx] Kubernetes Path Mapping The system intelligently maps TOML sections to Kubernetes paths:\nTOML Section Kubernetes Path (Deployment) Kubernetes Path (Service) containers spec.template.spec.containers spec.containers ports spec.template.spec.containers.ports spec.ports volumes spec.template.spec.volumes spec.volumes env spec.template.spec.containers.env N/A Variable Substitution Support for dynamic values using variable substitution:\n[deployment.app.containers.name=main] image.tag: \"${values.version}\" resources.requests.cpu: \"${values.cpu_request}\" debug.enabled: \"${features.enable_debug}\" Variable context:\n\u0026patch.VariableContext{ Values: map[string]interface{}{ \"version\": \"1.20\", \"cpu_request\": \"100m\", }, Features: map[string]bool{ \"enable_debug\": true, }, } Examples by Complexity Basic Resource Targeting [deployment.cert-manager] spec.replicas: 3 metadata.labels.environment: production Container-Specific Configuration [deployment.cert-manager.containers.name=cert-manager-controller] resources.requests.cpu: 100m resources.limits.memory: 512Mi securityContext.readOnlyRootFilesystem: true Service Configuration [service.cert-manager-webhook.ports.name=https] port: 9443 nodePort: 30443 Complex Array Manipulation # Add new environment variable [deployment.app.containers.name=main.env[+]] name: DEBUG_MODE value: \"true\" # Add new volume mount [deployment.app.containers.name=main.volumeMounts[+]] name: config mountPath: /etc/config readOnly: true Key Features Intelligent Path Resolution - Automatic mapping based on resource kind Precise Targeting - Container-specific, port-specific, rule-specific patches Variable Substitution - Dynamic values with ${values.key} syntax Complex Selectors - Multiple ways to target list items Backward Compatibility - Still supports legacy YAML format Context Awareness - Different behavior for different resource types Migration from YAML Old YAML format:\n- target: cert-manager patch: spec.template.spec.containers[0].resources.requests.cpu: \"100m\" New TOML format:\n[deployment.cert-manager.containers.0] resources.requests.cpu: 100m Or with semantic selector:\n[deployment.cert-manager.containers.name=cert-manager-controller] resources.requests.cpu: 100m The TOML format provides better readability, more precise targeting, and eliminates the need for long JSONPath expressions.",
    "description": "Kure Patch Module Examples (TOML Format) This directory demonstrates the new TOML-style patch system with cert-manager as an example.\nFiles cert-manager-simple.yaml - Base cert-manager resources (simplified) resources.kpatch - Resource limits using TOML container selectors ingress.kpatch - Service configuration with port selectors security.kpatch - Security contexts with deployment targeting advanced.kpatch - Complex selectors and variable substitution Running the Demo go run ./cmd/demo -patches TOML Patch Format The patch files now use TOML-style headers for precise resource targeting:",
    "tags": [],
    "title": "Patches",
    "uri": "/examples/patches/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Overview",
    "content": "Kure Kure is a Go library for programmatically building Kubernetes resources used by GitOps tools like Flux, cert-manager, MetalLB, and External Secrets. The library emphasizes strongly-typed object construction over templating engines, providing a clean, type-safe approach to Kubernetes manifest generation.\nFeatures Type-safe Kubernetes resource builders (Deployments, Services, RBAC, etc.) GitOps workflow support (Flux CD and ArgoCD) Hierarchical domain model (Cluster → Node → Bundle → Application) Declarative JSONPath-based patching Certificate management (cert-manager, ACME) Secret management (External Secrets, multiple cloud providers) Network configuration (MetalLB) Manifest layout engine with auto-generated kustomization.yaml Built-in configuration validation Installation go get github.com/go-kure/kure CLI Tools Kure ships two CLI tools: kure for manifest generation and patching, and kurel for package-based resource management. Run kure --help or kurel --help for usage details.\nDocumentation Website — guides, architecture, and tutorials API Reference — full Go package documentation examples/ — cluster configurations, kurel packages, and patching samples Development DEVELOPMENT.md — setup, build, test, and lint instructions CHANGELOG.md — release history Quick commands:\nmake build # Build all executables make test # Run tests make lint # Run linter License This project is licensed under the Apache License 2.0 .",
    "description": "Kure Kure is a Go library for programmatically building Kubernetes resources used by GitOps tools like Flux, cert-manager, MetalLB, and External Secrets. The library emphasizes strongly-typed object construction over templating engines, providing a clean, type-safe approach to Kubernetes manifest generation.",
    "tags": [],
    "title": "Project README",
    "uri": "/overview/readme/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Getting Started",
    "content": "Quickstart Guide This guide walks you through installing Kure, generating your first cluster configuration, and deploying with Flux.\nInstallation Install the Kure CLI tools using Go:\ngo install github.com/go-kure/kure/cmd/kure@latest go install github.com/go-kure/kure/cmd/kurel@latest Verify the installation:\nkure version kurel version Hello World: Generate a Simple Cluster Config Create a minimal Go program that generates Kubernetes manifests:\npackage main import ( \"os\" \"github.com/go-kure/kure/pkg/kubernetes/fluxcd\" \"github.com/go-kure/kure/pkg/io\" kustv1 \"github.com/fluxcd/kustomize-controller/api/v1\" ) func main() { // Create a Flux Kustomization ks := fluxcd.NewKustomization(\u0026fluxcd.KustomizationConfig{ Name: \"hello-world\", Namespace: \"flux-system\", Interval: \"5m\", Path: \"./clusters/production\", SourceRef: kustv1.CrossNamespaceSourceReference{ Kind: \"GitRepository\", Name: \"flux-system\", }, }) // Print YAML to stdout printer := io.NewYAMLPrinter() printer.PrintObj(ks, os.Stdout) } Run the program to see the generated YAML:\ngo run main.go Output:\napiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: hello-world namespace: flux-system spec: interval: 5m path: ./clusters/production prune: true sourceRef: kind: GitRepository name: flux-system Build a Kurel Package Kurel packages provide a structured way to define reusable Kubernetes applications. Try building the example Frigate package:\ncd examples/kurel/frigate kurel build . This generates Kubernetes manifests from the package definition. Inspect the package structure:\nfrigate/ kurel.yaml # Package metadata parameters.yaml # Configurable parameters resources/ # Base Kubernetes resources patches/ # Optional patches Validate a package before deployment:\nkurel validate . View package information:\nkurel info . Deploy with Flux Once you have generated manifests, deploy them using Flux:\nCommit the manifests to your Git repository git add clusters/ git commit -m \"Add hello-world kustomization\" git push Flux reconciles automatically If Flux is already watching your repository, it will automatically apply the new Kustomization. Check the status:\nflux get kustomizations Or trigger manually flux reconcile kustomization flux-system --with-source Next Steps Architecture: Read ARCHITECTURE.md for a deep dive into Kure’s design Examples: Explore the examples/ directory for more complex configurations API Reference: See the full API at pkg.go.dev Patching: Learn about declarative patching in the README CLI Reference: Run kure --help and kurel --help for all available commands",
    "description": "Quickstart Guide This guide walks you through installing Kure, generating your first cluster configuration, and deploying with Flux.\nInstallation Install the Kure CLI tools using Go:\ngo install github.com/go-kure/kure/cmd/kure@latest go install github.com/go-kure/kure/cmd/kurel@latest Verify the installation:\nkure version kurel version Hello World: Generate a Simple Cluster Config Create a minimal Go program that generates Kubernetes manifests:\npackage main import ( \"os\" \"github.com/go-kure/kure/pkg/kubernetes/fluxcd\" \"github.com/go-kure/kure/pkg/io\" kustv1 \"github.com/fluxcd/kustomize-controller/api/v1\" ) func main() { // Create a Flux Kustomization ks := fluxcd.NewKustomization(\u0026fluxcd.KustomizationConfig{ Name: \"hello-world\", Namespace: \"flux-system\", Interval: \"5m\", Path: \"./clusters/production\", SourceRef: kustv1.CrossNamespaceSourceReference{ Kind: \"GitRepository\", Name: \"flux-system\", }, }) // Print YAML to stdout printer := io.NewYAMLPrinter() printer.PrintObj(ks, os.Stdout) } Run the program to see the generated YAML:",
    "tags": [],
    "title": "Quickstart",
    "uri": "/getting-started/quickstart/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Changelog",
    "content": "Changelog All notable changes to this project will be documented in this file.\n[0.1.0-alpha.1] - 2026-01-30 Fixed Run tests directly in release workflow instead of checking CI status [0.1.0-alpha.0] - 2026-01-30 Added Add storageclass helpers Add kustomize helpers Add flux source helpers Add helpers Add fluxcd builders package Add layout grouping and app file mode Support file- and dir-per-application layouts Implement OCI artifact separation in layout system Implement GitOps bootstrap and refactor demo system to data-driven architecture Implement comprehensive Kubernetes printer wrappers in io module Modernize error handling with custom error types and standardization Implement professional Cobra CLI with comprehensive command structure Implement comprehensive structured error handling system Add shorthand flags for common CLI options across all commands Complete kurel package system design documentation Implement package loader with hybrid error handling Implement variable resolver with cycle detection Implement patch processor with dependency resolution Implement schema generation and validation for launcher Complete Phase 4 - schema generation and validation Complete Phase 5 - output builder and local extensions Implement Phase 6 - CLI command integration Implement Phase 7 - comprehensive integration tests Implement GVK-based ApplicationConfig generator system Implement GVK-based versioning for stack module structs Implement GVK-based versioning for stack module structs Add comprehensive Makefile and CI/CD pipeline Complete KurelPackage generator implementation Enable Kubernetes schema inclusion in kurel CLI Implement fluent builder pattern Phase 1 Implement comprehensive interval validation for GitOps configurations Add Go version management tools Add fast precommit target for git hooks Add PodDisruptionBudget builder Add HorizontalPodAutoscaler builder Add combined-output mode to kure patch Add –diff option to kure patch Build Update Go to 1.24.12 to fix govulncheck vulnerabilities Automate changelog generation with git-cliff CI Add GitHub Action to refresh Go proxy on main branch commits Enforce Go version consistency in PR checks Remove Qodana workflow due to licensing issues Fix security scan action to use official gosec action Remove gosec security scan (CodeQL provides coverage) Changed Loop over YAML prints Split appsets module Export ApplyPatch Register k8s schemes on demand Move pkg/layout to pkg/stack/layout for better organization Move pkg/fluxcd to pkg/k8s/fluxcd for better organization Yaml dir naming and proper marshalling Modernize errors package to follow Go best practices Modernize patch module with clean syntax and comprehensive tooling Rename cmd/patch to cmd/kure for better CLI naming Promote patch command from subcommand to top-level command Rename .patch files to .kpatch to avoid conflicts with diff patches Eliminate circular references in Node and Bundle structures Centralize validation logic across Kubernetes builders Standardize error handling to use KureError consistently Standardize function naming conventions across codebase Multi-CLI architecture and package naming standardization Implement clean workflow interface architecture Implement launcher base types with shared libraries Implement shared internal/gvk infrastructure Apply go fmt formatting to codebase Simplify Claude settings with symlink and expanded permissions Reorganize task files with numbered prefixes Migrate to GoReleaser v2 workflow Consolidate Makefile targets and enhance dev workflow Standardize validation patterns across packages Consolidate 4 GitHub workflows into 2 (ci.yml + release.yml) Consolidate 4 GitHub workflows into 2 (ci.yml + release.yml) Improve pkg/kubernetes testability and coverage Dependencies Align k8s.io/cli-runtime to v0.33.2 to match replace directive Bump tj-actions/changed-files Bump github.com/external-secrets/external-secrets Bump sigs.k8s.io/kustomize/api from 0.20.0 to 0.21.0 Bump sigs.k8s.io/yaml from 1.5.0 to 1.6.0 Implement centralized dependency version management Document blocked dependency updates for Go 1.25 Bump github.com/spf13/cobra in the go-safe group Bump github.com/cert-manager/cert-manager Update versions.yaml for cert-manager 1.16.5 Documentation Add project README Mention base resources and expose constructor Expand kio package documentation Expand kio documentation Expand fluxcd package overview Correct Flux auto-generated kustomization details Update README to reflect current repository state Add comprehensive architectural documentation Add comprehensive architectural documentation for generators Add comprehensive UX design document and recommendations Update project status and document remaining features Add comprehensive plugin architecture design Update CLAUDE.md with current project priorities and status Update CLAUDE.md with current project status and accurate metrics Update user documentation with current project state Add detailed explanation of CEL Validation Enhancement task Add comprehensive repository review and task management system Update task statuses after upstream rebase Add comprehensive puzl-cloud/kubesdk review with kure comparison Add task #1 for CEL validation enhancement Add workflow guidelines to tasks.md Remove references to non-existent demo-internals make target Add HPA and PDB builder tasks for Crane OAM support Add Crane integration documentation Add tasks README and update task 03 status Add quickstart guide Expand README with end-to-end examples Mark high-priority tasks 1-5, 23, 24 as completed Mark task #8 as completed Add comprehensive GoDoc documentation Mark task #10 as completed Mark task #6 as completed Mark tasks #7, #9, #11, #12 as completed Fixed Separate helper group comments Add missing unstructured import to patch CLI Correct type usage in generators package tests Resolve all layout module test failures Ensure all manifest directories have kustomization.yaml for GitOps compliance Resolve test failures in launcher module Resolve CLI test output capture issues Resolve all failing tests and improve TOML patch support Correct appworkload test to match ServiceConfig structure Update demo and kure commands to use new GVK-based ApplicationWrapper Resolve intermittent test failures in cmd/demo package Resolve stdout capture synchronization in demo tests Configure golangci-lint compatibility and resolve linting issues Correct YAML structure in CI workflow Add goimports to make fmt for CI/local parity Add GOPATH/bin to PATH in lint and fmt targets Upgrade Go to 1.24.11 to resolve security vulnerabilities Make max_depth_exceeded test deterministic Fix CVE in mapstructure and add workflow permissions Resolve repo issues across docs, CI, validation, and caching Propagate –strict flag to validator in kurel validate Update K8s compatibility matrix to test supported versions Remove K8s 0.33 from CI compatibility matrix Align mise.toml Go version with CI workflows Lower coverage threshold to 70% to match current main coverage Improve dependabot wildcard pattern matching in validation Block FluxCD major version updates in dependabot Testing Check errors Add runCluster coverage Add comprehensive test coverage for all packages Add comprehensive test coverage for FluxHelm internal package Skip demo integration tests in short mode Skip demo tests when examples directory is missing Fix data race in TestMainFunction Skip max_depth_exceeded test due to resolver bugs Add integration tests for stack generation workflows Add fuzz tests for patch parser Add Kubernetes version matrix to CI Add tests to improve coverage and fix Go version Add Phase 1 coverage for simple getters/setters Add Phase 2 parsing tests, reach 70.5% coverage Add Phase 3 validation tests, reach 100% validation coverage Add Phase 4 stack domain model tests Add Phase 5 layout integrator tests Add wrapper function tests, reach 94.8% gvk coverage Add setter function tests for internal packages Add comprehensive IO table and printer tests Add comprehensive appworkload internal tests Release V0.1.0-alpha.0",
    "description": "Changelog All notable changes to this project will be documented in this file.\n[0.1.0-alpha.1] - 2026-01-30 Fixed Run tests directly in release workflow instead of checking CI status [0.1.0-alpha.0] - 2026-01-30 Added Add storageclass helpers Add kustomize helpers Add flux source helpers Add helpers Add fluxcd builders package Add layout grouping and app file mode Support file- and dir-per-application layouts Implement OCI artifact separation in layout system Implement GitOps bootstrap and refactor demo system to data-driven architecture Implement comprehensive Kubernetes printer wrappers in io module Modernize error handling with custom error types and standardization Implement professional Cobra CLI with comprehensive command structure Implement comprehensive structured error handling system Add shorthand flags for common CLI options across all commands Complete kurel package system design documentation Implement package loader with hybrid error handling Implement variable resolver with cycle detection Implement patch processor with dependency resolution Implement schema generation and validation for launcher Complete Phase 4 - schema generation and validation Complete Phase 5 - output builder and local extensions Implement Phase 6 - CLI command integration Implement Phase 7 - comprehensive integration tests Implement GVK-based ApplicationConfig generator system Implement GVK-based versioning for stack module structs Implement GVK-based versioning for stack module structs Add comprehensive Makefile and CI/CD pipeline Complete KurelPackage generator implementation Enable Kubernetes schema inclusion in kurel CLI Implement fluent builder pattern Phase 1 Implement comprehensive interval validation for GitOps configurations Add Go version management tools Add fast precommit target for git hooks Add PodDisruptionBudget builder Add HorizontalPodAutoscaler builder Add combined-output mode to kure patch Add –diff option to kure patch Build Update Go to 1.24.12 to fix govulncheck vulnerabilities Automate changelog generation with git-cliff CI Add GitHub Action to refresh Go proxy on main branch commits Enforce Go version consistency in PR checks Remove Qodana workflow due to licensing issues Fix security scan action to use official gosec action Remove gosec security scan (CodeQL provides coverage) Changed Loop over YAML prints Split appsets module Export ApplyPatch Register k8s schemes on demand Move pkg/layout to pkg/stack/layout for better organization Move pkg/fluxcd to pkg/k8s/fluxcd for better organization Yaml dir naming and proper marshalling Modernize errors package to follow Go best practices Modernize patch module with clean syntax and comprehensive tooling Rename cmd/patch to cmd/kure for better CLI naming Promote patch command from subcommand to top-level command Rename .patch files to .kpatch to avoid conflicts with diff patches Eliminate circular references in Node and Bundle structures Centralize validation logic across Kubernetes builders Standardize error handling to use KureError consistently Standardize function naming conventions across codebase Multi-CLI architecture and package naming standardization Implement clean workflow interface architecture Implement launcher base types with shared libraries Implement shared internal/gvk infrastructure Apply go fmt formatting to codebase Simplify Claude settings with symlink and expanded permissions Reorganize task files with numbered prefixes Migrate to GoReleaser v2 workflow Consolidate Makefile targets and enhance dev workflow Standardize validation patterns across packages Consolidate 4 GitHub workflows into 2 (ci.yml + release.yml) Consolidate 4 GitHub workflows into 2 (ci.yml + release.yml) Improve pkg/kubernetes testability and coverage Dependencies Align k8s.io/cli-runtime to v0.33.2 to match replace directive Bump tj-actions/changed-files Bump github.com/external-secrets/external-secrets Bump sigs.k8s.io/kustomize/api from 0.20.0 to 0.21.0 Bump sigs.k8s.io/yaml from 1.5.0 to 1.6.0 Implement centralized dependency version management Document blocked dependency updates for Go 1.25 Bump github.com/spf13/cobra in the go-safe group Bump github.com/cert-manager/cert-manager Update versions.yaml for cert-manager 1.16.5 Documentation Add project README Mention base resources and expose constructor Expand kio package documentation Expand kio documentation Expand fluxcd package overview Correct Flux auto-generated kustomization details Update README to reflect current repository state Add comprehensive architectural documentation Add comprehensive architectural documentation for generators Add comprehensive UX design document and recommendations Update project status and document remaining features Add comprehensive plugin architecture design Update CLAUDE.md with current project priorities and status Update CLAUDE.md with current project status and accurate metrics Update user documentation with current project state Add detailed explanation of CEL Validation Enhancement task Add comprehensive repository review and task management system Update task statuses after upstream rebase Add comprehensive puzl-cloud/kubesdk review with kure comparison Add task #1 for CEL validation enhancement Add workflow guidelines to tasks.md Remove references to non-existent demo-internals make target Add HPA and PDB builder tasks for Crane OAM support Add Crane integration documentation Add tasks README and update task 03 status Add quickstart guide Expand README with end-to-end examples Mark high-priority tasks 1-5, 23, 24 as completed Mark task #8 as completed Add comprehensive GoDoc documentation Mark task #10 as completed Mark task #6 as completed Mark tasks #7, #9, #11, #12 as completed Fixed Separate helper group comments Add missing unstructured import to patch CLI Correct type usage in generators package tests Resolve all layout module test failures Ensure all manifest directories have kustomization.yaml for GitOps compliance Resolve test failures in launcher module Resolve CLI test output capture issues Resolve all failing tests and improve TOML patch support Correct appworkload test to match ServiceConfig structure Update demo and kure commands to use new GVK-based ApplicationWrapper Resolve intermittent test failures in cmd/demo package Resolve stdout capture synchronization in demo tests Configure golangci-lint compatibility and resolve linting issues Correct YAML structure in CI workflow Add goimports to make fmt for CI/local parity Add GOPATH/bin to PATH in lint and fmt targets Upgrade Go to 1.24.11 to resolve security vulnerabilities Make max_depth_exceeded test deterministic Fix CVE in mapstructure and add workflow permissions Resolve repo issues across docs, CI, validation, and caching Propagate –strict flag to validator in kurel validate Update K8s compatibility matrix to test supported versions Remove K8s 0.33 from CI compatibility matrix Align mise.toml Go version with CI workflows Lower coverage threshold to 70% to match current main coverage Improve dependabot wildcard pattern matching in validation Block FluxCD major version updates in dependabot Testing Check errors Add runCluster coverage Add comprehensive test coverage for all packages Add comprehensive test coverage for FluxHelm internal package Skip demo integration tests in short mode Skip demo tests when examples directory is missing Fix data race in TestMainFunction Skip max_depth_exceeded test due to resolver bugs Add integration tests for stack generation workflows Add fuzz tests for patch parser Add Kubernetes version matrix to CI Add tests to improve coverage and fix Go version Add Phase 1 coverage for simple getters/setters Add Phase 2 parsing tests, reach 70.5% coverage Add Phase 3 validation tests, reach 100% validation coverage Add Phase 4 stack domain model tests Add Phase 5 layout integrator tests Add wrapper function tests, reach 94.8% gvk coverage Add setter function tests for internal packages Add comprehensive IO table and printer tests Add comprehensive appworkload internal tests Release V0.1.0-alpha.0",
    "tags": [],
    "title": "Releases",
    "uri": "/changelog/releases/index.html"
  },
  {
    "breadcrumb": "Go Kure",
    "content": "Getting Started with Kure This section helps you get up and running with Kure quickly.\nInstallation Install the Kure CLI tools using Go:\ngo install github.com/go-kure/kure/cmd/kure@latest go install github.com/go-kure/kure/cmd/kurel@latest Verify the installation:\nkure version kurel version Next Steps Follow the Quickstart guide Explore the Packages documentation Read the Architecture overview Try the Examples",
    "description": "Getting Started with Kure This section helps you get up and running with Kure quickly.\nInstallation Install the Kure CLI tools using Go:\ngo install github.com/go-kure/kure/cmd/kure@latest go install github.com/go-kure/kure/cmd/kurel@latest Verify the installation:\nkure version kurel version Next Steps Follow the Quickstart guide Explore the Packages documentation Read the Architecture overview Try the Examples",
    "tags": [],
    "title": "Getting Started",
    "uri": "/getting-started/index.html"
  },
  {
    "breadcrumb": "Go Kure",
    "content": "Architecture Deep dive into Kure’s design and architecture.\nArchitecture Details - Core architecture and design principles Plugin Architecture - Plugin system design UX Design - User experience design decisions",
    "description": "Architecture Deep dive into Kure’s design and architecture.\nArchitecture Details - Core architecture and design principles Plugin Architecture - Plugin system design UX Design - User experience design decisions",
    "tags": [],
    "title": "Architecture",
    "uri": "/architecture/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Packages \u003e Stack \u003e Generators",
    "content": "Generators Architecture Guide Overview The generators package implements a versioned, extensible system for generating Kubernetes resources and other artifacts from declarative configurations. This document explains the architectural patterns and conventions used throughout the generators subsystem.\nDirectory Structure generators/ ├── DESIGN.md # High-level design documentation ├── ARCHITECTURE.md # This file - architectural patterns ├── registry.go # Global registry for all generators ├── interfaces.go # Common interfaces ├── doc.go # Package documentation │ ├── appworkload/ # AppWorkload generator │ ├── v1alpha1.go # Version v1alpha1 API types and registration │ └── internal/ # Shared implementation │ └── appworkload.go │ ├── fluxhelm/ # FluxHelm generator │ ├── v1alpha1.go # Version v1alpha1 API types and registration │ └── internal/ # Shared implementation │ └── fluxhelm.go │ └── kurelpackage/ # KurelPackage generator (in development) └── v1alpha1.go # Version v1alpha1 API types and registration └── internal/ # (Will be added when implementation is complete) The Version/Internal Pattern Why This Pattern? Each generator follows a consistent pattern that separates API versioning from implementation:\nVersion Files (v1alpha1.go, v1beta1.go, v1.go)\nDefine the YAML/JSON schema as Go structs Register with the GVK system Implement thin wrapper methods Delegate actual work to internal package Internal Package (internal/*.go)\nContains the actual implementation logic Shared across all versions Complex business logic and resource generation Not exposed in public API Benefits 1. Version Evolution // v1alpha1.go type ConfigV1Alpha1 struct { Replicas int32 `yaml:\"replicas\"` } // v1beta1.go - can add fields type ConfigV1Beta1 struct { Replicas *int32 `yaml:\"replicas\"` // Now optional Scaling *ScalingConfig `yaml:\"scaling\"` } // Both use the same internal implementation func (c *ConfigV1Alpha1) Generate() { internal.GenerateResources(...) } 2. Code Reuse The internal package is shared across versions, avoiding duplication:\n// internal/appworkload.go func GenerateResources(config *Config) ([]*client.Object, error) { // 500+ lines of complex logic used by all versions } 3. Clean API Surface Version types are pure data structures:\n// Clean, declarative API in v1alpha1.go type ConfigV1Alpha1 struct { Workload WorkloadType Replicas int32 Container ContainerConfig } // Complex logic hidden in internal/ func createDeployment(...) { /* complex logic */ } func createService(...) { /* complex logic */ } 4. Independent Testing // Test API parsing func TestV1Alpha1Parsing(t *testing.T) { } // Test implementation logic separately func TestDeploymentGeneration(t *testing.T) { } Implementation Examples Example 1: AppWorkload Generator Structure:\nappworkload/v1alpha1.go - 100 lines of type definitions and registration appworkload/internal/appworkload.go - 500+ lines of implementation The version file (v1alpha1.go):\npackage appworkload import ( \"github.com/go-kure/kure/pkg/stack\" \"github.com/go-kure/kure/pkg/stack/generators/appworkload/internal\" ) // API types - clean data structures type ConfigV1Alpha1 struct { Workload internal.WorkloadType Replicas int32 Container internal.ContainerConfig } // Thin wrapper that delegates to internal func (c *ConfigV1Alpha1) Generate(app *stack.Application) ([]*client.Object, error) { return internal.GenerateResources(\u0026internal.Config{ Workload: c.Workload, Replicas: c.Replicas, Container: c.Container, }, app) } The internal implementation (internal/appworkload.go):\npackage internal // Actual implementation - complex logic func GenerateResources(cfg *Config, app *stack.Application) ([]*client.Object, error) { var resources []*client.Object // Complex deployment generation switch cfg.Workload { case DeploymentWorkload: deployment := createDeployment(cfg, app) resources = append(resources, \u0026deployment) case StatefulSetWorkload: sts := createStatefulSet(cfg, app) resources = append(resources, \u0026sts) } // Service generation with port mapping if len(cfg.Services) \u003e 0 { for _, svc := range cfg.Services { service := createService(svc, app) resources = append(resources, \u0026service) } } // Ingress, volumes, etc... return resources, nil } Example 2: FluxHelm Generator Similar pattern with different domain logic:\nVersion file focuses on API:\ntype ConfigV1Alpha1 struct { Chart ChartConfig Source SourceConfig Values interface{} } Internal handles Flux-specific logic:\nfunc GenerateResources(cfg *Config) ([]*client.Object, error) { // Generate HelmRelease // Handle different source types (Helm, OCI, Git, S3) // Process values and dependencies } Example 3: KurelPackage Generator (Future) Currently just has API definitions, but will need internal package for:\n// Future internal/kurelpackage.go package internal func GeneratePackageFiles(cfg *Config) (map[string][]byte, error) { // Read resource files from disk // Apply include/exclude patterns // Process patches // Handle values schemas // Build package structure // Generate OCI artifacts } When to Use Internal Package Use Internal Package When: Implementation is more than ~100 lines Complex business logic exists Multiple versions will share the logic You need helper functions not part of the API Resource generation involves multiple steps Don’t Use Internal Package When: Generator is very simple (\u003c 50 lines) It’s a prototype or proof of concept The entire logic fits cleanly in the Generate method No version evolution is expected Adding a New Generator Step 1: Create the generator directory mkdir -p pkg/stack/generators/mygenerator Step 2: Define v1alpha1 API types // pkg/stack/generators/mygenerator/v1alpha1.go package mygenerator type ConfigV1Alpha1 struct { generators.BaseMetadata `yaml:\",inline\"` // Your fields here } func init() { // Register with GVK system } func (c *ConfigV1Alpha1) Generate(app *stack.Application) ([]*client.Object, error) { // For simple generators, implement here // For complex ones, delegate to internal package } Step 3: Add internal package (if needed) // pkg/stack/generators/mygenerator/internal/mygenerator.go package internal func GenerateResources(cfg *Config) ([]*client.Object, error) { // Complex implementation here } Step 4: Add tests // pkg/stack/generators/mygenerator/v1alpha1_test.go func TestMyGeneratorV1Alpha1(t *testing.T) { // Test YAML parsing // Test generation } Step 5: Document Update this ARCHITECTURE.md if you introduce new patterns Add examples to DESIGN.md Include sample YAML in your test files Best Practices Keep version files thin - They should only define types and delegate Share types via internal - Common types go in internal package Version independence - Each version can have different fields Forward compatibility - Design v1alpha1 with future versions in mind Comprehensive tests - Test both API parsing and generation logic Document YAML schema - Include examples in tests and comments Migration Between Versions When evolving versions:\n// v1alpha1 → v1beta1 migration func (c *ConfigV1Alpha1) ConvertTo(version string) (interface{}, error) { if version == \"v1beta1\" { return \u0026ConfigV1Beta1{ // Map fields, provide defaults for new fields }, nil } return nil, fmt.Errorf(\"unsupported version %s\", version) } Testing Strategy 1. API Tests (version files) YAML parsing Field validation Version conversion 2. Implementation Tests (internal package) Resource generation logic Edge cases Error handling 3. Integration Tests End-to-end with actual Kubernetes objects Multi-generator scenarios Layout generation Future Considerations Plugin System Eventually support external generators:\n// External generators could register themselves func RegisterExternalGenerator(gvk GVK, factory GeneratorFactory) { externalRegistry.Register(gvk, factory) } Code Generation Consider generating version boilerplate:\n# Future tool kurel generate generator --name MyGenerator --version v1alpha1 Conclusion The version/internal pattern provides a clean separation between API versioning and implementation logic. This enables:\nClean API evolution Code reuse across versions Testable implementations Maintainable codebase Follow this pattern for all new generators unless there’s a compelling reason to deviate.\nLast updated: 2025-01-08",
    "description": "Generators Architecture Guide Overview The generators package implements a versioned, extensible system for generating Kubernetes resources and other artifacts from declarative configurations. This document explains the architectural patterns and conventions used throughout the generators subsystem.\nDirectory Structure generators/ ├── DESIGN.md # High-level design documentation ├── ARCHITECTURE.md # This file - architectural patterns ├── registry.go # Global registry for all generators ├── interfaces.go # Common interfaces ├── doc.go # Package documentation │ ├── appworkload/ # AppWorkload generator │ ├── v1alpha1.go # Version v1alpha1 API types and registration │ └── internal/ # Shared implementation │ └── appworkload.go │ ├── fluxhelm/ # FluxHelm generator │ ├── v1alpha1.go # Version v1alpha1 API types and registration │ └── internal/ # Shared implementation │ └── fluxhelm.go │ └── kurelpackage/ # KurelPackage generator (in development) └── v1alpha1.go # Version v1alpha1 API types and registration └── internal/ # (Will be added when implementation is complete) The Version/Internal Pattern Why This Pattern? Each generator follows a consistent pattern that separates API versioning from implementation:",
    "tags": [],
    "title": "Architecture",
    "uri": "/packages/stack/generators/architecture/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Packages \u003e Launcher",
    "content": "Kurel Package System — Design Specification This document describes the structure, behavior, and purpose of a Kurel package (Kubernetes Resources Launcher), which encapsulates a reusable, versionable application for Kubernetes. Kurel builds on Kure’s patch engine to enable declarative configuration of application instances without templates or overlays.\nGoals Enable reusable packaging of Kubernetes applications Declarative customization via parameters + patches No Helm-style templating or Kustomize overlays Strong schema validation and deployment safety Compatible with GitOps workflows (Flux/ArgoCD) Multi-namespace and multi-phase deployment support Design Philosophy “Kurel just generates YAML” - Kurel is a declarative system for generating Kubernetes manifests with validation and customization capabilities. It is not a runtime system, orchestrator, or complex package manager.\nCore Principles Explicit over Implicit - Always prefer explicit configuration Flexible but Validated - Maximum flexibility with comprehensive validation GitOps Compatible - Generate proper Kubernetes manifests for GitOps workflows No Templating - Use patches instead of complex template logic Deterministic Output - Same inputs always produce same outputs Package Directory Structure A Kurel package is a directory containing Kubernetes resources, parameters, patches, and metadata.\nmy-app.kurel/ ├── parameters.yaml # All variables and package metadata ├── resources/ # Base Kubernetes manifests (one GVK per file) │ ├── deployment.yaml │ ├── service.yaml │ └── namespaces.yaml ├── patches/ # Modular patches with conditional enabling │ ├── 00-base.kpatch # Global patterns (explicit) │ ├── features/ │ │ ├── 10-monitoring.kpatch │ │ ├── 10-monitoring.yaml # Patch metadata │ │ └── 20-ingress.kpatch │ └── profiles/ │ ├── 10-development.kpatch │ └── 10-production.kpatch ├── schemas/ # Auto-generated validation schemas │ └── parameters.schema.json ├── examples/ # Example parameter configurations │ └── production.yaml └── README.md # Package documentation my-app.local.kurel/ # User extensions (optional) ├── patches/ # Additional user patches │ └── 50-custom.kpatch └── parameters.yaml # Parameter overrides Core Concepts 1. parameters.yaml - Variables and Metadata Contains all package metadata and configurable variables with a structured hierarchy:\n# Package metadata (fixed key) kurel: name: prometheus-operator version: 0.68.0 appVersion: 0.68.0 description: \"Prometheus Operator for monitoring\" home: https://github.com/prometheus-operator/prometheus-operator # Global defaults (fixed key) global: labels: app.kubernetes.io/name: \"${kurel.name}\" app.kubernetes.io/managed-by: \"kurel\" resources: requests: cpu: 100m memory: 128Mi limits: cpu: 1000m memory: 1Gi # Feature configuration (author-defined) monitoring: enabled: false retention: 30d persistence: enabled: true size: 10Gi 2. resources/ - Base Kubernetes Manifests Contains the base Kubernetes resources. These are standard Kubernetes YAML files without templating. Multi-document YAML files are supported and will be parsed into separate Resource objects during loading.\n3. patches/ - Modular Patch System Multiple patch files organized in subdirectories with numeric ordering:\n# patches/features/10-monitoring.kpatch [deployment.prometheus.spec.template.spec] securityContext.runAsNonRoot: true [deployment.prometheus.spec.template.spec.containers.0] resources: \"${global.resources}\" image.tag: \"${kurel.appVersion}\" Each patch can have corresponding metadata:\n# patches/features/10-monitoring.yaml enabled: \"${monitoring.enabled}\" description: \"Adds Prometheus monitoring capabilities\" requires: - \"features/05-metrics-base.kpatch\" conflicts: - \"features/20-lightweight-monitoring.kpatch\" 4. GitOps Deployment Phases Resources can be annotated for phase organization. Note that Kurel only generates YAML - actual deployment ordering is handled by GitOps tools.\napiVersion: v1 kind: Namespace metadata: name: monitoring annotations: kurel.gokure.dev/install-phase: \"pre-install\" kurel.gokure.dev/wait-for-ready: \"true\" Phases (for YAML organization):\npre-install - CRDs, namespaces, RBAC main - Primary application resources (default) post-install - Monitoring, backups, optional components 5. Schema Generation and Validation Schemas are auto-generated from parameters and Kubernetes API tracing:\nkurel schema generate my-app.kurel/ # Generates schemas/parameters.schema.json Validation includes:\nParameter value validation against schema Variable reference validation in patches Kubernetes resource validation Patch dependency and conflict checking 6. User Extensions (.local.kurel) Users can extend packages without modifying the original:\nmy-app.local.kurel/ ├── patches/50-custom.kpatch # Additional patches └── parameters.yaml # Parameter overrides Variable System Variable Reference Syntax Dot notation: ${section.subsection.value} Nested references: ${monitoring.serviceMonitor.enabled} can reference ${monitoring.enabled} Metadata access: ${kurel.appVersion} for package metadata Variable Resolution Load package parameters.yaml Override with local my-app.local.kurel/parameters.yaml Resolve all variable references Apply type validation and casting Fixed Top-Level Keys kurel: - Package metadata (name, version, description, etc.) global: - Default values applied across resources via base patches Everything else - Author-defined variable hierarchy Patch System Patch Discovery and Ordering Discovery: patches/**/*.kpatch Ordering: Numeric prefixes (10-, 20-) then alphabetical Organization: Subdirectories preferred for better organization Conditional Patch Enabling Simple boolean expressions: enabled: \"${monitoring.enabled}\" Auto-enable dependencies: requires: automatically enables referenced patches Conflict detection: conflicts: prevents incompatible patches Patch Processing Flow Discover all patch files Load metadata and evaluate conditions Build dependency graph and auto-enable required patches Validate no conflicts exist Apply patches in order: Package patches (by numeric prefix) Local patches (by numeric prefix, can override package patches) Important: Patches MUST apply successfully or return an error. There are no silent failures in patch application.\nMulti-Namespace Support Flexible Namespace Handling Full flexibility - Resources can target any namespaces Namespace creation control - global.namespaces.create flag Cross-namespace references - Supported and validated Namespace Creation Pattern # parameters.yaml global: namespaces: create: true exclude: [\"kube-system\", \"default\"] # patches/00-base.kpatch [namespace.*] enabled: \"${global.namespaces.create}\" metadata.labels: \"${global.labels}\" Build and Validation CLI Commands # Validate package and parameters kurel validate my-app.kurel/ --values custom.yaml # Generate schemas from package kurel schema generate my-app.kurel/ # Build final manifests (dry-run to stdout by default) kurel build my-app.kurel/ --values custom.yaml # Build with output to files kurel build my-app.kurel/ --values custom.yaml --output ./manifests/ # Build with verbose patch debugging kurel build my-app.kurel/ --values custom.yaml --verbose # Show package information kurel info my-app.kurel/ Generated Output Structure output/ ├── pre-install/ # Phase 1 resources │ ├── kustomization.yaml │ └── namespaces.yaml ├── main/ # Phase 2 resources (depends on pre-install) │ ├── kustomization.yaml │ ├── deployments.yaml │ └── services.yaml └── post-install/ # Phase 3 resources (depends on main) ├── kustomization.yaml └── monitoring.yaml Design Constraints ❌ No templating or embedded logic in YAML ❌ No overlays or merging strategies (use patches) ❌ No conditionals or loops in YAML ❌ No complex package dependencies (handled at GitOps level) ❌ No direct secret creation (use references to external-secrets instead) ✅ Variable substitution allowed for keys in parameters.yaml ✅ All patches are deterministic, declarative, and validated ✅ Multi-namespace and multi-phase organization supported ✅ Dry-run mode via stdout output (default behavior) Use Cases Application Packaging - Reusable packages for common applications Platform Engineering - Standardized app bundles with customization boundaries GitOps Deployments - Generate manifests compatible with Flux/ArgoCD Multi-Environment - Same package deployed with different configurations Complex Applications - Multi-namespace apps with ordered deployment phases Future Extensions Enhanced Schema Generation - Better Kubernetes API tracing and CRD support Package Registry - Central repository for sharing kurel packages Advanced Validation - Integration with policy engines and security scanners IDE Integration - Language servers and editor support Testing Framework - Unit and integration testing for packages Plugin Architecture - Custom validators and extensions (future consideration) Observability - Metrics, logging, and debugging tools (future consideration)",
    "description": "Kurel Package System — Design Specification This document describes the structure, behavior, and purpose of a Kurel package (Kubernetes Resources Launcher), which encapsulates a reusable, versionable application for Kubernetes. Kurel builds on Kure’s patch engine to enable declarative configuration of application instances without templates or overlays.\nGoals Enable reusable packaging of Kubernetes applications Declarative customization via parameters + patches No Helm-style templating or Kustomize overlays Strong schema validation and deployment safety Compatible with GitOps workflows (Flux/ArgoCD) Multi-namespace and multi-phase deployment support Design Philosophy “Kurel just generates YAML” - Kurel is a declarative system for generating Kubernetes manifests with validation and customization capabilities. It is not a runtime system, orchestrator, or complex package manager.",
    "tags": [],
    "title": "Design",
    "uri": "/packages/launcher/design/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Packages \u003e Patch",
    "content": "Error Handling Philosophy and Guidelines This document establishes the unified error handling philosophy for the Kure patch module, reconciling the current mix of strict and graceful approaches.\nCore Philosophy: Graceful by Default, Strict When Critical The patch module should prioritize operational continuity while maintaining data integrity. This means:\nWarn and Continue: Missing targets, optional fields, or recoverable issues generate warnings but don’t stop processing Fail Fast: Data corruption, syntax errors, or critical system issues cause immediate failures Rich Context: All errors include actionable information for debugging and resolution Error Categories and Handling Category 1: Graceful Warnings (Continue Processing) These issues generate warnings but allow processing to continue:\nMissing Patch Targets // Current inconsistent behavior - should be unified if targetResource == nil { log.Printf(\"WARNING: Patch target '%s' not found, skipping patch: %s\", target, patch.Path) continue // Don't fail the entire operation } Optional Field Access // Field doesn't exist but could be created if !fieldExists { log.Printf(\"WARNING: Field '%s' doesn't exist, creating with default value\", fieldPath) // Create field with appropriate default } Type Inference Fallbacks // Can't infer type, fall back to string if inferredType == nil { log.Printf(\"WARNING: Could not infer type for '%s', using string value\", fieldName) value = stringValue } Selector Resolution Issues // Key-value selector doesn't match any items if matchingItems == 0 { log.Printf(\"WARNING: Selector '%s' matched no items, skipping patch\", selector) continue } Category 2: Critical Failures (Stop Processing) These issues require immediate failure with detailed context:\nSyntax Errors // Malformed patch syntax if !isValidPatchSyntax(patchLine) { return fmt.Errorf(\"invalid patch syntax at line %d: %s\\n\" + \"Expected format: path[selector]: value\\n\" + \"Example: spec.containers[name=main].image: nginx:latest\", lineNum, patchLine) } Data Corruption Risks // Would corrupt existing data structure if wouldCorruptData(operation, target) { return fmt.Errorf(\"patch operation would corrupt data structure:\\n\" + \"Path: %s\\n\" + \"Operation: %s\\n\" + \"Target type: %T\\n\" + \"Suggested fix: Use correct selector syntax\", path, operation, target) } System/IO Failures // File system or network issues if err := writeOutput(data); err != nil { return fmt.Errorf(\"failed to write output to %s: %w\\n\" + \"Check file permissions and available disk space\", outputPath, err) } Resource Schema Violations // Kubernetes API violations if !isValidKubernetesField(resource, field) { return fmt.Errorf(\"invalid Kubernetes field for %s %s: %s\\n\" + \"Valid fields: %v\\n\" + \"API Version: %s\", resource.GetKind(), resource.GetName(), field, validFields, resource.GetAPIVersion()) } Implementation Guidelines 1. Error Context Requirements Every error must include:\nWhat went wrong (specific operation/field) Where it occurred (file, line, resource) Why it failed (root cause) How to fix it (actionable suggestion) // Good: Rich context with actionable information return fmt.Errorf(\"failed to apply patch to container selector:\\n\" + \"Resource: %s/%s\\n\" + \"Path: %s\\n\" + \"Selector: %s\\n\" + \"Reason: %v\\n\" + \"Available containers: %v\\n\" + \"Suggestion: Check container names match exactly\", resource.GetKind(), resource.GetName(), patch.Path, patch.Selector, err, containerNames) // Bad: Vague, no context return fmt.Errorf(\"patch failed: %w\", err) 2. Warning Format Standards All warnings should follow a consistent format:\nfunc logWarning(category, context, issue, suggestion string) { if os.Getenv(\"KURE_DEBUG\") != \"\" { log.Printf(\"WARNING [%s]: %s - %s\\n Suggestion: %s\", category, context, issue, suggestion) } } // Usage logWarning(\"PATCH_TARGET\", \"deployment.app containers[name=sidecar]\", \"container not found, skipping patch\", \"verify container name matches spec.template.spec.containers[].name\") 3. Debug Logging Integration Enhanced debug information when KURE_DEBUG=1:\nfunc debugPatchOperation(op PatchOp, resource *unstructured.Unstructured) { if os.Getenv(\"KURE_DEBUG\") != \"\" { log.Printf(\"DEBUG: Applying patch operation:\\n\" + \" Resource: %s/%s (%s)\\n\" + \" Operation: %s\\n\" + \" Path: %s\\n\" + \" Selector: %s\\n\" + \" Value: %v (%T)\", resource.GetKind(), resource.GetName(), resource.GetAPIVersion(), op.Op, op.Path, op.Selector, op.Value, op.Value) } } 4. Batch Processing Error Handling When processing multiple patches:\nfunc processPatchBatch(patches []PatchOp) error { var criticalErrors []error var warnings []string for i, patch := range patches { if err := processPatch(patch); err != nil { if isCriticalError(err) { criticalErrors = append(criticalErrors, fmt.Errorf(\"patch %d: %w\", i+1, err)) } else { warnings = append(warnings, fmt.Sprintf(\"patch %d: %v\", i+1, err)) } } } // Log all warnings for _, warning := range warnings { log.Printf(\"WARNING: %s\", warning) } // Fail only if critical errors occurred if len(criticalErrors) \u003e 0 { return fmt.Errorf(\"critical errors in patch processing:\\n%v\", criticalErrors) } return nil } Migration Strategy Phase 1: Identify Current Inconsistencies Audit all error returns in the patch module Categorize each error type (graceful vs critical) Document current behavior vs desired behavior Phase 2: Implement Unified Error Types // Define error types for consistent handling type PatchError struct { Type ErrorType Context string Message string Cause error Fix string } type ErrorType int const ( ErrorTypeWarning ErrorType = iota // Log and continue ErrorTypeCritical // Stop processing ErrorTypeDebug // Only show in debug mode ) Phase 3: Update Error Handling Replace inconsistent error handling with unified approach Add comprehensive test coverage for error scenarios Update CLI to respect the graceful/critical distinction Phase 4: Documentation and Examples Update all examples to show proper error handling Create troubleshooting guide with common error scenarios Document debug logging capabilities Testing Error Scenarios Every error handling path must be tested:\nfunc TestPatchErrorHandling(t *testing.T) { tests := []struct { name string patch PatchOp expectError bool expectWarning bool errorType ErrorType }{ { name: \"missing target - graceful\", patch: PatchOp{Path: \"nonexistent.field\", Op: \"replace\"}, expectError: false, expectWarning: true, errorType: ErrorTypeWarning, }, { name: \"invalid syntax - critical\", patch: PatchOp{Path: \"malformed[[[syntax\", Op: \"replace\"}, expectError: true, expectWarning: false, errorType: ErrorTypeCritical, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { // Test error handling behavior }) } } This unified approach ensures predictable behavior while maintaining operational robustness and providing excellent debugging support.",
    "description": "Error Handling Philosophy and Guidelines This document establishes the unified error handling philosophy for the Kure patch module, reconciling the current mix of strict and graceful approaches.\nCore Philosophy: Graceful by Default, Strict When Critical The patch module should prioritize operational continuity while maintaining data integrity. This means:\nWarn and Continue: Missing targets, optional fields, or recoverable issues generate warnings but don’t stop processing Fail Fast: Data corruption, syntax errors, or critical system issues cause immediate failures Rich Context: All errors include actionable information for debugging and resolution Error Categories and Handling Category 1: Graceful Warnings (Continue Processing) These issues generate warnings but allow processing to continue:",
    "tags": [],
    "title": "Error Handling",
    "uri": "/packages/patch/error-handling/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Examples",
    "content": "Generator Examples This directory contains examples of the GVK-based generator system for Kure.\nOverview Kure uses a Group, Version, Kind (GVK) pattern similar to Kubernetes for identifying generator types. Each generator:\nHas a unique GVK identifier Implements the ApplicationConfig interface Can generate specific types of Kubernetes resources Available Generators AppWorkload (generators.gokure.dev/v1alpha1) Creates standard Kubernetes workloads (Deployments, StatefulSets, DaemonSets) with associated resources.\nExample: appworkload.yaml This example creates:\nA Deployment with 3 replicas A LoadBalancer Service An Ingress resource Proper resource limits and volume mounts FluxHelm (generators.gokure.dev/v1alpha1) Creates Flux HelmRelease resources with their source configurations.\nExamples:\nfluxhelm.yaml - Traditional Helm repository source fluxhelm-oci.yaml - OCI registry source These examples demonstrate:\nHelmRepository and OCIRepository sources Values customization Release configuration options Flux-specific settings (interval, timeout, suspend) Usage To parse and generate resources from these examples:\npackage main import ( \"fmt\" \"io/ioutil\" \"gopkg.in/yaml.v3\" \"github.com/go-kure/kure/pkg/stack\" _ \"github.com/go-kure/kure/pkg/stack/generators/appworkload\" _ \"github.com/go-kure/kure/pkg/stack/generators/fluxhelm\" ) func main() { // Read YAML file data, err := ioutil.ReadFile(\"appworkload.yaml\") if err != nil { panic(err) } // Parse into ApplicationWrapper var wrapper stack.ApplicationWrapper if err := yaml.Unmarshal(data, \u0026wrapper); err != nil { panic(err) } // Convert to Application app := wrapper.ToApplication() // Generate Kubernetes resources resources, err := app.Config.Generate(app) if err != nil { panic(err) } fmt.Printf(\"Generated %d resources\\n\", len(resources)) } Creating New Generators To create a new generator type:\nCreate a package under pkg/stack/generators/\u003ctype\u003e/ Implement the ApplicationConfig interface Register with GVK in init() Add version files (v1alpha1.go, etc.) Example structure:\ngenerators/ └── mytype/ ├── v1alpha1.go # Version implementation ├── internal/ # Internal logic │ └── mytype.go └── doc.go # Documentation GVK Convention All generators follow the pattern:\nGroup: generators.gokure.dev Version: v1alpha1, v1beta1, v1 Kind: Generator type name (e.g., AppWorkload, FluxHelm) This allows for:\nClear type identification Version evolution Backward compatibility Schema validation",
    "description": "Generator Examples This directory contains examples of the GVK-based generator system for Kure.\nOverview Kure uses a Group, Version, Kind (GVK) pattern similar to Kubernetes for identifying generator types. Each generator:\nHas a unique GVK identifier Implements the ApplicationConfig interface Can generate specific types of Kubernetes resources Available Generators AppWorkload (generators.gokure.dev/v1alpha1) Creates standard Kubernetes workloads (Deployments, StatefulSets, DaemonSets) with associated resources.\nExample: appworkload.yaml This example creates:\nA Deployment with 3 replicas A LoadBalancer Service An Ingress resource Proper resource limits and volume mounts FluxHelm (generators.gokure.dev/v1alpha1) Creates Flux HelmRelease resources with their source configurations.",
    "tags": [],
    "title": "Generators",
    "uri": "/examples/generators/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Development",
    "content": "GitHub Workflows Documentation This document provides an overview of all GitHub Actions workflows used in the kure project.\nLast Updated: 2026-02-12\nWorkflow Summary Workflow File Triggers Purpose CI ci.yml push, PR, schedule, manual Comprehensive testing, linting, building, security Auto-Rebase auto-rebase.yml push to main Rebase all open PRs when main is updated Release release.yml version tags GoReleaser-based release with CI validation CI Workflow File: .github/workflows/ci.yml Name: CI\nTriggers Push to: main, develop, release/* Pull requests to: main, develop Schedule: 4am UTC daily (catch external changes) Manual dispatch Concurrency Uses github.sha to avoid duplicate runs:\nSame commit won’t run CI twice (e.g., PR merge → push to main) Different commits run independently concurrency: group: ci-${{ github.sha }} cancel-in-progress: false Job Dependency Graph ┌─────────────────┐ │ lint │ ← Fast checks: go-version, fmt, tidy, vet, lint └────────┬────────┘ │ ┌────┴────┐ ▼ ▼ ┌───────┐ ┌───────────┐ │ test │ │ security │ ← Tests + govulncheck (parallel) └───┬───┘ └───────────┘ │ ▼ ┌───────────────────┐ │ coverage-check │ ← 80% threshold enforcement └─────────┬─────────┘ │ ┌─────┴─────┐ ▼ ▼ ┌───────┐ ┌────────────┐ │ build │ │ k8s-compat │ ← Build artifacts + K8s matrix └───┬───┘ └────────────┘ │ ▼ ┌─────────────────────┐ │ cross-platform │ ← Only on main/release branches └─────────┬───────────┘ │ ▼ ┌─────────────────────────────────────────────────────────────┐ │ mirror-to-gitlab (main push only, after all checks) │ └─────────────────────────────────────────────────────────────┘ PR-only jobs (parallel, no blocking): ┌──────────────┐ ┌─────────────────┐ ┌────────────┐ │ rebase-check │ │ analyze-changes │ │ docs-check │ └──────────────┘ └─────────────────┘ └────────────┘ Jobs Detail Job Check Name Timeout Dependencies Purpose validate lint 5 min - Go version check, fmt, tidy, vet, lint test test 15 min validate Unit tests, race tests, coverage security Security 10 min validate govulncheck, outdated deps, sensitive file check coverage-check Coverage Check 5 min test 80% threshold, Codecov upload, PR comment build build 10 min coverage-check Build kure, kurel, demo k8s-compat K8s Compatibility 15 min coverage-check K8s 0.34, 0.35 compatibility matrix cross-platform Cross-Platform Build 15 min build linux/darwin/windows × amd64/arm64 (main/release only) rebase-check rebase-check 2 min - Verify PR branch is rebased on main (PR only) analyze-changes Analyze Changes 5 min - Changed files analysis, breaking change warnings (PR only) docs-check Docs Check 5 min - API changes need docs check (PR only) mirror-to-gitlab Mirror to GitLab 5 min build, security, k8s-compat, cross-platform, docs-build Push main and tags to GitLab mirror; fails on divergence (main only) Configuration Go Version: 1.24.13 Golangci-lint Version: v1.64.8 Coverage Threshold: 80% K8s Versions: 0.34, 0.35 Platforms: linux/amd64, linux/arm64, darwin/amd64, darwin/arm64, windows/amd64 Features gotestfmt - Nice formatted test output Fail fast - Jobs depend on validate, so lint failure stops everything Artifact sharing - Coverage uploaded as artifact, reused by coverage-check PR comments - Coverage report comment on PRs Skip draft PRs - if: github.event.pull_request.draft == false Sensitive file check - Warn about potential secrets in code Matrix fail-fast: false - K8s and cross-platform continue if one fails Release Workflow File: .github/workflows/release.yml Name: Release\nTriggers Push tags: v* (e.g., v1.0.0, v0.1.0-alpha.0) Jobs check-ci - Verify CI passed for this commit (waits up to 5 min) validate - Strict tag format, changelog, and version progression validation goreleaser - Cross-platform builds using GoReleaser v2 post-release - Go proxy refresh Configuration Go Version: 1.24.13 Build Tool: GoReleaser v2 Platforms: linux/amd64, linux/arm64, darwin/amd64, darwin/arm64, windows/amd64, windows/arm64 Tag Format: ^v[0-9]+\\.[0-9]+\\.[0-9]+(-alpha\\.[0-9]+|-beta\\.[0-9]+|-rc\\.[0-9]+)?$ Changelog: Required (must have ## v0.1.0 section) CI Status Check Release workflow verifies CI passed before releasing:\ncheck-ci: name: Verify CI passed steps: - name: Check CI status for this commit run: | # Wait up to 5 minutes for CI to complete for i in {1..30}; do STATUS=$(gh api repos/.../commits/$COMMIT_SHA/status --jq '.state') if [ \"$STATUS\" = \"success\" ]; then exit 0; fi if [ \"$STATUS\" = \"failure\" ]; then exit 1; fi sleep 10 done Local Release Management # Preview release plan make release TYPE=alpha # Execute release (creates commits + tag) make release-do TYPE=alpha # Push tag to trigger CI git push origin v0.1.0-alpha.0 Auto-Rebase Workflow File: .github/workflows/auto-rebase.yml Name: Auto-Rebase\nTriggers Push to main (runs after every merge to main) Purpose Automatically rebases all open PRs targeting main when main is updated. This mirrors the GitLab auto-rebase CI template used in other Wharf repositories.\nHow It Works Uses peter-evans/rebase@v4 to:\nFind all open PRs targeting main Rebase each PR branch onto the latest main Force-push the rebased branch (triggers CI re-run) Skip PRs with conflicts (reports them without failing) Configuration Excluded labels: dependencies (Dependabot manages its own branches) Excluded drafts: yes (no point rebasing work-in-progress) Fork protection: only runs on go-kure/kure (forks lack the required secret) Concurrency: cancel-in-progress: true (newer main state supersedes) Authentication Requires AUTO_REBASE_PAT repository secret — a fine-grained PAT with:\nRepository: go-kure/kure only Permissions: Contents: Read+Write, Pull requests: Read A PAT is required because pushes made with GITHUB_TOKEN do not trigger subsequent workflow runs. The PAT ensures CI re-runs on rebased branches.\nTest Jobs in CI Job Matrix Command Uses Makefile? test - go test -json -v ./... ✅ (deps) test - make test-race ✅ test - make test-coverage ✅ Test Targets in Makefile Target Command Used in CI? In precommit? test go test -timeout 30s ./... ✅ ✅ test-race go test -race -timeout 30s ./... ✅ - test-coverage go test -coverprofile=... ./... ✅ - test-integration go test -tags=integration -timeout 5m ./... - - vuln govulncheck ./... ✅ - CI vs Pre-commit Target Tasks Use Case precommit fmt, tidy, lint, test Fast local checks (~10s) ci deps, fmt, tidy, lint, vet, test, test-race, test-coverage, test-integration, build, vuln Comprehensive CI pipeline (~2min) Configuration Standards Go Version All workflows use Go 1.24.13 consistently, defined via environment variable:\nenv: GO_VERSION: '1.24.13' Caching Most workflows use Go module caching:\n- name: Cache Go modules uses: actions/cache@v4 with: path: | ~/.cache/go-build ~/go/pkg/mod key: ${{ runner.os }}-go-${{ hashFiles('**/go.sum') }} restore-keys: | ${{ runner.os }}-go- Branch Patterns Release branches: release/* (note: not releases/*) Development branches: main, develop Estimated CI Time Scenario Before (4 workflows) After (2 workflows) PR opened ~8 min (duplicate work) ~4 min Push to main ~5 min ~4 min PR merge ~5 min (full re-run) ~0 min (same SHA, skipped) Maintenance Notes When adding/modifying workflows: Update this document with changes Version updates: Run make sync-go-version to update Go version in all files Version check: Run make check-go-version to verify consistency Action versions: Keep GitHub Actions up to date (currently using v4-v6) See Also Makefile - Local development commands CLAUDE.md - Development guidelines mise.toml - Local tool version management",
    "description": "GitHub Workflows Documentation This document provides an overview of all GitHub Actions workflows used in the kure project.\nLast Updated: 2026-02-12\nWorkflow Summary Workflow File Triggers Purpose CI ci.yml push, PR, schedule, manual Comprehensive testing, linting, building, security Auto-Rebase auto-rebase.yml push to main Rebase all open PRs when main is updated Release release.yml version tags GoReleaser-based release with CI validation CI Workflow File: .github/workflows/ci.yml Name: CI\nTriggers Push to: main, develop, release/* Pull requests to: main, develop Schedule: 4am UTC daily (catch external changes) Manual dispatch Concurrency Uses github.sha to avoid duplicate runs:",
    "tags": [],
    "title": "GitHub Workflows",
    "uri": "/development/github-workflows/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Packages",
    "content": "Patch Package The patch package implements a JSONPath-based declarative patching system for Kubernetes resources. It allows you to modify resources using a simple, powerful patch language that maintains YAML structure and comments.\nDocumentation Design - Patch system design Error Handling - Error handling approach Patch Engine Design - Engine internals Path Resolution - How paths are resolved API Reference pkg.go.dev/github.com/go-kure/kure/pkg/patch",
    "description": "Patch Package The patch package implements a JSONPath-based declarative patching system for Kubernetes resources. It allows you to modify resources using a simple, powerful patch language that maintains YAML structure and comments.\nDocumentation Design - Patch system design Error Handling - Error handling approach Patch Engine Design - Engine internals Path Resolution - How paths are resolved API Reference pkg.go.dev/github.com/go-kure/kure/pkg/patch",
    "tags": [],
    "title": "Patch",
    "uri": "/packages/patch/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Architecture",
    "content": "Plugin Architecture Design for Kure Overview This document outlines the design for implementing a plugin architecture that allows external generators to extend Kure’s capabilities. The plugin system would enable users to create custom generators for specific needs (Terraform, Pulumi, custom CRDs, etc.) while maintaining type safety and security.\nCurrent Generator Architecture Kure currently uses a static registration system where generators implement the ApplicationConfig interface:\ntype ApplicationConfig interface { Generate(*Application) ([]*client.Object, error) } Built-in generators like AppWorkload and FluxHelm register themselves via init() functions using the GVK (Group, Version, Kind) system:\nfunc init() { gvkObj := gvk.GVK{ Group: \"generators.gokure.dev\", Version: \"v1alpha1\", Kind: \"AppWorkload\", } factory := func() stack.ApplicationConfig { return \u0026ConfigV1Alpha1{} } stack.RegisterApplicationConfig(gvkObj, factory) } Plugin Architecture Requirements Core Components Plugin Interface: Standard contract for all plugins Plugin Loader: Dynamic loading from shared libraries Plugin Manager: Lifecycle management and registry Security Layer: Digital signature verification Registry Integration: Seamless integration with existing GVK system Development SDK: Helper library for plugin authors CLI Integration: Commands for plugin management Key Features Dynamic Loading: Runtime loading from .so/.dylib files Type Safety: Strong typing through Go interfaces and GVK system Security: Digital signature verification for authenticity Extensibility: Support for generators, validators, and documentation providers Isolation: Clear interface boundaries between plugins and core Discovery: CLI tools for plugin management SDK Support: Simplified development experience Detailed Implementation Plan 1. Plugin Interface Definition // pkg/stack/plugins/interface.go package plugins import ( \"context\" \"github.com/go-kure/kure/internal/gvk\" \"github.com/go-kure/kure/pkg/stack\" ) // Plugin represents a loadable generator plugin type Plugin interface { // Metadata Name() string Version() string Description() string Author() string // Registration SupportedGVKs() []gvk.GVK Register(registry Registry) error // Lifecycle Initialize(ctx context.Context) error Shutdown(ctx context.Context) error // Health HealthCheck(ctx context.Context) error } // Registry allows plugins to register their generators type Registry interface { RegisterGenerator(gvk gvk.GVK, factory stack.ApplicationConfigFactory) error RegisterValidator(gvk gvk.GVK, validator Validator) error RegisterDocumentationProvider(gvk gvk.GVK, provider DocumentationProvider) error } // Validator provides validation for plugin-specific configurations type Validator interface { Validate(config stack.ApplicationConfig) error GetSchema() ([]byte, error) // JSON Schema } // DocumentationProvider provides documentation and examples type DocumentationProvider interface { GetDocumentation() string GetExamples() []Example GetUsageGuide() string } type Example struct { Name string Description string Config string // YAML example } 2. Plugin Loader and Manager // pkg/stack/plugins/loader.go package plugins import ( \"context\" \"fmt\" \"plugin\" \"sync\" \"time\" ) // LoaderConfig configures plugin loading behavior type LoaderConfig struct { PluginDirs []string // Directories to search for plugins AllowUnsigned bool // Allow unsigned plugins (development only) TrustedSigners []string // Trusted plugin signer keys Timeout time.Duration } // Manager manages the lifecycle of plugins type Manager struct { config LoaderConfig plugins map[string]Plugin registry *PluginRegistry mu sync.RWMutex } func NewManager(config LoaderConfig) *Manager { return \u0026Manager{ config: config, plugins: make(map[string]Plugin), registry: NewPluginRegistry(), } } // LoadPlugins discovers and loads all plugins from configured directories func (m *Manager) LoadPlugins(ctx context.Context) error { m.mu.Lock() defer m.mu.Unlock() for _, dir := range m.config.PluginDirs { if err := m.loadFromDirectory(ctx, dir); err != nil { return fmt.Errorf(\"failed to load plugins from %s: %w\", dir, err) } } return nil } func (m *Manager) loadPlugin(ctx context.Context, path string) error { // Verify plugin signature if security is enabled if !m.config.AllowUnsigned { if err := m.verifyPluginSignature(path); err != nil { return fmt.Errorf(\"plugin signature verification failed: %w\", err) } } // Load the plugin p, err := plugin.Open(path) if err != nil { return fmt.Errorf(\"failed to open plugin: %w\", err) } // Look for the plugin entry point symbol, err := p.Lookup(\"NewPlugin\") if err != nil { return fmt.Errorf(\"plugin missing NewPlugin function: %w\", err) } // Cast to plugin factory function factory, ok := symbol.(func() Plugin) if !ok { return fmt.Errorf(\"NewPlugin has wrong signature\") } // Create and initialize plugin instance pluginInstance := factory() if err := pluginInstance.Initialize(ctx); err != nil { return fmt.Errorf(\"plugin initialization failed: %w\", err) } // Register plugin generators if err := pluginInstance.Register(m.registry); err != nil { return fmt.Errorf(\"plugin registration failed: %w\", err) } // Store plugin m.plugins[pluginInstance.Name()] = pluginInstance return nil } // Additional methods: GetPlugin, ListPlugins, Shutdown... 3. Plugin Registry Integration // pkg/stack/plugins/registry.go package plugins import ( \"fmt\" \"sync\" \"github.com/go-kure/kure/internal/gvk\" \"github.com/go-kure/kure/pkg/stack\" ) // PluginRegistry manages plugin-registered generators type PluginRegistry struct { generators map[gvk.GVK]stack.ApplicationConfigFactory validators map[gvk.GVK]Validator docProviders map[gvk.GVK]DocumentationProvider plugins map[gvk.GVK]string // Maps GVK to plugin name mu sync.RWMutex } func NewPluginRegistry() *PluginRegistry { return \u0026PluginRegistry{ generators: make(map[gvk.GVK]stack.ApplicationConfigFactory), validators: make(map[gvk.GVK]Validator), docProviders: make(map[gvk.GVK]DocumentationProvider), plugins: make(map[gvk.GVK]string), } } func (r *PluginRegistry) RegisterGenerator(gvkObj gvk.GVK, factory stack.ApplicationConfigFactory) error { r.mu.Lock() defer r.mu.Unlock() if _, exists := r.generators[gvkObj]; exists { return fmt.Errorf(\"generator for GVK %s already registered\", gvkObj) } r.generators[gvkObj] = factory // Also register with the global stack registry stack.RegisterApplicationConfig(gvkObj, factory) return nil } // Additional methods for validators, documentation providers, queries... 4. Security and Verification // pkg/stack/plugins/security.go package plugins import ( \"crypto\" \"crypto/rsa\" \"crypto/sha256\" \"crypto/x509\" \"encoding/pem\" \"fmt\" \"os\" ) // verifyPluginSignature verifies a plugin's digital signature func (m *Manager) verifyPluginSignature(pluginPath string) error { // Look for signature file sigPath := pluginPath + \".sig\" sigData, err := os.ReadFile(sigPath) if err != nil { return fmt.Errorf(\"signature file not found: %w\", err) } // Read plugin binary pluginData, err := os.ReadFile(pluginPath) if err != nil { return fmt.Errorf(\"failed to read plugin: %w\", err) } // Hash the plugin hash := sha256.Sum256(pluginData) // Verify signature against trusted signers for _, signerKey := range m.config.TrustedSigners { if err := m.verifySignature(hash[:], sigData, signerKey); err == nil { return nil // Valid signature found } } return fmt.Errorf(\"no valid signature found\") } func (m *Manager) verifySignature(hash, signature []byte, pubKeyPath string) error { // Read and parse public key keyData, err := os.ReadFile(pubKeyPath) if err != nil { return fmt.Errorf(\"failed to read public key: %w\", err) } // Parse PEM format block, _ := pem.Decode(keyData) if block == nil { return fmt.Errorf(\"invalid PEM format\") } // Parse public key pubKey, err := x509.ParsePKIXPublicKey(block.Bytes) if err != nil { return fmt.Errorf(\"failed to parse public key: %w\", err) } rsaPubKey, ok := pubKey.(*rsa.PublicKey) if !ok { return fmt.Errorf(\"only RSA keys supported\") } // Verify signature return rsa.VerifyPKCS1v15(rsaPubKey, crypto.SHA256, hash, signature) } 5. Plugin Development SDK // pkg/stack/plugins/sdk/base.go package sdk import ( \"context\" \"github.com/go-kure/kure/internal/gvk\" \"github.com/go-kure/kure/pkg/stack/plugins\" ) // BasePlugin provides a default implementation of common plugin functionality type BasePlugin struct { name string version string description string author string supportedGVKs []gvk.GVK } func NewBasePlugin(name, version, description, author string) *BasePlugin { return \u0026BasePlugin{ name: name, version: version, description: description, author: author, } } func (p *BasePlugin) Name() string { return p.name } func (p *BasePlugin) Version() string { return p.version } func (p *BasePlugin) Description() string { return p.description } func (p *BasePlugin) Author() string { return p.author } func (p *BasePlugin) SupportedGVKs() []gvk.GVK { return p.supportedGVKs } func (p *BasePlugin) AddSupportedGVK(gvkObj gvk.GVK) { p.supportedGVKs = append(p.supportedGVKs, gvkObj) } // Default implementations that can be overridden func (p *BasePlugin) Initialize(ctx context.Context) error { return nil } func (p *BasePlugin) Shutdown(ctx context.Context) error { return nil } func (p *BasePlugin) HealthCheck(ctx context.Context) error { return nil } // Register must be implemented by concrete plugins func (p *BasePlugin) Register(registry plugins.Registry) error { panic(\"Register method must be implemented by concrete plugin\") } 6. CLI Integration // pkg/cmd/kure/plugins.go package main import ( \"context\" \"fmt\" \"os\" \"text/tabwriter\" \"time\" \"github.com/spf13/cobra\" \"github.com/go-kure/kure/pkg/stack/plugins\" ) func newPluginsCmd() *cobra.Command { cmd := \u0026cobra.Command{ Use: \"plugins\", Short: \"Manage generator plugins\", } cmd.AddCommand(newPluginsListCmd()) cmd.AddCommand(newPluginsInstallCmd()) cmd.AddCommand(newPluginsUninstallCmd()) cmd.AddCommand(newPluginsInfoCmd()) return cmd } func newPluginsListCmd() *cobra.Command { return \u0026cobra.Command{ Use: \"list\", Short: \"List installed plugins\", RunE: func(cmd *cobra.Command, args []string) error { manager := getPluginManager() ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second) defer cancel() if err := manager.LoadPlugins(ctx); err != nil { return fmt.Errorf(\"failed to load plugins: %w\", err) } plugins := manager.ListPlugins() if len(plugins) == 0 { fmt.Println(\"No plugins installed.\") return nil } w := tabwriter.NewWriter(os.Stdout, 0, 0, 3, ' ', 0) fmt.Fprintln(w, \"NAME\\tVERSION\\tAUTHOR\\tDESCRIPTION\") for _, plugin := range plugins { fmt.Fprintf(w, \"%s\\t%s\\t%s\\t%s\\n\", plugin.Name(), plugin.Version(), plugin.Author(), plugin.Description()) } return w.Flush() }, } } func getPluginManager() *plugins.Manager { config := plugins.LoaderConfig{ PluginDirs: []string{\"/usr/local/lib/kure/plugins\", \"~/.kure/plugins\", \"./plugins\"}, AllowUnsigned: false, // Should be configurable TrustedSigners: []string{\"/etc/kure/trusted-signers.pem\"}, Timeout: 30 * time.Second, } return plugins.NewManager(config) } Example Plugin Implementation Terraform Generator Plugin // examples/plugins/terraform-generator/main.go package main import ( \"context\" \"sigs.k8s.io/controller-runtime/pkg/client\" \"github.com/go-kure/kure/internal/gvk\" \"github.com/go-kure/kure/pkg/stack\" \"github.com/go-kure/kure/pkg/stack/plugins\" \"github.com/go-kure/kure/pkg/stack/plugins/sdk\" ) // TerraformPlugin generates Kubernetes resources that deploy Terraform configurations type TerraformPlugin struct { *sdk.BasePlugin } func NewPlugin() plugins.Plugin { plugin := \u0026TerraformPlugin{ BasePlugin: sdk.NewBasePlugin( \"terraform-generator\", \"v1.0.0\", \"Generates Kubernetes Jobs that run Terraform configurations\", \"Example Corp \u003cplugins@example.com\u003e\", ), } plugin.AddSupportedGVK(gvk.GVK{ Group: \"generators.example.com\", Version: \"v1alpha1\", Kind: \"TerraformConfig\", }) return plugin } func (p *TerraformPlugin) Register(registry plugins.Registry) error { gvkObj := gvk.GVK{ Group: \"generators.example.com\", Version: \"v1alpha1\", Kind: \"TerraformConfig\", } factory := func() stack.ApplicationConfig { return \u0026TerraformConfig{} } return registry.RegisterGenerator(gvkObj, factory) } // TerraformConfig represents a Terraform configuration to be deployed type TerraformConfig struct { APIVersion string `yaml:\"apiVersion\" json:\"apiVersion\"` Kind string `yaml:\"kind\" json:\"kind\"` // Terraform-specific configuration Module string `yaml:\"module\" json:\"module\"` Variables map[string]string `yaml:\"variables,omitempty\" json:\"variables,omitempty\"` BackendConfig BackendConfig `yaml:\"backend\" json:\"backend\"` RequiredVersion string `yaml:\"requiredVersion,omitempty\" json:\"requiredVersion,omitempty\"` } type BackendConfig struct { Type string `yaml:\"type\" json:\"type\"` Config map[string]string `yaml:\"config\" json:\"config\"` } func (c *TerraformConfig) Generate(app *stack.Application) ([]*client.Object, error) { // Implementation would create: // - Kubernetes Job to run terraform apply // - ConfigMap with terraform files // - Secret for backend credentials // - ServiceAccount with appropriate RBAC return []*client.Object{ // Job, ConfigMap, Secret, ServiceAccount, etc. }, nil } func (c *TerraformConfig) GetAPIVersion() string { return c.APIVersion } func (c *TerraformConfig) GetKind() string { return c.Kind } // Required for Go plugins func main() {} // Empty main required for buildmode=plugin Plugin Build and Distribution Building Plugins # Makefile for building plugins .PHONY: plugin plugin: go build -buildmode=plugin -o terraform-generator.so ./examples/plugins/terraform-generator/ # Plugin signing sign-plugin: openssl dgst -sha256 -sign private-key.pem -out terraform-generator.so.sig terraform-generator.so # Installation script install-plugin: sudo cp terraform-generator.so /usr/local/lib/kure/plugins/ sudo cp terraform-generator.so.sig /usr/local/lib/kure/plugins/ Usage Example # Using a plugin-provided generator apiVersion: generators.example.com/v1alpha1 kind: TerraformConfig metadata: name: infrastructure spec: module: \"./terraform/modules/vpc\" variables: region: \"us-west-2\" environment: \"production\" backend: type: \"s3\" config: bucket: \"terraform-state-bucket\" key: \"infrastructure/terraform.tfstate\" region: \"us-west-2\" # CLI plugin management kure plugins list kure plugins install terraform-generator.so kure plugins info terraform-generator Implementation Phases Phase 1: Core Infrastructure Plugin interface definition Basic plugin loader without security Plugin registry integration Simple CLI commands Phase 2: Security and Validation Digital signature verification Plugin validation framework Configuration schema support Security documentation Phase 3: Developer Experience Plugin development SDK Example plugins Documentation and tutorials Testing utilities Phase 4: Advanced Features Plugin dependency management Hot-reloading support Plugin marketplace integration Monitoring and metrics Security Considerations Code Execution: Plugins run in the same process space Digital Signatures: Required for production use Sandboxing: Consider future isolation mechanisms Resource Limits: Memory and CPU constraints Network Access: Plugin network restrictions File System: Limited file system access Benefits Extensibility: Users can create domain-specific generators Community: Ecosystem of third-party plugins Maintenance: Reduces core maintenance burden Innovation: Faster iteration on new generator types Adoption: Easier integration with existing toolchains Risks and Mitigation Security: Mitigated by signature verification and sandboxing Stability: Mitigated by plugin isolation and error handling Performance: Mitigated by lazy loading and resource limits Compatibility: Mitigated by versioned interfaces and testing Status This task is currently postponed pending completion of higher-priority features like:\nKurelPackage generator completion ArgoCD bootstrap implementation OpenAPI schema generation The design is ready for implementation when development resources become available.",
    "description": "Plugin Architecture Design for Kure Overview This document outlines the design for implementing a plugin architecture that allows external generators to extend Kure’s capabilities. The plugin system would enable users to create custom generators for specific needs (Terraform, Pulumi, custom CRDs, etc.) while maintaining type safety and security.\nCurrent Generator Architecture Kure currently uses a static registration system where generators implement the ApplicationConfig interface:\ntype ApplicationConfig interface { Generate(*Application) ([]*client.Object, error) } Built-in generators like AppWorkload and FluxHelm register themselves via init() functions using the GVK (Group, Version, Kind) system:",
    "tags": [],
    "title": "Plugin Architecture",
    "uri": "/architecture/plugin-design/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Packages \u003e Stack",
    "content": "Stack Module Status Last Updated: 2025-01-08\nOverall Status: Beta - Core functionality complete, additional features in progress\nCompleted Features ✅ Core Infrastructure ✅ Hierarchical Domain Model: Cluster → Node → Bundle → Application ✅ GitOps Workflow Support: Flux and ArgoCD implementations ✅ Layout Generation: Flexible manifest organization strategies ✅ GVK System: Full Group/Version/Kind support for all types GVK Implementation ✅ Internal GVK Package (internal/gvk)\nGeneric registry with Go generics Type-safe factory patterns YAML unmarshaling with automatic type detection Version conversion framework ✅ ApplicationConfig Generators (generators.gokure.dev)\nAppWorkload v1alpha1 - Deployments, StatefulSets, DaemonSets FluxHelm v1alpha1 - HelmRelease with multiple source types ✅ Stack Types (stack.gokure.dev/v1alpha1)\nClusterV1Alpha1 - Cluster configuration with GitOps NodeV1Alpha1 - Hierarchical node structure BundleV1Alpha1 - Application bundles with dependencies In Progress 🔄 Kurel Package Generator 🔄 KurelPackage Generator (generators.gokure.dev/v1alpha1) Generate kurel packages from stack configurations Support for package dependencies and extensions Integration with launcher module for package validation Planned Features 📋 High Priority 1. Kurel Package Generator 🎯 Create a new generator that produces kurel packages:\napiVersion: generators.gokure.dev/v1alpha1 kind: KurelPackage metadata: name: my-package namespace: kurel-system spec: package: name: my-app version: 1.0.0 description: \"My application package\" resources: - source: ./manifests includes: [\"*.yaml\"] excludes: [\"*-test.yaml\"] patches: - target: kind: Deployment name: my-app patch: | - op: replace path: /spec/replicas value: 3 values: schema: ./values.schema.json defaults: ./values.yaml extensions: - name: monitoring when: .Values.monitoring.enabled resources: - source: ./monitoring dependencies: - name: base-config version: \"\u003e=1.0.0\" Implementation Tasks:\nCreate pkg/stack/generators/kurelpackage/v1alpha1.go Implement spec types for package metadata, resources, patches Add values schema support Implement extension conditions Create package dependency resolution Generate kurel.yaml and package structure Add validation against launcher module Write comprehensive tests Add integration with kurel build command Medium Priority 2. Additional Generators CronJobGenerator - Kubernetes CronJob resources ConfigMapGenerator - ConfigMaps from files/literals SecretGenerator - Secrets with encoding support NetworkPolicyGenerator - Network policies KustomizationGenerator - Flux Kustomizations 3. Version Migration Implement Convertible interface for all types Add conversion webhooks Create migration paths (v1alpha1 → v1beta1 → v1) Version compatibility matrix 4. CLI Integration kurel validate - Validate GVK resources kurel convert - Convert between versions kurel list-kinds - List registered types kurel generate - Generate resources from GVK Low Priority 5. Schema Validation OpenAPI schema generation Runtime validation rules Custom validation functions Schema documentation generation 6. Registry Enhancements Plugin system for external generators Type deprecation warnings Type aliases Registry introspection API 7. Performance Optimizations Parsing cache Lazy generator loading Parallel YAML processing Memory usage optimization Known Issues 🐛 Limited Error Context: YAML parsing errors don’t include line numbers No Version Negotiation: Can’t automatically upgrade old configs Missing Validation: No schema validation for generator configs Documentation Needs 📚 User guide for writing GVK YAML Generator development guide API reference documentation Migration guide from old format Example configurations Testing Coverage 🧪 Current coverage:\n✅ Unit tests for all GVK types ✅ Integration tests for generators ✅ YAML parsing tests ⏳ Multi-version migration tests ⏳ Performance benchmarks ⏳ Fuzz testing Dependencies 📦 Key dependencies that need monitoring:\nsigs.k8s.io/controller-runtime - v0.19.3 github.com/fluxcd/pkg/apis - v0.38.0 gopkg.in/yaml.v3 - v3.0.1 Contributing 🤝 Priority areas for contribution:\nKurel package generator implementation Additional generator types Documentation and examples Test coverage improvements Performance optimizations Release Milestones 🚀 v0.9.0 (Current) ✅ Core GVK infrastructure ✅ Basic generators (AppWorkload, FluxHelm) ✅ Stack versioning v0.10.0 (Next) Kurel package generator Version migration support CLI integration v1.0.0 (Target) All planned generators Complete documentation Production-ready validation Performance optimized This status document is updated regularly to reflect the current state of development.",
    "description": "Stack Module Status Last Updated: 2025-01-08\nOverall Status: Beta - Core functionality complete, additional features in progress\nCompleted Features ✅ Core Infrastructure ✅ Hierarchical Domain Model: Cluster → Node → Bundle → Application ✅ GitOps Workflow Support: Flux and ArgoCD implementations ✅ Layout Generation: Flexible manifest organization strategies ✅ GVK System: Full Group/Version/Kind support for all types GVK Implementation ✅ Internal GVK Package (internal/gvk)\nGeneric registry with Go generics Type-safe factory patterns YAML unmarshaling with automatic type detection Version conversion framework ✅ ApplicationConfig Generators (generators.gokure.dev)",
    "tags": [],
    "title": "Status",
    "uri": "/packages/stack/status/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Packages \u003e Launcher",
    "content": "Kurel Package System - Comprehensive Design Details This document captures the complete design discussion, decisions, alternatives considered, and rationale for the Kurel (Kubernetes Resources Launcher) package system. It serves as a comprehensive record of all design choices made during the extensive design iteration process.\nDesign Philosophy \u0026 Core Principles Fundamental Philosophy “Kurel just generates YAML” - This principle guided every design decision. Kurel is not a runtime system, orchestrator, or complex package manager. It’s a declarative system for generating Kubernetes manifests with validation and customization capabilities.\nCore Design Principles Explicit over Implicit - Always prefer explicit configuration over hidden defaults Flexible but Validated - Don’t constrain unnecessarily but validate what we can GitOps Compatible - Generate proper Kubernetes manifests for GitOps workflows No Templating Engines - Use patches instead of complex template logic Deterministic Output - Same inputs always produce same outputs Key Design Constraints ❌ No templating or embedded logic in YAML ❌ No overlays or merging strategies (use patches instead) ❌ No conditionals or loops in YAML ❌ No composition or shared libraries between packages ✅ Variable substitution allowed, but only for keys in parameters.yaml ✅ All patches are deterministic, declarative, and validated Research Insights from Existing Systems The kurel design was informed by extensive research into existing package management and deployment systems. Here are the key insights that shaped our decisions:\nHelm Charts Analysis Structure patterns adopted:\nChart.yaml → Inspired our metadata in parameters.yaml under kurel: key values.yaml → Our parameters.yaml serves similar purpose templates/ → Our resources/ for base manifests Patterns rejected:\nComplex templating with {{ }} syntax → Use patches instead Dependencies in Chart.yaml → Handle at GitOps level .helmignore → Not needed for kurel’s simpler model Docker Compose Learnings Patterns adopted:\ndocker-compose.override.yml → Our .local.kurel extension pattern Environment variable substitution → Our ${variable} syntax Service profiles → Our conditional patch enabling Insights gained:\nOverride files work well for user customization Simple variable substitution is sufficient for most cases Profiles enable different deployment configurations Terraform Modules Study Structure patterns adopted:\nvariables.tf → Our parameter documentation approach README.md → Documentation importance Clear input/output interface → Our parameters/generated manifests Patterns adapted:\nVersion constraints → Decided to handle at GitOps level Module composition → Kept packages self-contained Kustomize Patterns Concepts adopted:\npatches/ directory structure Declarative customization without templating Base + overlay pattern → Our package + .local.kurel pattern Improvements made:\nBetter patch organization with subdirectories Conditional patch application vs static overlays Integrated variable system vs separate files ArgoCD Applications Research Patterns adopted:\nSync waves → Our install phase annotations Application dependencies → Our phase-based deployment Health checks → Our wait-for-ready annotations GitOps integration insights:\nNeed for deployment ordering in complex applications Importance of GitOps-native manifest generation Value of dependency management at orchestration level Key Research Conclusions No single system does everything well - Each has strengths for specific use cases Templating complexity - Most users struggle with complex template syntax Override patterns work - Docker Compose override model is intuitive Validation is crucial - All successful systems provide parameter validation GitOps compatibility - Modern systems must integrate well with GitOps workflows Package Structure Evolution Final Package Structure my-app.kurel/ ├── parameters.yaml # All variables + metadata ├── resources/ # Base Kubernetes manifests (one GVK per file) │ ├── deployment.yaml │ ├── service.yaml │ └── namespaces.yaml ├── patches/ # Modular patches with numeric ordering │ ├── 00-base.kpatch # Standard global patterns (explicit) │ ├── features/ │ │ ├── 10-monitoring.kpatch │ │ ├── 10-monitoring.yaml # Patch metadata │ │ └── 20-ingress.kpatch │ └── profiles/ │ ├── 10-development.kpatch │ └── 10-production.kpatch ├── schemas/ # Auto-generated validation │ └── parameters.schema.json ├── examples/ # Example configurations │ └── production.yaml └── README.md # Documentation my-app.local.kurel/ # User extensions (optional) ├── patches/ # Additional patches only │ └── 50-custom.kpatch └── parameters.yaml # Override parameter values Original Structure (Rejected) The initial design from the existing DESIGN.md included:\nmy-app.kurel/ ├── resources/ ├── parameters.kpatch # Single patch file ├── config.kpatch # Multi-resource patch set ├── config.schema.json # JSONSchema for validation ├── instance.schema.json # Schema for instance-level fields ├── instance.yaml # External instance configuration └── README.md Evolution \u0026 Rejected Alternatives Directory Name Chosen: my-app.kurel/ - Clear package identity Considered: .kurel suffix vs directory structure - decided on directory for better organization Patch Organization Original: Single parameters.kpatch file Intermediate: config.kpatch for multi-resource patches Final: Multiple .kpatch files in patches/ subdirectories Rationale: Better organization, modular patches, easier to maintain Configuration Files Original: Separate instance.yaml external to package Final: parameters.yaml within package, .local.kurel for overrides Rationale: Simpler structure, explicit override pattern Metadata Location Considered: Separate kurel.yaml for package metadata Final: Metadata in parameters.yaml under kurel: key Rationale: Single source of truth, metadata available as variables Parameters System Design Final Parameter Structure # parameters.yaml # Package metadata (fixed key) kurel: name: prometheus-operator version: 0.68.0 appVersion: 0.68.0 description: \"Prometheus Operator creates/manages Prometheus clusters\" home: https://github.com/prometheus-operator/prometheus-operator keywords: [\"monitoring\", \"prometheus\", \"operator\"] maintainers: - name: \"Prometheus Team\" email: \"prometheus-operator@googlegroups.com\" # Global defaults (fixed key) global: labels: app.kubernetes.io/name: \"${kurel.name}\" app.kubernetes.io/version: \"${kurel.appVersion}\" app.kubernetes.io/managed-by: \"kurel\" annotations: kurel.gokure.dev/package: \"${kurel.name}\" kurel.gokure.dev/version: \"${kurel.version}\" resources: requests: cpu: 100m memory: 128Mi limits: cpu: 1000m memory: 1Gi securityContext: runAsNonRoot: true runAsUser: 65534 fsGroup: 65534 imagePullPolicy: IfNotPresent nodeSelector: {} tolerations: [] # Author-defined variables (any structure) monitoring: enabled: false serviceMonitor: enabled: \"${monitoring.enabled}\" # Nested reference interval: 30s image: registry: quay.io repository: prometheus-operator/prometheus-operator tag: \"v${kurel.appVersion}\" # Reference to metadata pullPolicy: IfNotPresent persistence: enabled: true size: 10Gi storageClass: \"\" resources: controller: requests: cpu: 200m # Override global default memory: 256Mi Variable Reference System Syntax: ${section.subsection.value} with dot notation Nested references supported: ${monitoring.serviceMonitor.enabled} can reference ${monitoring.enabled} Metadata references: Variables can reference package metadata like ${kurel.appVersion} Global patterns: ${global.*} for cross-cutting defaults Fixed Top-Level Keys kurel: Key (Package Metadata) Purpose: Package identification and metadata, also available as variables Required fields:\nname: Package name (used in generated resources) version: Package version appVersion: Upstream application version Optional fields:\ndescription: Human-readable description home: URL to project homepage keywords: Array of keywords for discovery maintainers: Array of maintainer objects global: Key (Default Values) Purpose: Default values applied across all resources via base patches Common patterns:\nlabels: Applied to all resources annotations: Applied to all resources resources: Default resource requests/limits securityContext: Default security settings nodeSelector: Default node selection tolerations: Default tolerations imagePullPolicy: Default image pull policy ExtendedValue Evolution Original ExtendedValue Design (Rejected) We initially considered an “ExtendedValue” struct to provide validation metadata:\n# Original ExtendedValue approach persistence: size: _schema: extended # Explicit marker value: 10Gi type: string pattern: \"^[0-9]+[KMGT]i$\" description: \"Storage size in Kubernetes format\" required: true minimum: null maximum: null Detection Problem The challenge was distinguishing between ExtendedValue objects and regular nested configuration:\n# Is this an ExtendedValue or regular config? database: credentials: value: secret-ref # Could be ExtendedValue.value or just a config key type: kubernetes # Could be ExtendedValue.type or just a config key Final Decision: Direct Schema Generation Chosen approach: Generate JSON Schema directly from parameters.yaml + K8s API tracing Rationale:\nAvoids duplication between ExtendedValue and schema Leverages existing Kubernetes validation Cleaner parameter files Standard JSON Schema tooling support Parameter Override System Resolution Order Package parameters.yaml - Base values and metadata Local my-app.local.kurel/parameters.yaml - User overrides (highest priority) Error if variable not found Local Override Pattern Same filename: parameters.yaml in both locations Rejected alternatives: values.yaml, overrides.yaml, local.yaml Rationale: Consistency and simplicity Patch System Architecture Patch Discovery \u0026 Organization File Organization patches/ ├── 00-base.kpatch # Global patterns (explicit) ├── features/ # Feature-specific patches │ ├── 10-monitoring.kpatch │ ├── 10-monitoring.yaml # Patch metadata │ ├── 20-ingress.kpatch │ └── 30-persistence.kpatch ├── profiles/ # Environment profiles │ ├── 10-development.kpatch │ ├── 20-staging.kpatch │ └── 30-production.kpatch └── resources/ # Resource-specific patches ├── 10-limits-small.kpatch ├── 20-limits-medium.kpatch └── 30-limits-large.kpatch Naming Convention Evolution Initial idea: Required prefixes like feature-, profile- Final decision: Numeric prefixes only: NN-descriptive-name.kpatch Rationale: Flexibility without unnecessary constraints Numeric Prefix Guidelines (Rejected) We considered prescriptive numeric ranges:\n10-19: Core features/settings 20-29: Additional features 30-39: Advanced/optional features 90-99: Override/cleanup patches Decision: No prescribed ranges - users decide their own numbering system Rationale: Avoid artificial constraints, let package authors organize as they see fit\nDirectory Structure Preferences Allow: Direct patches in patches/ root Prefer: Subdirectories for organization Rationale: Flexibility with gentle guidance toward better organization Patch Discovery \u0026 Ordering Discovery Pattern Glob: patches/**/*.kpatch Processing order: Alphabetical by full path (directory + filename) Numeric sorting: 10- comes before 20- comes before 9- (string sort) Example Processing Order patches/00-base.kpatch patches/features/10-monitoring.kpatch patches/features/20-ingress.kpatch patches/profiles/10-development.kpatch patches/resources/10-limits-small.kpatch Conditional Patch Enabling Patch Metadata Files Each patch can have a corresponding .yaml file with the same base name:\n# features/10-monitoring.yaml enabled: \"${monitoring.enabled}\" # Simple boolean expression description: \"Adds Prometheus monitoring sidecars and annotations\" requires: # Auto-enable these patches - \"features/05-metrics-base.kpatch\" - \"features/15-monitoring-rbac.kpatch\" conflicts: # Cannot be enabled together - \"features/25-lightweight-monitoring.kpatch\" Enabling Expression Language Chosen: Simple boolean variables only Syntax: \"${variable.name}\" evaluates to true/false Rejected: Complex expressions like \"${environment == 'production'}\" Rationale: Keep it simple, avoid expression language complexity Dependency Resolution Evolution Option A: Requirements as Prerequisites (Initially Chosen)\nrequires: - \"features/10-metrics-base.kpatch\" If metrics-base NOT enabled → Error: “monitoring requires metrics-base” User must explicitly enable both patches More explicit control, prevents surprises Option B: Requirements with Auto-Enable (Final Choice)\nrequires: - \"features/10-metrics-base.kpatch\" If monitoring enabled → automatically enables metrics-base Creates dependency chains Transitive dependencies supported User changed mind during design process Rationale for change: User convenience outweighs explicitness concern\nDependency Resolution Process Parse all patch metadata files Evaluate enabled expressions against parameters Build dependency graph from requires fields Auto-enable required patches transitively Check for conflicts between enabled patches Detect circular dependencies Report what was auto-enabled to user Conflict Resolution Error on conflicts: Cannot enable conflicting patches simultaneously Example: Monitoring and lightweight-monitoring are mutually exclusive User feedback: Clear error messages explaining conflicts Base Patch Pattern Evolution from Implicit to Explicit Original idea: Automatic base.kpatch applied to all resources Problem: If it’s always applied, why not just update the base YAML? Final decision: Explicit 00-base.kpatch that package author must include\nBase Patch Content # patches/00-base.kpatch - Applied to all resources # Global labels and annotations metadata.labels: \"${global.labels}\" metadata.annotations: \"${global.annotations}\" # Apply to all Deployments [deployment.*.spec.template.spec] securityContext: \"${global.securityContext}\" nodeSelector: \"${global.nodeSelector}\" tolerations: \"${global.tolerations}\" # Apply to all containers in all Deployments [deployment.*.spec.template.spec.containers.*] resources: \"${global.resources}\" imagePullPolicy: \"${global.imagePullPolicy}\" What Goes in Base Patches Cross-cutting concerns: Labels, annotations applied to all resources Security defaults: SecurityContext, RBAC patterns Resource defaults: CPU/memory requests and limits Image patterns: Registry, pull policies Scheduling: Node selectors, tolerations, affinity TOML Headers Clarification What We Kept Standard TOML headers for patch targeting remain part of the core patch design Example: [deployment.my-app.spec.template.spec.containers.0] What We Rejected Special TOML headers for variable definitions in patch files Example (rejected): [variables] cpu_request = \"100m\" memory_request = \"128Mi\" Final Decision All variable definitions go in parameters.yaml Patch files contain only targeting headers and patch operations Metadata files (.yaml) contain patch enabling/dependency info Clean separation of concerns Multi-Namespace Support \u0026 Validation Namespace Handling Philosophy Design Decision: Full Flexibility Allow: Resources targeting any namespaces Allow: Creating multiple namespaces Allow: Cross-namespace references Rejected: Single-namespace enforcement (like some Helm charts) Rationale: “kurel just generates YAML” - don’t artificially constrain users Example Multi-Namespace Package # resources/namespaces.yaml apiVersion: v1 kind: Namespace metadata: name: apps --- apiVersion: v1 kind: Namespace metadata: name: monitoring --- # resources/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: frontend namespace: apps # Different namespace --- # resources/service.yaml apiVersion: v1 kind: Service metadata: name: metrics namespace: monitoring # Another namespace Namespace Creation Control Control Mechanism # parameters.yaml global: namespaces: create: true # Default: create namespaces exclude: # Don't create these - \"kube-system\" - \"default\" Base Patch Integration # patches/00-base.kpatch - Conditional namespace creation [namespace.${monitoring.namespace}] enabled: \"${global.namespaces.create}\" metadata.labels: \"${global.labels}\" metadata.annotations: \"${global.annotations}\" Manual Namespace Control Users can disable specific namespace creation:\n# patches/99-namespace-overrides.kpatch [namespace.kube-system] enabled: false # Don't try to create kube-system Validation Scope \u0026 Approach Validation Scope Decision Within package only: Check conflicts within the generated manifests Not against live cluster: No validation against existing cluster resources Rationale: Keep kurel simple, cluster validation is GitOps tool responsibility Validation Checks kurel validate my-app.kurel/ ✓ No naming conflicts within namespaces ✓ Resources reference consistent namespaces ⚠ Warning: Resources use namespace 'custom-ns' but no Namespace resource found → Enable global.namespaces.create=true or create Namespace resource manually ✓ Cross-namespace Service→Deployment references look valid ✓ All patch variable references exist in parameters.yaml ✗ Error: Two Services named 'api' in namespace 'apps' Cross-Namespace Reference Validation Basic validation: Check that referenced resources exist in package Example: Service targeting Deployment in different namespace Future enhancement: More sophisticated reference checking GitOps Integration \u0026 Deployment Phases Install Phase Annotations Annotation Design Domain: kurel.gokure.dev/ (uses our domain as discussed) Install phase: kurel.gokure.dev/install-phase Valid values: pre-install, main, post-install Default: main if not specified Additional Control Annotations # Resource-level deployment control metadata: annotations: kurel.gokure.dev/install-phase: \"pre-install\" kurel.gokure.dev/wait-for-ready: \"true\" kurel.gokure.dev/timeout: \"5m\" Three-Phase Deployment Pattern Pre-install: CRDs, namespaces, RBAC, secrets Main: Primary application resources (default) Post-install: Monitoring, backups, optional components Flux Translation Example Generated Kustomization Structure kustomizations/ ├── my-app-pre-install/ │ ├── kustomization.yaml # No dependencies │ └── ... ├── my-app-main/ │ ├── kustomization.yaml # Depends on: my-app-pre-install │ └── ... └── my-app-post-install/ ├── kustomization.yaml # Depends on: my-app-main └── ... Flux Kustomization Dependencies # my-app-main/kustomization.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: my-app-main spec: dependsOn: - name: my-app-pre-install # ... rest of spec ArgoCD Compatibility Sync waves: Install phases can map to ArgoCD sync wave annotations Health checks: Compatible with ArgoCD health assessment Dependency management: ArgoCD can handle phase dependencies Patch Modification of Phases Patches can modify install phase annotations Use case: User wants to move a resource to different phase Example: # User patch to move monitoring to pre-install [deployment.monitoring] metadata.annotations[\"kurel.gokure.dev/install-phase\"]: \"pre-install\" User Extension System .local.kurel Design Pattern Design Philosophy Simple overlay model: Local extends, doesn’t modify package Docker Compose inspiration: Similar to docker-compose.override.yml No resource overrides: Can only add patches, not replace base resources Extension Structure my-app.local.kurel/ ├── patches/ # Additional patches only │ ├── 50-custom-limits.kpatch │ ├── 60-local-config.kpatch │ └── team/ │ └── 70-team-policy.kpatch └── parameters.yaml # Override parameter values Processing Order Load package parameters.yaml Merge local parameters.yaml (local values override package values) Resolve all variables with final parameter values Apply package patches (enabled based on final parameters) Apply local patches (enabled based on final parameters) Local Patch Capabilities \u0026 Restrictions Can do:\nAdd new patches that apply to any resources Use any variables from merged parameters Target any resources generated by package Cannot do:\nReference package patches in requires field Override or disable package patches directly Replace base resources (only patch them) Rationale: Keep local extensions simple and avoid complex interactions\nRejected Extension Patterns Multiple Overlay Types Considered: .env.kurel, .team.kurel, .local.kurel for different contexts Rejected: Too complex, single .local.kurel is sufficient Rationale: Most users need one level of customization\nDirect Patch Disabling Considered: Allow local config to disable package patches\n# Rejected approach disable_patches: - \"features/20-monitoring.kpatch\" Rejected: User can already control this via parameter values Rationale: Don’t duplicate control mechanisms\nResource Replacement Considered: Allow local extensions to replace base resources Rejected: Too complex, patches are sufficient Rationale: Maintain clear separation between base resources and customizations\nSchema Generation \u0026 Validation Schema Generation Approach Three-Phase Strategy Phase 1: Basic Type Inference\nScan parameters.yaml and infer types from current values 3 → \"type\": \"integer\" true → \"type\": \"boolean\" \"10Gi\" → \"type\": \"string\" with K8s quantity pattern detection Phase 2: Kubernetes Path Tracing\nParse all .kpatch files for variable usage Trace patch paths to Kubernetes resource fields Query Kubernetes OpenAPI schemas for validation rules Generate constraints based on K8s field definitions Phase 3: Manual Overrides\nSupport manual schema additions for complex cases Allow override of auto-generated constraints Handle cases where tracing fails or is insufficient Path Tracing Example # parameters.yaml replicas: 3 resources: memory: 1Gi # patches/10-scale.kpatch [deployment.my-app] spec.replicas: \"${replicas}\" spec.template.spec.containers[0].resources.limits.memory: \"${resources.memory}\" Tracing process:\n${replicas} → deployment.spec.replicas → K8s: integer, minimum: 0 ${resources.memory} → container.resources.limits.memory → K8s: string, K8s quantity format Generated schema:\n{ \"replicas\": { \"type\": \"integer\", \"minimum\": 0, \"description\": \"Number of replicas (from Deployment.spec.replicas)\" }, \"resources\": { \"type\": \"object\", \"properties\": { \"memory\": { \"type\": \"string\", \"pattern\": \"^[0-9]+[KMGT]i$\", \"description\": \"Memory limit (from Container.resources.limits.memory)\" } } } } Validation Command Design Comprehensive Validation kurel validate my-app.kurel/ --values custom-values.yaml ✓ Validating parameters against generated schema ✓ Validating patch variable references ✓ Checking patch dependencies and conflicts ✓ Validating generated Kubernetes resources ⚠ Warning: Variable 'monitoring.retention' used but not in schema ✗ Error: persistence.size \"10GB\" doesn't match pattern \"^[0-9]+[KMGT]i$\" ✗ Error: Circular dependency: monitoring → metrics → monitoring Validation Levels Parameter schema validation: User values against generated/manual schema Variable reference validation: All ${...} exist in parameters Patch dependency validation: No circular dependencies, no conflicts Kubernetes resource validation: Against K8s OpenAPI when possible Naming conflict validation: Within package scope CRD Support Strategy Well-Known CRDs Start: Support popular CRDs (Cert-Manager, External Secrets, MetalLB) Mechanism: Bundle known CRD schemas with kurel Discovery: Detect CRD usage in patches and apply appropriate schemas Custom CRDs Future: Allow users to provide CRD schemas Mechanism: schemas/crds/ directory in package Auto-detection: Parse CRD YAML to extract schema Schema Distribution Generation Strategy On-demand generation: Generate schemas when needed Caching: Cache generated schemas for performance Version awareness: Regenerate when parameters or patches change Pre-generation (Rejected) Considered: Include pre-generated schemas in packages Rejected: Adds complexity, schemas can become stale Rationale: Generate fresh schemas ensure accuracy\nRejected Features \u0026 Design Alternatives Package Dependencies What Was Considered Helm-style package dependencies with version constraints:\n# Rejected approach kurel: name: my-app dependencies: - name: postgresql version: \"\u003e=11.0.0\" repository: \"https://charts.bitnami.com\" - name: redis version: \"^6.0.0\" condition: caching.enabled Why Rejected Philosophy conflict: “kurel just generates YAML” Complexity: Would require package registry, version resolution Better handled elsewhere: GitOps tools (Flux/ArgoCD) handle dependencies User preference: Dependencies “explicitly configured at higher level” RBAC Auto-Management What Was Considered Automatic RBAC generation and validation:\n# Rejected approach kurel: rbac: required: true permissions: - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"list\", \"watch\"] Why Rejected No clear “automagical” benefit: RBAC requirements are application-specific Simple alternative: Include RBAC resources in resources/ like any other K8s resource Validation complexity: Hard to automatically determine required permissions Philosophy: Keep kurel focused on YAML generation, not security analysis Multi-Tenancy Enforcement What Was Considered Built-in tenancy validation and namespace enforcement:\n# Rejected approach kurel: tenancy: mode: strict allowedNamespaces: [\"app-*\"] resourceNaming: tenant-prefixed Why Rejected Higher-level concern: Tenancy better handled by GitOps tools and admission controllers Flexibility loss: Would constrain valid use cases unnecessarily Philosophy: kurel generates YAML, tenancy tools enforce policies Complex Expression Language What Was Considered Rich expression language for patch enabling:\n# Rejected approach enabled: \"${environment == 'production' \u0026\u0026 monitoring.enabled \u0026\u0026 !minimal_install}\" Why Rejected Complexity: Would require expression parser and evaluator Simple alternative: Boolean variables work for most cases Debugging difficulty: Complex expressions hard to troubleshoot Philosophy: Keep patch enabling simple and predictable Conditional YAML Structures What Was Considered Helm-style conditional blocks in YAML:\n# Rejected approach (Helm-style) {{- if .Values.persistence.enabled }} spec: volumeClaimTemplates: - metadata: name: data {{- end }} Why Rejected Design constraint: No templating or embedded logic Alternative: Use patches to add/remove structures Cleaner separation: Base YAML + patches vs templated YAML Pre-Generated Schemas in Packages What Was Considered Include generated schemas in package distribution:\nmy-app.kurel/ ├── schemas/ │ ├── parameters.schema.json # Pre-generated │ └── resources.schema.json # Pre-generated Why Rejected Staleness risk: Schemas become outdated when parameters change Build complexity: Requires build step to generate schemas Size overhead: Adds to package size unnecessarily Alternative: Generate on-demand with caching Complex Package Composition What Was Considered Ability to compose packages from multiple sources:\n# Rejected approach kurel: name: my-app includes: - package: base-app patches: [\"security/*\"] - package: monitoring version: \"1.0.0\" Why Rejected Complexity: Would require dependency resolution, version management Philosophy: Keep packages self-contained Alternative: Copy/fork packages for customization Implementation Considerations CLI Command Design Core Commands # Validate package and user configuration kurel validate my-app.kurel/ --values custom-values.yaml # Generate schemas from package kurel schema generate my-app.kurel/ # Build final manifests kurel build my-app.kurel/ --values custom-values.yaml --output ./manifests/ # Package information kurel info my-app.kurel/ # List available patches and their status kurel patches list my-app.kurel/ --values custom-values.yaml Validation Output Design $ kurel validate my-app.kurel/ --values production.yaml ✓ Package structure valid ✓ Parameters schema validation passed ✓ All patch variables resolved Enabled patches: ✓ patches/00-base.kpatch (always enabled) ✓ patches/features/10-monitoring.kpatch (monitoring.enabled=true) → patches/features/05-metrics-base.kpatch (required by 10-monitoring) → patches/features/15-monitoring-rbac.kpatch (required by 10-monitoring) ✗ patches/features/20-ingress.kpatch (conflicts with 25-simple-ingress) Generated resources: ✓ 3 Namespaces ✓ 5 Deployments ✓ 8 Services ✓ 2 Ingresses ⚠ Warning: Resources span 3 namespaces Validation summary: 1 error, 1 warning Variable Resolution Engine Resolution Algorithm Parse parameter files: Package + local override Build variable map: Flatten nested structure to dot notation Scan patch files: Extract all ${...} references Validate references: Ensure all variables exist Resolve nested references: Handle ${a.b} where a.b: \"${c.d}\" Type casting: Apply schema-based type conversion Circular dependency detection: Prevent infinite resolution Variable Resolution Example # parameters.yaml kurel: appVersion: \"1.0.0\" image: tag: \"v${kurel.appVersion}\" # References metadata full: \"${image.registry}/${image.repository}:${image.tag}\" Resolution steps:\nkurel.appVersion = \"1.0.0\" image.tag = \"v${kurel.appVersion}\" → \"v1.0.0\" image.full = \"${image.registry}/${image.repository}:${image.tag}\" → \"quay.io/myapp:v1.0.0\" Patch Processing Engine Processing Pipeline Discovery: Glob patches/**/*.kpatch Metadata loading: Load corresponding .yaml files Dependency resolution: Build DAG, detect cycles, auto-enable Conflict checking: Validate no conflicting patches enabled Variable substitution: Replace ${...} with resolved values Patch application: Apply patches to base resources using Kure engine Phase organization: Group resources by install phase Dependency Graph Example 10-monitoring.kpatch ├── requires: 05-metrics-base.kpatch └── requires: 15-monitoring-rbac.kpatch └── requires: 02-base-rbac.kpatch Result: Enable 02-base-rbac → 05-metrics-base → 15-monitoring-rbac → 10-monitoring GitOps Manifest Generation Phase-Based Organization output/ ├── pre-install/ │ ├── kustomization.yaml # Phase resources only │ ├── namespaces.yaml │ ├── rbac.yaml │ └── secrets.yaml ├── main/ │ ├── kustomization.yaml # dependsOn: pre-install │ ├── deployments.yaml │ └── services.yaml └── post-install/ ├── kustomization.yaml # dependsOn: main ├── monitoring.yaml └── backups.yaml Kustomization Generation # main/kustomization.yaml (auto-generated) apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization # Flux dependency dependsOn: - name: my-app-pre-install resources: - deployments.yaml - services.yaml # Common labels applied to all resources commonLabels: app.kubernetes.io/name: my-app app.kubernetes.io/managed-by: kurel Future Considerations Schema Generation Improvements Enhanced K8s API Tracing CRD discovery: Automatic detection of CRD schemas Version-aware tracing: Handle multiple K8s API versions Complex path resolution: Better handling of array selectors and wildcards Machine Learning Schema Enhancement Pattern recognition: Learn common parameter patterns from existing packages Validation suggestions: Suggest additional validation rules based on usage Package Ecosystem Package Registry Distribution: Central registry for sharing kurel packages Discovery: Search and browse available packages Versioning: Semantic versioning for packages Security: Package signing and verification Package Management Tools Installation: kurel install prometheus-operator Updates: kurel upgrade --check Dependencies: Automatic dependency resolution Advanced Patch Features Patch Testing Unit tests: Test individual patches against known resources Integration tests: Test full package generation Regression tests: Ensure patches don’t break existing functionality Patch Composition Mixins: Reusable patch fragments Inheritance: Base patches that others extend Conditional application: More sophisticated enabling logic Developer Experience IDE Integration Language servers: Completion and validation in editors Schema integration: Real-time parameter validation Patch debugging: Visual patch application tracing Package Development Tools Scaffolding: Generate package templates Validation: Real-time package structure validation Testing: Framework for package testing Conclusion This comprehensive design represents the result of extensive iteration and consideration of alternatives. The key insight that guided all decisions was the principle that “kurel just generates YAML” - keeping the system focused on its core purpose while providing powerful customization and validation capabilities.\nThe design balances several important concerns:\nSimplicity vs Power: Provide powerful features without overwhelming complexity Flexibility vs Validation: Allow maximum flexibility while catching common errors Explicitness vs Convenience: Prefer explicit configuration with convenient defaults Standards vs Innovation: Build on existing patterns (Kubernetes, GitOps) while solving real problems The modular patch system, comprehensive parameter handling, and GitOps-native output make kurel well-suited for managing complex Kubernetes applications in a declarative, version-controlled manner. The extensive validation and schema generation capabilities help prevent common configuration errors while maintaining the flexibility that makes Kubernetes powerful.\nThe decision to reject certain features (package dependencies, complex templating, automatic RBAC) keeps kurel focused on its core competency while allowing the broader ecosystem (GitOps tools, policy engines, package managers) to handle their respective concerns.",
    "description": "Kurel Package System - Comprehensive Design Details This document captures the complete design discussion, decisions, alternatives considered, and rationale for the Kurel (Kubernetes Resources Launcher) package system. It serves as a comprehensive record of all design choices made during the extensive design iteration process.\nDesign Philosophy \u0026 Core Principles Fundamental Philosophy “Kurel just generates YAML” - This principle guided every design decision. Kurel is not a runtime system, orchestrator, or complex package manager. It’s a declarative system for generating Kubernetes manifests with validation and customization capabilities.",
    "tags": [],
    "title": "Design Details",
    "uri": "/packages/launcher/design-details/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Examples",
    "content": "Frigate Kurel Package This is a Kurel package for deploying Frigate , a complete and local NVR designed for Home Assistant with AI object detection.\nOverview Frigate is a complete and local NVR designed for Home Assistant with AI object detection. It uses OpenCV and Tensorflow to perform realtime object detection locally for IP cameras.\nPrerequisites Kubernetes cluster Coral USB TPU device (for hardware acceleration) Node labeled with coral-usb=true where the Coral USB is attached MQTT broker (for Home Assistant integration) Storage provisioner for persistent volumes cert-manager (for TLS certificates) Installation Basic Installation kurel build examples/kurel/frigate | kubectl apply -f - Installation with Custom Values Create a values file my-values.yaml: app: namespace: my-frigate image: tag: 0.13.0 service: loadBalancerIP: 10.0.0.100 ingress: hostname: frigate.mydomain.com storage: size: 200Gi Build and deploy: kurel build examples/kurel/frigate --values my-values.yaml | kubectl apply -f - Using Patches Apply environment-specific patches:\n# For production environment kurel build examples/kurel/frigate --patch production | kubectl apply -f - # For development environment kurel build examples/kurel/frigate --patch development | kubectl apply -f - # For high availability setup kurel build examples/kurel/frigate --patch high-availability | kubectl apply -f - Configuration Key Parameters Parameter Description Default app.namespace Namespace to deploy into frigate app.image.repository Frigate image repository ghcr.io/blakeblackshear/frigate app.image.tag Frigate image tag 0.12.0 service.type Service type LoadBalancer service.loadBalancerIP Static IP for LoadBalancer 172.32.104.0 ingress.enabled Enable ingress true ingress.hostname Hostname for ingress frigate.home.vanginderachter.be storage.size PVC storage size 100Gi storage.storageClass Storage class name local-path mqtt.host MQTT broker host mqtt-broker-home.mqttbroker detector.type Detector type edgetpu detector.device Detector device usb Secrets Management This package requires several secrets to be configured:\nMQTT Password: Create a secret named mqttuser with key FRIGATE_MQTT_PASSWORD Camera Passwords: Create a secret named frigate-camera-secrets with keys for each camera Frigate Plus API Key: Create a secret named frigate-secrets with key plus-api-key Example using kubectl:\nkubectl create secret generic mqttuser \\ --from-literal=FRIGATE_MQTT_PASSWORD=your-mqtt-password \\ -n frigate kubectl create secret generic frigate-camera-secrets \\ --from-literal=cam0-password=password0 \\ --from-literal=cam1-password=password1 \\ --from-literal=cam2-password=password2 \\ --from-literal=cam3-password=password3 \\ --from-literal=cam360-password=password360 \\ -n frigate For production, consider using SealedSecrets or External Secrets Operator.\nCamera Configuration The actual camera configuration should be added to the ConfigMap in resources/configmap.yaml. Here’s an example:\ncameras: front_door: ffmpeg: inputs: - path: rtsp://{FRIGATE_CAM_USERNAME}:{FRIGATE_CAM0_PASSWORD}@192.168.1.100:554/stream1 roles: - detect - record detect: width: 1920 height: 1080 fps: 5 record: enabled: true retain: days: 7 snapshots: enabled: true Hardware Acceleration This package is configured to use a Coral USB TPU for hardware acceleration. The deployment will be scheduled on nodes labeled with coral-usb=true.\nTo label a node:\nkubectl label node \u003cnode-name\u003e coral-usb=true Monitoring The deployment includes liveness and readiness probes to ensure Frigate is running correctly.\nTroubleshooting Pod not scheduling Ensure a node is labeled with coral-usb=true Check if the Coral USB device is properly connected MQTT connection issues Verify MQTT broker is running and accessible Check MQTT credentials in secrets Storage issues Ensure storage class exists and can provision volumes Check available storage capacity License This Kurel package is provided as-is. Frigate itself is licensed under the MIT License.",
    "description": "Frigate Kurel Package This is a Kurel package for deploying Frigate , a complete and local NVR designed for Home Assistant with AI object detection.\nOverview Frigate is a complete and local NVR designed for Home Assistant with AI object detection. It uses OpenCV and Tensorflow to perform realtime object detection locally for IP cameras.\nPrerequisites Kubernetes cluster Coral USB TPU device (for hardware acceleration) Node labeled with coral-usb=true where the Coral USB is attached MQTT broker (for Home Assistant integration) Storage provisioner for persistent volumes cert-manager (for TLS certificates) Installation Basic Installation kurel build examples/kurel/frigate | kubectl apply -f - Installation with Custom Values Create a values file my-values.yaml: app: namespace: my-frigate image: tag: 0.13.0 service: loadBalancerIP: 10.0.0.100 ingress: hostname: frigate.mydomain.com storage: size: 200Gi Build and deploy: kurel build examples/kurel/frigate --values my-values.yaml | kubectl apply -f - Using Patches Apply environment-specific patches:",
    "tags": [],
    "title": "Kurel Frigate",
    "uri": "/examples/kurel-frigate/index.html"
  },
  {
    "breadcrumb": "Go Kure",
    "content": "Kure Packages Explore the core packages that make up the Kure library. Each package provides specific functionality for building and managing Kubernetes resources.\nCore Packages Launcher The launcher package provides a package system for creating reusable, customizable Kubernetes applications without the complexity of templating engines. It uses a declarative patch-based approach to customize base Kubernetes manifests.\nPatch The patch package implements a JSONPath-based declarative patching system. It allows you to modify Kubernetes resources using a simple, powerful patch language that maintains YAML structure and comments.\nStack The stack package provides the core data model for organizing Kubernetes resources into a hierarchical structure (Cluster → Node → Bundle → Application) suitable for GitOps deployment.\nLayout The layout package handles manifest organization and directory structure generation. It provides flexible rules for grouping and organizing generated Kubernetes resources into a clean directory structure.\nIO The io package provides utilities for reading, writing, and parsing YAML representations of Kubernetes resources, including kubectl-compatible resource printing.\nErrors The errors package provides structured error types and handling utilities for Kubernetes resource validation, file operations, and configuration errors.\nCLI The cli package provides shared utilities and abstractions for building command-line interfaces in the Kure and kurel tools.\nAdditional Resources Architecture Overview - Understand how these packages fit together Examples - See the packages in action API Documentation - Detailed API reference",
    "description": "Kure Packages Explore the core packages that make up the Kure library. Each package provides specific functionality for building and managing Kubernetes resources.\nCore Packages Launcher The launcher package provides a package system for creating reusable, customizable Kubernetes applications without the complexity of templating engines. It uses a declarative patch-based approach to customize base Kubernetes manifests.\nPatch The patch package implements a JSONPath-based declarative patching system. It allows you to modify Kubernetes resources using a simple, powerful patch language that maintains YAML structure and comments.",
    "tags": [],
    "title": "Packages",
    "uri": "/packages/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Packages \u003e Patch",
    "content": "Kure patch Module — Purpose and Design Purpose The patch module provides the core abstraction layer for loading, representing, and mutating Kubernetes resources using structured patches — without templates, overlays, or DSLs.\nIt enables tools to declaratively define resource configurations and safe modifications using dual-format support (YAML and TOML) with Go-native data structures. Patches modify an existing base resource, with advanced structure preservation that maintains comments, formatting, and document order.\nThis forms the foundation for Kure’s deterministic, introspectable Kubernetes manifest generation pipeline with sophisticated intelligent path resolution and automatic type inference.\nDesign Principles Dual-format declarative patching over templating:\nSupport both YAML and TOML patch formats with automatic detection Use single-line syntax to express changes to Kubernetes objects Avoid logic or conditional expressions in patch files One patch = one operation:\nPatches operate on individual fields or list items Operations include: replace, delete, insertbefore, insertafter, append Intelligent patch targeting with disambiguation:\nAutomatically routes patches to matching resources using metadata and field validation Advanced resolution algorithms handle multiple resources with same names Supports target: override and kind.name disambiguation Smart target matching validates patch compatibility with resource schemas Structure preservation and introspection:\nAll paths are parsed and normalized into PathPart structs YAML structure preservation maintains comments, formatting, and document order Path syntax is explicitly validated before application Comprehensive debug logging with KURE_DEBUG=1 Automatic type inference and conversion:\nIntelligent conversion of string values to appropriate Go types (int, bool, etc.) Context-aware type inference based on Kubernetes field patterns Compatible with unstructured.Unstructured requirements Graceful error handling:\nMissing patch targets generate warnings rather than failures Batch processing continues on individual patch failures Comprehensive error context for debugging Base resource required:\nEach patch is applied on top of an existing object loaded from a file or provided programmatically Core Types PatchOp: A single parsed patch line (path, value, operation, selector) PathPart: A structured representation of a patch path segment with match types PatchableAppSet: Holds resources and their associated patch operations with structure preservation YAMLDocumentSet: Advanced structure preservation for comments and formatting YAMLDocument: Individual document with preserved structure and metadata TOMLHeader: Parsed TOML-style header with intelligent Kubernetes path mapping VariableContext: Context for variable substitution with values and features Interfaces and Helpers Resource Loading LoadResourcesFromMultiYAML(io.Reader) — Load 1..n Kubernetes resources LoadResourcesWithStructure(io.Reader) — Load resources with structure preservation Patch Loading LoadPatchFile(io.Reader) — Load patches with automatic format detection LoadPatchFileWithVariables(io.Reader, *VariableContext) — Load with variable substitution LoadYAMLPatchFile(io.Reader, *VariableContext) — Load YAML format patches LoadTOMLPatchFile(io.Reader, *VariableContext) — Load TOML format patches PatchableAppSet Construction NewPatchableAppSet([]*unstructured.Unstructured, []PatchSpec) — Create from in-memory objects NewPatchableAppSetWithStructure(*YAMLDocumentSet, []PatchSpec) — Create with structure preservation LoadPatchableAppSet([]io.Reader, io.Reader) — Create a complete working set Path Processing and Validation NormalizePath() — Validate and parse patch paths before application ParsePatchPath(string) — Parse paths into structured PathPart components ParseTOMLHeader(string) — Parse TOML headers with intelligent path mapping Type System and Variable Support inferValueType(key, value string) — Automatic type inference for Kubernetes compatibility SubstituteVariables(string, *VariableContext) — Variable substitution with ${values.key} syntax Format Support YAML Format Supports expressive list modification syntax:\n# Simple patch map spec.containers[3].image: nginx:latest # replace by index spec.containers[+name=web].image: sidecar:1 # insert after matching item spec.containers[-]: { name: debug } # append to list metadata.labels[delete=app]: \"\" # delete field # Targeted patch list - target: my-deployment patch: spec.replicas: 5 spec.template.metadata.labels.foo: bar metadata.labels[delete=app]: \"\" TOML Format (Preferred) Intelligent header-based targeting with automatic path mapping:\n# Basic resource targeting [deployment.app] spec.replicas: 3 metadata.labels.env: production # Container-specific patches with semantic selectors [deployment.app.containers.name=main] resources.requests.cpu: 100m resources.limits.memory: 512Mi image.tag: \"${values.version}\" # Service port configuration [service.app.ports.name=https] port: 443 nodePort: 30443 # Complex array operations [deployment.app.containers.name=main.env[-]] name: DEBUG_MODE value: \"true\" Advanced Features Automatic Format Detection The system automatically detects patch format based on content structure:\nTOML format: Detected by presence of [header] sections YAML format: Detected by YAML structure patterns Structure Preservation Maintains original YAML comments, formatting, and document order Preserves multi-document YAML structure with --- separators Updates only targeted fields while leaving surrounding structure intact Intelligent Path Resolution Context-aware mapping of TOML sections to Kubernetes paths Different behavior based on resource type (Deployment vs Service vs Pod) Smart disambiguation for resources with identical names Variable Substitution [deployment.app.containers.name=main] image.tag: \"${values.version}\" resources.requests.cpu: \"${values.cpu_request}\" debug.enabled: \"${features.enable_debug}\" Automatic Type Inference Converts string values to appropriate Go types based on field context Recognizes port numbers, replica counts, timeout values automatically Kubernetes-compatible type conversion for unstructured objects",
    "description": "Kure patch Module — Purpose and Design Purpose The patch module provides the core abstraction layer for loading, representing, and mutating Kubernetes resources using structured patches — without templates, overlays, or DSLs.\nIt enables tools to declaratively define resource configurations and safe modifications using dual-format support (YAML and TOML) with Go-native data structures. Patches modify an existing base resource, with advanced structure preservation that maintains comments, formatting, and document order.",
    "tags": [],
    "title": "Patch Engine Design",
    "uri": "/packages/patch/patch-engine-design/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Packages",
    "content": "Stack Package The stack package provides the core data model for organizing Kubernetes resources into a hierarchical structure suitable for GitOps deployment.\nHierarchy The stack organizes resources in a four-level hierarchy:\nCluster — Top-level grouping representing a Kubernetes cluster Node — Logical grouping within a cluster (e.g., infrastructure, applications) Bundle — Collection of related applications Application — Individual Kubernetes application with its resources This structure maps naturally to directory layouts consumed by GitOps tools like Flux and ArgoCD.\nSub-packages Generators — Resource generation strategies Layout — Directory structure generation Documentation Design - Stack system design Status - Current implementation status API Reference pkg.go.dev/github.com/go-kure/kure/pkg/stack",
    "description": "Stack Package The stack package provides the core data model for organizing Kubernetes resources into a hierarchical structure suitable for GitOps deployment.\nHierarchy The stack organizes resources in a four-level hierarchy:\nCluster — Top-level grouping representing a Kubernetes cluster Node — Logical grouping within a cluster (e.g., infrastructure, applications) Bundle — Collection of related applications Application — Individual Kubernetes application with its resources This structure maps naturally to directory layouts consumed by GitOps tools like Flux and ArgoCD.",
    "tags": [],
    "title": "Stack",
    "uri": "/packages/stack/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Architecture",
    "content": "Kure End User Experience Design Document Version: 1.0\nDate: August 2025\nStatus: Design Specification\nExecutive Summary This document outlines a comprehensive user experience strategy for Kure, a Go library for programmatically building Kubernetes resources for GitOps workflows. The design focuses on creating multiple interaction modalities that serve different user types while maintaining Kure’s core strengths of type safety, composability, and GitOps compatibility.\nKey Design Principles:\nProgressive Complexity: Simple defaults with expert-level customization Multi-Modal Interaction: Visual, programmatic, and hybrid approaches Type-Safe Configuration: Leveraging Kure’s existing validation and GVK system GitOps Native: Seamless integration with Flux and ArgoCD workflows Table of Contents User Research \u0026 Personas Current State Analysis Vision \u0026 Design Principles User Journey Maps Multi-Modal Interface Design Configuration Management System Implementation Roadmap Success Metrics User Research \u0026 Personas Primary User Personas 1. Platform Engineer (Sarah) Role: Senior Platform Engineer at mid-size tech company Experience: 5+ years Kubernetes, familiar with GitOps concepts Goals: Standardize cluster configurations, reduce operational overhead Pain Points: Complex YAML management, ensuring consistency across environments Preferred Interaction: Programmatic APIs with visual validation 2. DevOps Beginner (Mike) Role: Junior DevOps Engineer, transitioning from traditional ops Experience: 1 year Kubernetes, new to GitOps Goals: Learn best practices, avoid configuration mistakes Pain Points: Overwhelmed by Kubernetes complexity, unclear dependencies Preferred Interaction: Guided workflows with explanations 3. Solutions Architect (Alex) Role: Technical lead designing multi-cluster architectures Experience: 8+ years infrastructure, deep Kubernetes knowledge Goals: Design scalable, maintainable cluster architectures Pain Points: Visualizing complex dependencies, communicating designs to teams Preferred Interaction: Visual modeling with code generation 4. Development Team Lead (Jordan) Role: Engineering manager overseeing application deployment Experience: 3 years Kubernetes, focused on application concerns Goals: Self-service application deployment, consistent configurations Pain Points: Waiting for platform team, inconsistent environments Preferred Interaction: Template-based configuration with customization User Needs Analysis User Type Primary Need Secondary Need Success Criteria Platform Engineer Standardization \u0026 Automation Visual Validation Reduced config drift, faster onboarding DevOps Beginner Learning \u0026 Guidance Error Prevention Successful deployments, increased confidence Solutions Architect Design \u0026 Communication Code Generation Clear architecture docs, team alignment Development Team Lead Self-Service \u0026 Consistency Template Management Faster deployments, reduced dependencies Current State Analysis Existing Interaction Model Currently, users interact with Kure through direct Go code:\n// Manual, step-by-step creation cluster := stack.NewCluster(\"production\", rootNode) node := stack.NewNode(\"infrastructure\") bundle := stack.NewBundle(\"monitoring\") app := stack.NewApplication(\"prometheus\", appConfig) // Manual hierarchy construction cluster.AddNode(node) node.AddBundle(bundle) bundle.AddApplication(app) Pain Points Identified High Cognitive Load: Users must understand the entire hierarchy upfront Verbose Construction: Repetitive boilerplate for common patterns No Visual Feedback: Difficult to validate complex configurations Limited Discoverability: Hard to explore available options Expert-Only Access: High barrier to entry for newcomers Strengths to Preserve Type Safety: Compile-time validation prevents common errors Composability: Clean separation of concerns across hierarchy GitOps Integration: Native support for Flux and ArgoCD Extensibility: Plugin system for custom generators Schema Evolution: GVK system supports API versioning Vision \u0026 Design Principles Vision Statement “Make Kubernetes cluster configuration as intuitive as building with Lego blocks, while maintaining the precision and power of a programming language.”\nCore Design Principles 1. Progressive Disclosure Principle: Reveal complexity gradually based on user needs Implementation: Start with simple templates, expose advanced options on demand Example: Wizard → Builder → Direct Code → YAML Export 2. Multi-Modal Interaction Principle: Support different thinking and working styles Implementation: Visual, programmatic, and hybrid interfaces Example: Visual blocks for architects, code builders for engineers, forms for beginners 3. Fail-Fast Validation Principle: Catch errors as early as possible in the configuration process Implementation: Real-time validation with contextual help Example: Immediate feedback on invalid configurations with suggested fixes 4. Convention over Configuration Principle: Provide sensible defaults while allowing customization Implementation: Smart templates with override capabilities Example: Production-ready monitoring stack with one click, full customization available 5. Composable Architecture Principle: Enable building complex systems from simple, reusable components Implementation: Mixins, templates, and inheritance patterns Example: Security mixin + monitoring mixin + custom application = complete cluster User Journey Maps Journey 1: First-Time User Setup journey title First-Time User: Setting Up Monitoring Cluster section Discovery Visit documentation: 3: User Review examples: 4: User Choose template: 5: User section Configuration Start wizard: 5: User Basic cluster setup: 4: User Add monitoring components: 5: User Review generated config: 4: User section Deployment Export to YAML: 5: User Apply to cluster: 3: User Verify deployment: 5: User section Learning Explore tree view: 4: User Modify configuration: 3: User Learn advanced features: 4: User Journey 2: Platform Engineer Standardization journey title Platform Engineer: Creating Organization Standards section Analysis Audit existing configs: 2: User Identify patterns: 3: User Define requirements: 4: User section Template Creation Create base template: 4: User Add security policies: 3: User Test with dev cluster: 5: User Document template: 4: User section Team Adoption Share template library: 5: User Train team members: 4: User Monitor usage: 5: User Iterate based on feedback: 4: User Journey 3: Solutions Architect Design journey title Solutions Architect: Multi-Cluster Design section Planning Stakeholder requirements: 3: User Architecture constraints: 2: User Technology decisions: 4: User section Design Visual cluster modeling: 5: User Define dependencies: 4: User Resource allocation: 3: User Security boundaries: 4: User section Communication Generate diagrams: 5: User Export documentation: 4: User Present to stakeholders: 5: User Iterate on feedback: 4: User section Implementation Generate code templates: 5: User Hand off to team: 4: User Support deployment: 4: User Multi-Modal Interface Design Interface Architecture Overview The Kure UX consists of four primary interaction modes that users can switch between seamlessly:\nGuided Wizard - Step-by-step configuration for beginners Hierarchical Tree - Structural navigation for intermediate users Visual Builder - Block-based design for architects Code Editor - Direct programmatic control for experts 1. Guided Wizard Interface Purpose \u0026 Users Primary Users: DevOps beginners, first-time users Use Cases: Initial cluster setup, learning Kure concepts Key Features: Progressive disclosure, contextual help, error prevention Design Specifications interface WizardStep { id: string title: string description: string component: WizardStepComponent validation: ValidationRule[] dependencies: string[] canSkip: boolean } interface WizardFlow { steps: WizardStep[] currentStep: number userData: Record\u003cstring, any\u003e branching: BranchingLogic } Step Flow Design Welcome \u0026 Template Selection\nChoose from curated templates (monitoring, web-app, database) Preview template contents and generated resources Option to start from scratch for advanced users Cluster Configuration\nBasic cluster metadata (name, description) GitOps selection (Flux/ArgoCD) with explanations Bootstrap configuration with sensible defaults Node Architecture\nInfrastructure vs application node separation Hierarchical structure with visual preview Package reference configuration (OCI/Git) Bundle Configuration\nLogical grouping of applications Dependency management with visual graph Source configuration and intervals Application Setup\nGenerator selection (AppWorkload, FluxHelm, etc.) Configuration forms based on JSON schema Live preview of generated resources Review \u0026 Generate\nComplete configuration summary Generated YAML preview with syntax highlighting Validation results and recommendations Export options (YAML, Go code, Terraform) UI Components // Progress indicator \u003cWizardProgress steps={wizardSteps} currentStep={currentStep} completedSteps={completedSteps} /\u003e // Step content with validation \u003cWizardStep step={currentStepData} data={formData} onChange={handleStepChange} onValidate={validateStep} errors={validationErrors} /\u003e // Navigation controls \u003cWizardNavigation canGoBack={currentStep \u003e 0} canGoForward={isCurrentStepValid} onBack={handleBack} onForward={handleForward} onCancel={handleCancel} /\u003e 2. Hierarchical Tree Interface Purpose \u0026 Users Primary Users: Platform engineers, intermediate users Use Cases: Detailed configuration, structural modification Key Features: Expandable tree, contextual editing, bulk operations Design Specifications interface TreeNode { id: string type: 'cluster' | 'node' | 'bundle' | 'application' name: string metadata: Record\u003cstring, any\u003e children: TreeNode[] expanded: boolean selected: boolean errors: ValidationError[] } interface TreeViewState { rootNode: TreeNode selectedNodes: string[] expandedNodes: string[] searchFilter: string validationResults: ValidationResults } Tree Structure Visualization 🏭 Production Cluster ├── 📦 Infrastructure Node │ ├── 📋 Monitoring Bundle │ │ ├── 🔧 Prometheus App (AppWorkload) │ │ ├── 📊 Grafana App (AppWorkload) │ │ └── 🚨 AlertManager App (FluxHelm) │ ├── 📋 Logging Bundle │ │ ├── 📝 Fluentd App (FluxHelm) │ │ └── 🔍 ElasticSearch App (FluxHelm) │ └── 📋 Security Bundle │ ├── 🛡️ Falco App (FluxHelm) │ └── 🔐 OPA Gatekeeper App (FluxHelm) └── 🎯 Applications Node ├── 📋 Frontend Bundle │ ├── 🌐 React App (AppWorkload) │ └── 🔄 Nginx Proxy (AppWorkload) └── 📋 Backend Bundle ├── ⚙️ API Server (AppWorkload) └── 🗄️ Database (FluxHelm) Interaction Patterns Expand/Collapse: Click chevron to show/hide children Selection: Click item to select, multi-select with Ctrl/Cmd Context Menu: Right-click for add/edit/delete/duplicate options Drag \u0026 Drop: Reorganize hierarchy by dragging items Inline Editing: Double-click names for quick renaming Search \u0026 Filter: Real-time filtering with highlighting Property Panel Design \u003cPropertyPanel\u003e \u003cTabGroup\u003e \u003cTab name=\"General\"\u003e \u003cFormField label=\"Name\" value={selectedNode.name} /\u003e \u003cFormField label=\"Description\" value={selectedNode.description} /\u003e \u003cFormField label=\"Labels\" component={KeyValueEditor} /\u003e \u003c/Tab\u003e \u003cTab name=\"Configuration\"\u003e \u003cDynamicForm schema={nodeSchema} data={nodeData} /\u003e \u003c/Tab\u003e \u003cTab name=\"Advanced\"\u003e \u003cCodeEditor language=\"yaml\" value={nodeYaml} /\u003e \u003c/Tab\u003e \u003cTab name=\"Validation\"\u003e \u003cValidationResults errors={validationErrors} /\u003e \u003c/Tab\u003e \u003c/TabGroup\u003e \u003c/PropertyPanel\u003e 3. Visual Builder Interface Purpose \u0026 Users Primary Users: Solutions architects, visual learners Use Cases: Architecture design, dependency visualization Key Features: Drag-drop composition, visual connections, layout algorithms Design Specifications interface VisualComponent { id: string type: ComponentType position: { x: number, y: number } size: { width: number, height: number } properties: Record\u003cstring, any\u003e connections: Connection[] } interface Connection { from: string to: string type: 'hierarchy' | 'dependency' | 'communication' style: ConnectionStyle } interface CanvasState { components: VisualComponent[] connections: Connection[] selectedComponents: string[] viewport: { x: number, y: number, zoom: number } } Component Palette const componentPalette = [ { category: \"Core\", components: [ { type: \"cluster\", icon: \"🏭\", label: \"Cluster\" }, { type: \"node\", icon: \"📦\", label: \"Node\" }, { type: \"bundle\", icon: \"📋\", label: \"Bundle\" }, { type: \"application\", icon: \"🎯\", label: \"Application\" } ] }, { category: \"Templates\", components: [ { type: \"monitoring-stack\", icon: \"📊\", label: \"Monitoring Stack\" }, { type: \"web-app\", icon: \"🌐\", label: \"Web Application\" }, { type: \"database\", icon: \"🗄️\", label: \"Database\" }, { type: \"security-stack\", icon: \"🛡️\", label: \"Security Stack\" } ] }, { category: \"Generators\", components: [ { type: \"app-workload\", icon: \"⚙️\", label: \"App Workload\" }, { type: \"flux-helm\", icon: \"📦\", label: \"Flux Helm\" }, { type: \"kurel-package\", icon: \"📄\", label: \"Kurel Package\" } ] } ] Visual Design System /* Component visual styles */ .component { border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); transition: all 0.2s ease; } .component.cluster { background: linear-gradient(135deg, #4a90e2, #357abd); color: white; border: 3px solid #2c5aa0; } .component.node { background: linear-gradient(135deg, #52c41a, #389e0d); color: white; border: 2px solid #237804; } .component.bundle { background: linear-gradient(135deg, #fa8c16, #d46b08); color: white; border: 2px solid #ad4e00; } .component.application { background: linear-gradient(135deg, #722ed1, #531dab); color: white; border: 2px solid #391085; } /* Connection styles */ .connection { stroke-width: 2px; stroke-dasharray: none; } .connection.hierarchy { stroke: #4a90e2; } .connection.dependency { stroke: #fa8c16; stroke-dasharray: 5,5; } .connection.communication { stroke: #52c41a; stroke-dasharray: 2,3; } Layout Algorithms interface LayoutAlgorithm { name: string description: string apply: (components: VisualComponent[]) =\u003e VisualComponent[] } const layoutAlgorithms: LayoutAlgorithm[] = [ { name: \"Hierarchical\", description: \"Top-down tree layout following cluster hierarchy\", apply: hierarchicalLayout }, { name: \"Force-Directed\", description: \"Physics-based layout emphasizing relationships\", apply: forceDirectedLayout }, { name: \"Grid\", description: \"Organized grid layout for clean presentation\", apply: gridLayout }, { name: \"Manual\", description: \"Free-form positioning with manual control\", apply: (components) =\u003e components } ] 4. Code Editor Interface Purpose \u0026 Users Primary Users: Expert developers, power users Use Cases: Advanced customization, automation, debugging Key Features: Syntax highlighting, autocomplete, live validation Design Specifications interface CodeEditorState { content: string language: 'go' | 'yaml' | 'json' cursorPosition: { line: number, column: number } validationErrors: ValidationError[] suggestions: CompletionItem[] } interface CodeGenerator { generateFromConfig(config: ClusterConfig): string parseFromCode(code: string): ClusterConfig validateCode(code: string): ValidationResult } Editor Features Syntax Highlighting: Language-specific highlighting for Go, YAML, JSON Auto-completion: Context-aware suggestions for Kure APIs Error Squiggles: Real-time validation with inline error display Code Folding: Collapse sections for better navigation Mini-map: Overview of large files with scroll position Search \u0026 Replace: Advanced find/replace with regex support Code Generation Templates const codeTemplates = { go: { cluster: ` cluster := stack.NewClusterBuilder(\"{{name}}\"). WithGitOps(\"{{gitopsType}}\"). {{#nodes}} WithNode(\"{{name}}\"). {{#bundles}} WithBundle(\"{{name}}\"). {{#applications}} WithApplication(\"{{name}}\", {{configVar}}). {{/applications}} End(). {{/bundles}} End(). {{/nodes}} Build() `, monitoring: ` monitoringStack := stack.NewMonitoringStackBuilder(). WithPrometheus({{prometheus}}). WithGrafana({{grafana}}). WithAlertManager({{alertmanager}}). Build() ` }, yaml: { cluster: ` apiVersion: stack.gokure.dev/v1alpha1 kind: Cluster metadata: name: {{name}} spec: gitops: type: {{gitopsType}} bootstrap: enabled: {{bootstrapEnabled}} nodes: {{#nodes}} - name: {{name}} bundles: {{#bundles}} - name: {{name}} applications: {{#applications}} - {{\u0026yaml}} {{/applications}} {{/bundles}} {{/nodes}} ` } } Configuration Management System Fluent Builder Architecture Core Builder Pattern // Base builder interface type Builder interface { Build() (interface{}, error) Validate() error Clone() Builder } // Cluster builder with fluent API type ClusterBuilder struct { name string nodes []NodeBuilder gitops *GitOpsConfig validation ValidationLevel metadata map[string]interface{} } func NewClusterBuilder(name string) *ClusterBuilder { return \u0026ClusterBuilder{ name: name, nodes: []NodeBuilder{}, validation: ValidationStrict, metadata: make(map[string]interface{}), } } func (cb *ClusterBuilder) WithNode(name string) *NodeBuilder { nb := NewNodeBuilder(name, cb) cb.nodes = append(cb.nodes, *nb) return nb } func (cb *ClusterBuilder) WithGitOps(gitopsType string) *GitOpsBuilder { return NewGitOpsBuilder(gitopsType, cb) } func (cb *ClusterBuilder) Build() (*stack.Cluster, error) { if err := cb.Validate(); err != nil { return nil, err } cluster := stack.NewCluster(cb.name, nil) // Build nodes for _, nodeBuilder := range cb.nodes { node, err := nodeBuilder.Build() if err != nil { return nil, err } cluster.AddNode(node.(*stack.Node)) } // Set GitOps configuration cluster.SetGitOps(cb.gitops) return cluster, nil } Hierarchical Builder Navigation type NodeBuilder struct { name string bundles []BundleBuilder parent *ClusterBuilder packageRef *schema.GroupVersionKind } func (nb *NodeBuilder) WithBundle(name string) *BundleBuilder { bb := NewBundleBuilder(name, nb) nb.bundles = append(nb.bundles, *bb) return bb } func (nb *NodeBuilder) WithPackageRef(gvk *schema.GroupVersionKind) *NodeBuilder { nb.packageRef = gvk return nb } func (nb *NodeBuilder) End() *ClusterBuilder { return nb.parent } type BundleBuilder struct { name string applications []ApplicationBuilder parent *NodeBuilder interval string sourceRef *stack.SourceRef } func (bb *BundleBuilder) WithApplication(name string, config stack.ApplicationConfig) *BundleBuilder { ab := NewApplicationBuilder(name, config, bb) bb.applications = append(bb.applications, *ab) return bb } func (bb *BundleBuilder) End() *NodeBuilder { return bb.parent } Preset Configuration System Template Registry type PresetTemplate struct { Name string Description string Category string Tags []string Builder func() Builder Metadata TemplateMetadata } type TemplateRegistry struct { templates map[string]PresetTemplate mutex sync.RWMutex } func (tr *TemplateRegistry) Register(template PresetTemplate) error { tr.mutex.Lock() defer tr.mutex.Unlock() if _, exists := tr.templates[template.Name]; exists { return fmt.Errorf(\"template %s already exists\", template.Name) } tr.templates[template.Name] = template return nil } func (tr *TemplateRegistry) Get(name string) (PresetTemplate, error) { tr.mutex.RLock() defer tr.mutex.RUnlock() template, exists := tr.templates[name] if !exists { return PresetTemplate{}, fmt.Errorf(\"template %s not found\", name) } return template, nil } func (tr *TemplateRegistry) List() []PresetTemplate { tr.mutex.RLock() defer tr.mutex.RUnlock() templates := make([]PresetTemplate, 0, len(tr.templates)) for _, template := range tr.templates { templates = append(templates, template) } return templates } Built-in Presets // Monitoring stack preset func init() { registry.Register(PresetTemplate{ Name: \"monitoring-stack\", Description: \"Complete monitoring solution with Prometheus, Grafana, and AlertManager\", Category: \"infrastructure\", Tags: []string{\"monitoring\", \"prometheus\", \"grafana\", \"alertmanager\"}, Builder: func() Builder { return NewMonitoringStackBuilder() }, }) } type MonitoringStackBuilder struct { name string prometheus bool grafana bool alertmanager bool nodeExporter bool namespace string storageSize string retentionTime string } func NewMonitoringStackBuilder() *MonitoringStackBuilder { return \u0026MonitoringStackBuilder{ name: \"monitoring\", prometheus: true, grafana: true, alertmanager: true, nodeExporter: true, namespace: \"monitoring\", storageSize: \"10Gi\", retentionTime: \"15d\", } } func (msb *MonitoringStackBuilder) WithPrometheus(enabled bool) *MonitoringStackBuilder { msb.prometheus = enabled return msb } func (msb *MonitoringStackBuilder) WithGrafana(enabled bool) *MonitoringStackBuilder { msb.grafana = enabled return msb } func (msb *MonitoringStackBuilder) WithStorageSize(size string) *MonitoringStackBuilder { msb.storageSize = size return msb } func (msb *MonitoringStackBuilder) Build() (*stack.Node, error) { nodeBuilder := stack.NewNodeBuilder(msb.name). WithBundle(\"monitoring\") if msb.prometheus { prometheusConfig := generators.NewPrometheusConfig(). WithRetention(msb.retentionTime). WithStorageSize(msb.storageSize). Build() nodeBuilder.WithApplication(\"prometheus\", prometheusConfig) } if msb.grafana { grafanaConfig := generators.NewGrafanaConfig(). WithPrometheusDataSource(msb.prometheus). Build() nodeBuilder.WithApplication(\"grafana\", grafanaConfig) } if msb.alertmanager { alertmanagerConfig := generators.NewAlertManagerConfig(). Build() nodeBuilder.WithApplication(\"alertmanager\", alertmanagerConfig) } return nodeBuilder.Build() } Web Application Preset func init() { registry.Register(PresetTemplate{ Name: \"web-application\", Description: \"Scalable web application with ingress and autoscaling\", Category: \"application\", Tags: []string{\"web\", \"frontend\", \"ingress\", \"autoscaling\"}, Builder: func() Builder { return NewWebApplicationBuilder() }, }) } type WebApplicationBuilder struct { name string image string replicas int32 port int32 ingress bool autoscaling bool minReplicas int32 maxReplicas int32 cpuThreshold int32 resources *corev1.ResourceRequirements } func NewWebApplicationBuilder() *WebApplicationBuilder { return \u0026WebApplicationBuilder{ replicas: 3, port: 80, ingress: true, autoscaling: false, minReplicas: 2, maxReplicas: 10, cpuThreshold: 70, resources: \u0026corev1.ResourceRequirements{ Requests: corev1.ResourceList{ corev1.ResourceCPU: resource.MustParse(\"100m\"), corev1.ResourceMemory: resource.MustParse(\"128Mi\"), }, Limits: corev1.ResourceList{ corev1.ResourceCPU: resource.MustParse(\"500m\"), corev1.ResourceMemory: resource.MustParse(\"512Mi\"), }, }, } } func (wab *WebApplicationBuilder) WithImage(image string) *WebApplicationBuilder { wab.image = image return wab } func (wab *WebApplicationBuilder) WithReplicas(replicas int32) *WebApplicationBuilder { wab.replicas = replicas return wab } func (wab *WebApplicationBuilder) WithAutoscaling(enabled bool) *WebApplicationBuilder { wab.autoscaling = enabled return wab } func (wab *WebApplicationBuilder) WithIngress(enabled bool) *WebApplicationBuilder { wab.ingress = enabled return wab } func (wab *WebApplicationBuilder) Build() (*stack.Application, error) { appConfig := generators.NewAppWorkloadConfig(). WithWorkload(\"Deployment\"). WithImage(wab.image). WithReplicas(wab.replicas). WithPort(wab.port). WithResources(wab.resources) if wab.ingress { appConfig.WithIngress(true) } if wab.autoscaling { appConfig.WithAutoscaling(wab.minReplicas, wab.maxReplicas, wab.cpuThreshold) } return stack.NewApplication(wab.name, \"\", appConfig.Build()), nil } Template Inheritance System type TemplateInheritance struct { baseTemplate string overrides map[string]interface{} mixins []string } type ApplicationTemplate struct { Name string Description string BaseTemplate string DefaultConfig map[string]interface{} RequiredFields []string OptionalFields []string Mixins []Mixin } func NewApplicationFromTemplate(name string, template ApplicationTemplate) *ApplicationBuilder { builder := NewApplicationBuilder(name) // Apply base template configuration if template.BaseTemplate != \"\" { baseTemplate := GetTemplate(template.BaseTemplate) builder.ApplyTemplate(baseTemplate) } // Apply default configuration for key, value := range template.DefaultConfig { builder.SetProperty(key, value) } // Apply mixins for _, mixin := range template.Mixins { builder.ApplyMixin(mixin) } return builder } // Base application template with security and monitoring defaults var BaseApplicationTemplate = ApplicationTemplate{ Name: \"base\", Description: \"Base template with security and monitoring defaults\", DefaultConfig: map[string]interface{}{ \"labels\": map[string]string{ \"managed-by\": \"kure\", \"version\": \"v1\", }, \"securityContext\": \u0026corev1.PodSecurityContext{ RunAsNonRoot: pointer.Bool(true), RunAsUser: pointer.Int64(1000), FSGroup: pointer.Int64(2000), }, \"resources\": \u0026corev1.ResourceRequirements{ Requests: corev1.ResourceList{ corev1.ResourceCPU: resource.MustParse(\"100m\"), corev1.ResourceMemory: resource.MustParse(\"128Mi\"), }, Limits: corev1.ResourceList{ corev1.ResourceCPU: resource.MustParse(\"500m\"), corev1.ResourceMemory: resource.MustParse(\"512Mi\"), }, }, }, Mixins: []Mixin{ SecurityMixin, MonitoringMixin, }, } Mixin System type Mixin struct { Name string Description string Resources []client.Object Labels map[string]string Annotations map[string]string Apply func(*ApplicationBuilder) error } var SecurityMixin = Mixin{ Name: \"security\", Description: \"Security best practices including network policies and pod security\", Apply: func(builder *ApplicationBuilder) error { // Add security context builder.WithSecurityContext(\u0026corev1.PodSecurityContext{ RunAsNonRoot: pointer.Bool(true), RunAsUser: pointer.Int64(1000), }) // Add network policy networkPolicy := kubernetes.CreateNetworkPolicy(builder.name, builder.namespace) builder.AddResource(networkPolicy) // Add pod security policy podSecurityPolicy := kubernetes.CreatePodSecurityPolicy(builder.name) builder.AddResource(podSecurityPolicy) return nil }, } var MonitoringMixin = Mixin{ Name: \"monitoring\", Description: \"Monitoring setup with service monitor and dashboards\", Apply: func(builder *ApplicationBuilder) error { // Add monitoring labels builder.WithLabels(map[string]string{ \"monitoring\": \"enabled\", \"metrics\": \"prometheus\", }) // Add service monitor serviceMonitor := prometheus.CreateServiceMonitor(builder.name, builder.namespace) builder.AddResource(serviceMonitor) // Add grafana dashboard dashboard := grafana.CreateDashboard(builder.name, builder.namespace) builder.AddResource(dashboard) return nil }, } Environment Profiles type EnvironmentProfile struct { Name string Description string DefaultValues map[string]interface{} Transformers []ProfileTransformer } type ProfileTransformer func(*stack.Cluster) error var DevelopmentProfile = EnvironmentProfile{ Name: \"development\", Description: \"Development environment with minimal resources and debugging enabled\", DefaultValues: map[string]interface{}{ \"replicas\": map[string]int32{ \"frontend\": 1, \"backend\": 1, \"database\": 1, }, \"resources\": map[string]corev1.ResourceRequirements{ \"frontend\": { Requests: corev1.ResourceList{ corev1.ResourceCPU: resource.MustParse(\"50m\"), corev1.ResourceMemory: resource.MustParse(\"64Mi\"), }, }, }, \"debug\": true, }, Transformers: []ProfileTransformer{ EnableDebugLogging, DisableAutoscaling, AddDevelopmentLabels, }, } var ProductionProfile = EnvironmentProfile{ Name: \"production\", Description: \"Production environment with high availability and performance\", DefaultValues: map[string]interface{}{ \"replicas\": map[string]int32{ \"frontend\": 3, \"backend\": 5, \"database\": 3, }, \"autoscaling\": true, \"monitoring\": true, \"security\": true, }, Transformers: []ProfileTransformer{ EnableAutoscaling, EnableMonitoring, EnableSecurityPolicies, SetProductionLabels, }, } func (cluster *stack.Cluster) ApplyProfile(profile EnvironmentProfile) error { // Apply default values for key, value := range profile.DefaultValues { if err := cluster.SetProperty(key, value); err != nil { return err } } // Apply transformers for _, transformer := range profile.Transformers { if err := transformer(cluster); err != nil { return err } } return nil } Configuration Validation System type ValidationLevel string const ( ValidationNone ValidationLevel = \"none\" ValidationBasic ValidationLevel = \"basic\" ValidationStrict ValidationLevel = \"strict\" ValidationCustom ValidationLevel = \"custom\" ) type ValidationRule struct { Name string Description string Level ValidationLevel Check func(interface{}) ValidationResult } type ValidationResult struct { Valid bool Errors []ValidationError Warnings []ValidationWarning Suggestions []ValidationSuggestion } type ValidationError struct { Field string Message string Code string Hint string } var ClusterValidationRules = []ValidationRule{ { Name: \"cluster-name-format\", Description: \"Cluster name must follow Kubernetes naming conventions\", Level: ValidationBasic, Check: func(obj interface{}) ValidationResult { cluster := obj.(*stack.Cluster) if !isValidKubernetesName(cluster.Name) { return ValidationResult{ Valid: false, Errors: []ValidationError{{ Field: \"name\", Message: \"Invalid cluster name format\", Code: \"INVALID_NAME_FORMAT\", Hint: \"Name must be lowercase alphanumeric with hyphens\", }}, } } return ValidationResult{Valid: true} }, }, { Name: \"gitops-consistency\", Description: \"GitOps configuration must be consistent across the cluster\", Level: ValidationStrict, Check: func(obj interface{}) ValidationResult { cluster := obj.(*stack.Cluster) if cluster.GitOps == nil { return ValidationResult{ Valid: false, Errors: []ValidationError{{ Field: \"gitops\", Message: \"GitOps configuration is required\", Code: \"MISSING_GITOPS_CONFIG\", Hint: \"Add GitOps configuration with type 'flux' or 'argocd'\", }}, } } return ValidationResult{Valid: true} }, }, } func ValidateCluster(cluster *stack.Cluster, level ValidationLevel) ValidationResult { result := ValidationResult{Valid: true} for _, rule := range ClusterValidationRules { if rule.Level \u003c= level { ruleResult := rule.Check(cluster) if !ruleResult.Valid { result.Valid = false result.Errors = append(result.Errors, ruleResult.Errors...) } result.Warnings = append(result.Warnings, ruleResult.Warnings...) result.Suggestions = append(result.Suggestions, ruleResult.Suggestions...) } } return result } Implementation Roadmap Phase 1: Foundation (Months 1-2) Core Builder System Implement fluent builder pattern for all stack components Create base template system with inheritance Add comprehensive validation framework Build configuration export/import functionality Deliverables:\nFluent API for cluster construction Basic validation system Template registry infrastructure Code generation utilities Success Criteria:\n80% reduction in boilerplate code for common configurations Zero compilation errors for generated configurations All existing Kure functionality accessible through builders Basic Presets Monitoring stack preset (Prometheus + Grafana) Web application preset (Deployment + Service + Ingress) Database preset (StatefulSet + PVC + Service) Security baseline preset (RBAC + Network Policies) Deliverables:\n4 core preset templates Template documentation and examples Template testing framework Success Criteria:\nNew users can deploy monitoring stack in \u003c5 minutes Presets generate production-ready configurations 90% test coverage for all presets Phase 2: Core UI Components (Months 3-4) Hierarchical Tree Interface Tree component with expand/collapse functionality Context menus for add/edit/delete operations Property panel with dynamic form generation Real-time validation display Deliverables:\nReact tree component library Property editing system Validation UI components Tree state management Success Criteria:\nUsers can navigate 100+ item trees smoothly Property changes reflect immediately in tree Validation errors visible within 100ms Configuration Wizard Multi-step wizard component framework Dynamic step generation based on templates Progress tracking and branching logic Integration with builder system Deliverables:\nWizard framework with step components Template-driven step generation Progress indicators and navigation Builder integration layer Success Criteria:\nFirst-time users complete setup in \u003c10 minutes \u003c5% abandonment rate in wizard flow Generated configs match expert-created equivalents Phase 3: Advanced Features (Months 5-6) Visual Builder Interface Canvas component with drag-drop functionality Component palette with preset blocks Connection visualization and editing Layout algorithms for automatic arrangement Deliverables:\nVisual editor with drag-drop Component connection system Multiple layout algorithms Export to tree/code views Success Criteria:\nArchitects can design complex clusters visually Generated diagrams suitable for documentation Visual and code representations stay synchronized Code Editor Integration Syntax-highlighted code editor Real-time compilation and validation Auto-completion for Kure APIs Bidirectional sync with visual editors Deliverables:\nCode editor with Kure syntax support Auto-completion engine Real-time error highlighting Multi-format export (Go, YAML, JSON) Success Criteria:\nPower users prefer code editor for complex edits Auto-completion reduces API lookup time by 70% Code validation catches 95% of errors before compilation Phase 4: Collaboration \u0026 Polish (Months 7-8) Template Sharing \u0026 Collaboration Template marketplace with discovery Version control integration for templates Team sharing and permissions Template composition and inheritance Deliverables:\nTemplate marketplace UI Git integration for template versioning Team collaboration features Template dependency management Success Criteria:\n50+ community-contributed templates Teams share templates across projects Template reuse reduces config time by 60% Documentation \u0026 Onboarding Interactive tutorials integrated in UI Context-sensitive help system Video tutorials and documentation Migration guides for existing users Deliverables:\nInteractive tutorial system Comprehensive documentation site Video tutorial library Migration tooling Success Criteria:\n90% of new users complete onboarding successfully Support ticket volume reduced by 40% Documentation satisfaction score \u003e4.5/5 Success Metrics Primary KPIs User Adoption Metrics Monthly Active Users: Target 500+ regular users within 6 months Template Usage: 70% of clusters created using presets/templates Completion Rate: 85% wizard completion rate for new users Retention: 60% user retention after 30 days Developer Experience Metrics Time to First Success: \u003c10 minutes for new user to deploy working cluster Configuration Time: 50% reduction in time to configure complex clusters Error Rate: \u003c5% of generated configurations fail validation API Discovery: 80% of users discover new features through UI Quality Metrics Bug Reports: \u003c2 critical bugs per release Performance: UI responds to user actions in \u003c100ms Accessibility: WCAG 2.1 AA compliance Mobile: Basic functionality on tablet devices Secondary KPIs Community Engagement Template Contributions: 10+ community templates per month Documentation: 90% of features have up-to-date documentation Support: Average issue resolution time \u003c24 hours Feedback: \u003e4.0/5 average satisfaction rating Technical Excellence Test Coverage: \u003e90% code coverage for UI components Performance: \u003c1MB initial bundle size Compatibility: Support for latest 2 versions of major browsers Reliability: 99.9% uptime for hosted components User Feedback Collection Quantitative Methods Analytics Tracking: User journey analysis, feature usage patterns A/B Testing: Interface variations, onboarding flows Performance Monitoring: Load times, error rates, user actions Survey Data: NPS scores, satisfaction ratings, feature requests Qualitative Methods User Interviews: Monthly sessions with power users and beginners Usability Testing: Task-based testing with screen recording Support Ticket Analysis: Common issues and confusion points Community Feedback: GitHub issues, forum discussions Continuous Improvement Process Weekly Reviews Performance metrics analysis Bug triage and prioritization User feedback review Feature usage assessment Monthly Assessments User journey optimization Feature success evaluation Documentation gap analysis Community engagement review Quarterly Planning Roadmap adjustment based on metrics Major feature planning User research planning Technology stack evaluation Conclusion This comprehensive UX design document outlines a path to transform Kure from a powerful but complex library into an accessible, multi-modal platform that serves users across the entire expertise spectrum. By implementing progressive disclosure, multi-modal interfaces, and intelligent configuration management, we can maintain Kure’s technical excellence while dramatically improving its usability.\nThe success of this initiative will be measured not just in adoption metrics, but in the quality of Kubernetes configurations users create and the confidence they gain in managing complex infrastructure. Through careful implementation of the roadmap and continuous feedback collection, Kure can become the definitive tool for GitOps-native cluster configuration.\nNext Steps:\nValidate design concepts with target users Create detailed technical specifications for Phase 1 Begin implementation of core builder system Establish user feedback collection processes Start building community around template sharing This document is a living specification that will evolve based on user feedback and implementation learnings.",
    "description": "Kure End User Experience Design Document Version: 1.0\nDate: August 2025\nStatus: Design Specification\nExecutive Summary This document outlines a comprehensive user experience strategy for Kure, a Go library for programmatically building Kubernetes resources for GitOps workflows. The design focuses on creating multiple interaction modalities that serve different user types while maintaining Kure’s core strengths of type safety, composability, and GitOps compatibility.\nKey Design Principles:\nProgressive Complexity: Simple defaults with expert-level customization Multi-Modal Interaction: Visual, programmatic, and hybrid approaches Type-Safe Configuration: Leveraging Kure’s existing validation and GVK system GitOps Native: Seamless integration with Flux and ArgoCD workflows Table of Contents User Research \u0026 Personas Current State Analysis Vision \u0026 Design Principles User Journey Maps Multi-Modal Interface Design Configuration Management System Implementation Roadmap Success Metrics User Research \u0026 Personas Primary User Personas 1. Platform Engineer (Sarah) Role: Senior Platform Engineer at mid-size tech company Experience: 5+ years Kubernetes, familiar with GitOps concepts Goals: Standardize cluster configurations, reduce operational overhead Pain Points: Complex YAML management, ensuring consistency across environments Preferred Interaction: Programmatic APIs with visual validation 2. DevOps Beginner (Mike) Role: Junior DevOps Engineer, transitioning from traditional ops Experience: 1 year Kubernetes, new to GitOps Goals: Learn best practices, avoid configuration mistakes Pain Points: Overwhelmed by Kubernetes complexity, unclear dependencies Preferred Interaction: Guided workflows with explanations 3. Solutions Architect (Alex) Role: Technical lead designing multi-cluster architectures Experience: 8+ years infrastructure, deep Kubernetes knowledge Goals: Design scalable, maintainable cluster architectures Pain Points: Visualizing complex dependencies, communicating designs to teams Preferred Interaction: Visual modeling with code generation 4. Development Team Lead (Jordan) Role: Engineering manager overseeing application deployment Experience: 3 years Kubernetes, focused on application concerns Goals: Self-service application deployment, consistent configurations Pain Points: Waiting for platform team, inconsistent environments Preferred Interaction: Template-based configuration with customization User Needs Analysis User Type Primary Need Secondary Need Success Criteria Platform Engineer Standardization \u0026 Automation Visual Validation Reduced config drift, faster onboarding DevOps Beginner Learning \u0026 Guidance Error Prevention Successful deployments, increased confidence Solutions Architect Design \u0026 Communication Code Generation Clear architecture docs, team alignment Development Team Lead Self-Service \u0026 Consistency Template Management Faster deployments, reduced dependencies Current State Analysis Existing Interaction Model Currently, users interact with Kure through direct Go code:",
    "tags": [],
    "title": "UX Design",
    "uri": "/architecture/ux-design/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Packages \u003e Launcher",
    "content": "Launcher Module - Code Design Overview The launcher module is the core engine for the Kurel package system, implementing a declarative approach to generating Kubernetes manifests with validation and customization capabilities. This document captures all design decisions made during the architecture planning phase.\nDesign Philosophy Core Principle: “Kurel just generates YAML” - The launcher is a declarative system for generating Kubernetes manifests, not a runtime system or orchestrator.\nArchitecture Decisions 1. Core Package Structure Decision: Immutable PackageDefinition with deep copy support\n// Immutable package definition with thread-safe access type PackageDefinition struct { Path string Metadata KurelMetadata // From kurel: key in parameters.yaml Parameters ParameterMap // Default parameters including global: Resources []Resource // Base K8s manifests Patches []Patch // Available patches with metadata mu sync.RWMutex // Protect concurrent reads } // DeepCopy creates an independent copy for safe mutation func (pd *PackageDefinition) DeepCopy() *PackageDefinition { pd.mu.RLock() defer pd.mu.RUnlock() return \u0026PackageDefinition{ Path: pd.Path, Metadata: pd.Metadata, // struct copy Parameters: deepCopyMap(pd.Parameters), Resources: deepCopyResources(pd.Resources), Patches: deepCopyPatches(pd.Patches), } } // Instance with user customization type PackageInstance struct { Definition *PackageDefinition // Immutable package reference UserValues ParameterMap // User-provided overrides Resolved ParameterMapWithSource // Final values with source tracking LocalPath string // Path to .local.kurel if exists } // Track parameter sources for debugging type ParameterSource struct { Value interface{} Location string // \"package\", \"local\", \"default\" File string // Which file it came from Line int // Line number if applicable } type ParameterMapWithSource map[string]ParameterSource Rationale:\nDeep copy prevents mutation bugs in concurrent processing Source tracking aids debugging Thread-safe access for concurrent reads Package definitions remain cacheable and reusable 2. Interface Organization \u0026 Common Options Decision: Small interfaces with centralized LauncherOptions\n// LauncherOptions centralizes common configuration type LauncherOptions struct { Logger Logger MaxDepth int // Variable resolution depth Timeout time.Duration // Operation timeout MaxWorkers int // Concurrent processing CacheDir string // Schema cache directory Debug bool // Enable debug output Verbose bool // Verbose logging ProgressFunc func(string) // Progress callback } // DefaultOptions provides sensible defaults func DefaultOptions() *LauncherOptions { return \u0026LauncherOptions{ Logger: NewDefaultLogger(), MaxDepth: 10, Timeout: 30 * time.Second, MaxWorkers: runtime.NumCPU(), CacheDir: \"/tmp/kurel-cache\", Debug: false, Verbose: false, } } Decision: Small, focused interfaces following Go idioms\n// pkg/launcher/interfaces.go - Small, composable interfaces type DefinitionLoader interface { LoadDefinition(ctx context.Context, path string, opts *LauncherOptions) (*PackageDefinition, error) } type ResourceLoader interface { LoadResources(ctx context.Context, path string) ([]Resource, error) } type PatchLoader interface { LoadPatches(ctx context.Context, path string) ([]Patch, error) } // Compose when needed type PackageLoader interface { DefinitionLoader ResourceLoader PatchLoader } type Resolver interface { Resolve(ctx context.Context, base, overrides ParameterMap, opts *LauncherOptions) (ParameterMapWithSource, error) DebugVariableGraph(params ParameterMap) string // Generate dependency graph } type Builder interface { Build(ctx context.Context, inst *PackageInstance, buildOpts BuildOptions, opts *LauncherOptions) error } Rationale:\nFollows Go’s preference for small interfaces (like io.Reader, io.Writer) Enables better testing through focused mocks Clean separation of contracts from data types Supports interface composition 3. Package Loading Strategy Decision: Hybrid error handling with context support and size limits\nCritical files (parameters.yaml): Must load successfully or fail immediately Other files (resources, patches): Collect all errors, load what’s possible Size limits: Enforce maximum package size (50MB) and resource count (1000) Context cancellation: Support timeout and cancellation const ( MaxPackageSize = 50 * 1024 * 1024 // 50MB hard limit WarnPackageSize = 10 * 1024 * 1024 // 10MB warning MaxResourceCount = 1000 // Max resources ) func (l *defaultLoader) LoadDefinition(ctx context.Context, path string) (*PackageDefinition, error) { // Check package size first if err := l.validatePackageSize(path); err != nil { return nil, err } // Critical: parameters.yaml MUST load params, err := l.loadParameters(ctx, filepath.Join(path, \"parameters.yaml\")) if err != nil { return nil, fmt.Errorf(\"critical: parameters.yaml: %w\", err) } // Best effort for others with context var errs []error resources, resourceErrs := l.loadAllResources(ctx, path) patches, patchErrs := l.loadAllPatches(ctx, path) errs = append(errs, resourceErrs...) errs = append(errs, patchErrs...) def := \u0026PackageDefinition{ Path: path, Metadata: extractMetadata(params), Parameters: params, Resources: resources, Patches: patches, } if len(errs) \u003e 0 { return def, \u0026LoadErrors{ PartialDefinition: def, Issues: errs, } } return def, nil } Rationale:\nCan’t proceed without valid parameters.yaml Size limits prevent memory issues (based on typical Helm chart sizes) Context support enables cancellation See all syntax errors at once for debugging 4. Variable Resolution Decision: No inline defaults, configurable depth\nVariables must exist in parameters.yaml (no ${var|default} syntax) Configurable maximum nesting depth to prevent infinite recursion Parameters.yaml is where all defaults are defined type variableResolver struct { maxDepth int // Configurable, default 10 } // Resolution without inline defaults // ${monitoring.namespace} - ERROR if not defined // No fallback syntax supported Rationale:\nKeeps variable syntax simple All defaults in one place (parameters.yaml) Prevents infinite recursion while allowing deep nesting 5. Patch Processing Decision: Strict validation with uniqueness enforcement and debug visualization\nName uniqueness: Enforced at load time to prevent confusion Conflicts: Hard error, refuse to continue Auto-enable: Verbose logging to stderr Missing targets: Error, patches must match something Application order: Package patches first (by numeric prefix), then local patches (can override) Failure handling: Patches MUST apply successfully or error - no silent failures Debug support: Patch dependency graph visualization // Enforce patch name uniqueness at load time func (l *defaultLoader) loadPatches(ctx context.Context, path string) ([]Patch, error) { patches := []Patch{} seen := make(map[string]string) // name -\u003e file path err := filepath.Walk(path, func(p string, info os.FileInfo, err error) error { if !strings.HasSuffix(p, \".kpatch\") { return nil } patch, err := l.loadPatch(p) if err != nil { return err } // Check uniqueness if existing, exists := seen[patch.Name]; exists { return fmt.Errorf(\"duplicate patch name '%s' in %s and %s\", patch.Name, existing, p) } seen[patch.Name] = p patches = append(patches, patch) return nil }) return patches, err } // Hard error on conflicts if hasConflicts(enabledPatches) { return nil, fmt.Errorf(\"conflict: %s and %s cannot both be enabled\", p1, p2) } // Verbose logging during build (--verbose flag) INFO: Enabling patch 10-monitoring.kpatch (monitoring.enabled=true) INFO: Auto-enabling 05-metrics.kpatch (required by 10-monitoring) DEBUG: Applying patch 10-monitoring.kpatch to deployment/prometheus DEBUG: Successfully patched field spec.template.spec.containers[0].resources // Error if patch doesn't match if matchCount == 0 { return fmt.Errorf(\"patch %s targets non-existent resource: deployment.frontend\", patchName) } // Patch works on deep copy to preserve immutability func (p *patchProcessor) ApplyPatches(ctx context.Context, def *PackageDefinition, patches []Patch, params ParameterMap) (*PackageDefinition, error) { // Work on deep copy, never mutate original working := def.DeepCopy() for _, patch := range patches { result, err := p.applyPatch(working, patch, params) if err != nil { return nil, fmt.Errorf(\"patch %s failed: %w\", patch.Name, err) } working = result } return working, nil } // Debug visualization for patch dependencies func (p *patchProcessor) DebugPatchGraph(patches []Patch) string { var buf strings.Builder buf.WriteString(\"digraph patches {\\n\") for _, patch := range patches { if patch.Metadata != nil { for _, req := range patch.Metadata.Requires { buf.WriteString(fmt.Sprintf(\" \\\"%s\\\" -\u003e \\\"%s\\\";\\n\", patch.Name, req)) } for _, conflict := range patch.Metadata.Conflicts { buf.WriteString(fmt.Sprintf(\" \\\"%s\\\" -\u003e \\\"%s\\\" [color=red,style=dashed];\\n\", patch.Name, conflict)) } } } buf.WriteString(\"}\\n\") return buf.String() } Rationale:\nImmutability prevents subtle bugs Name uniqueness prevents configuration errors Debug visualization aids troubleshooting Clear visibility into auto-enabled dependencies No silent failures 6. Schema Generation Decision: Hybrid approach with memoization and type validation\nBundle schemas for resources known in internal/ packages Package maintainers can specify CRD schema URLs in parameters.yaml Auto-generate if missing, allow explicit regeneration Memoize path tracing for performance Validate type consistency across patches # In parameters.yaml kurel: name: my-app schemas: - https://raw.githubusercontent.com/cert-manager/cert-manager/v1.13.0/deploy/crds/crd-certificates.yaml - https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.68.0/deploy/crds/crd-prometheuses.yaml // Memoized path tracer for performance type memoizedPathTracer struct { cache map[string]*TraceResult // path:variable → schema mapping mu sync.RWMutex } func (t *memoizedPathTracer) TracePath(patchPath, variable string) (*TraceResult, error) { key := fmt.Sprintf(\"%s:%s\", patchPath, variable) t.mu.RLock() if result, ok := t.cache[key]; ok { t.mu.RUnlock() return result, nil } t.mu.RUnlock() // Compute if not cached result, err := t.doTrace(patchPath, variable) if err != nil { return nil, err } t.mu.Lock() t.cache[key] = result t.mu.Unlock() return result, nil } // Validate type consistency across patches func (g *SchemaGenerator) ValidateSchemaMerge(patches []Patch) error { typeMap := make(map[string]string) // variable → expected type for _, patch := range patches { traces := g.tracer.TraceAll(patch.Content) for variable, trace := range traces { expectedType := trace.Schema.Type if existing, ok := typeMap[variable]; ok { if existing != expectedType { return fmt.Errorf(\"type conflict for %s: patch %s expects %s, but %s expected\", variable, patch.Name, expectedType, existing) } } else { typeMap[variable] = expectedType } } } return nil } Rationale:\nMemoization improves performance for repeated operations Type validation prevents schema conflicts Leverages existing Kure knowledge Extensible for custom CRDs 7. Validation System Decision: Errors block, warnings don’t; concurrent validation for performance\ntype ValidationResult struct { Errors []ValidationError Warnings []ValidationWarning } type ConcurrentValidator struct { maxWorkers int schemaGen SchemaGenerator } func (v *ConcurrentValidator) ValidateInstance(ctx context.Context, inst *PackageInstance) ValidationResult { // Validate resources concurrently for performance numWorkers := runtime.NumCPU() if len(inst.Definition.Resources) \u003c numWorkers { numWorkers = len(inst.Definition.Resources) } work := make(chan Resource, len(inst.Definition.Resources)) results := make(chan ValidationError, len(inst.Definition.Resources)) // Start workers var wg sync.WaitGroup for i := 0; i \u003c numWorkers; i++ { wg.Add(1) go func() { defer wg.Done() for resource := range work { if err := v.validateResource(ctx, resource); err != nil { results \u003c- ValidationError{Resource: resource.GetName(), Error: err} } } }() } // Queue work for _, r := range inst.Definition.Resources { work \u003c- r } close(work) // Collect results go func() { wg.Wait() close(results) }() var result ValidationResult for err := range results { result.Errors = append(result.Errors, err) } return result } Rationale:\nConcurrent validation for Helm-like performance Clear distinction between blocking and non-blocking issues Best-effort validation based on available information Worker pool pattern prevents resource exhaustion 8. Output Generation Decision: No GitOps-specific support, configurable output format\nKurel only manages kurel.gokure.dev/ annotations for phase organization Actual deployment handled by GitOps tools (Flux/ArgoCD) Configurable output: stdout (default/dry-run), single file, by-kind, by-resource Multi-document YAML files are properly handled # Default: multi-doc YAML to stdout (dry-run mode) kurel build my-app.kurel/ # Output to directory with by-kind grouping kurel build my-app.kurel/ -o out/ --format=by-kind # Single file output kurel build my-app.kurel/ -o manifests.yaml --format=single # JSON output kurel build my-app.kurel/ --output-format=json # Verbose mode for debugging kurel build my-app.kurel/ --verbose Rationale:\nKeeps kurel focused on YAML generation only Default stdout output serves as dry-run mode Flexible output for different workflows Clean separation of concerns Verbose mode aids in debugging patch application 9. Local Extensions Decision: Full integration with validation\nLocal patches CAN reference package patches in dependencies Parameter conflicts are validated for compatibility Local extensions can only add, not replace # my-app.local.kurel/patches/50-custom.yaml requires: - \"features/10-monitoring.kpatch\" # Can reference package patches conflicts: - \"features/20-basic-monitoring.kpatch\" # Can conflict with package # Parameter override validation # Error if local changes parameter type/structure incompatibly Rationale:\nAllows sophisticated customization Prevents breaking changes Maintains package integrity 10. CLI Integration Decision: YAML to stdout, logs to stderr, with timeout and progress indication\nconst ( DefaultBuildTimeout = 30 * time.Second // Similar to Helm MaxBuildTimeout = 5 * time.Minute ) type buildOptions struct { valuesFile string outputPath string outputFormat string outputType string localPath string timeout time.Duration verbose bool dryRun bool quiet bool showProgress bool } // Build command with proper flag handling func newBuildCommand() *cobra.Command { var opts buildOptions cmd := \u0026cobra.Command{ Use: \"build \u003cpackage\u003e\", Short: \"Build Kubernetes manifests from kurel package\", RunE: func(cmd *cobra.Command, args []string) error { // Apply timeout ctx, cancel := context.WithTimeout(context.Background(), opts.timeout) defer cancel() // Progress indication for long operations var progress ProgressReporter if opts.showProgress \u0026\u0026 !opts.quiet { progress = NewProgressBar(\"Building package...\") defer progress.Finish() } return runBuild(ctx, args[0], opts, progress) }, } flags := cmd.Flags() flags.StringVarP(\u0026opts.valuesFile, \"values\", \"f\", \"\", \"Values file\") flags.StringVarP(\u0026opts.outputPath, \"output\", \"o\", \"\", \"Output path (default: stdout)\") flags.DurationVar(\u0026opts.timeout, \"timeout\", DefaultBuildTimeout, \"Build timeout\") flags.BoolVar(\u0026opts.dryRun, \"dry-run\", false, \"Print to stdout without writing\") flags.BoolVarP(\u0026opts.verbose, \"verbose\", \"v\", false, \"Verbose output\") flags.BoolVar(\u0026opts.showProgress, \"progress\", true, \"Show progress bar\") return cmd } Rationale:\nTimeout prevents hanging builds Progress indication for better UX Unix philosophy: stdout for data, stderr for logs Matches Helm’s performance characteristics Module Organization pkg/launcher/ ├── interfaces.go # All public interfaces ├── types.go # Core data types with deep copy ├── options.go # LauncherOptions and configuration ├── loader.go # Package loading with uniqueness checks ├── variables.go # Variable resolution with source tracking ├── patches.go # Patch processing with immutability ├── schema.go # Schema generation with memoization ├── validator.go # Validation logic ├── builder.go # Manifest building and output ├── extensions.go # Local extension handling ├── errors.go # Custom error types ├── debug.go # Debug visualization (graphs, etc.) ├── deepcopy.go # Deep copy utilities └── testdata/ # Test fixtures ├── packages/ # Sample packages for testing └── mocks/ # Mock implementations for testing Error Handling Philosophy Fail fast for critical errors (missing parameters.yaml) Abort all on patch failures (transactional behavior) Use existing error patterns from pkg/errors Clear error messages with context and suggestions Distinguish between errors (blocking) and warnings (advisory) Context cancellation respected throughout // Leverage existing Kure error patterns type LoadErrors struct { errors.BaseError PartialDefinition *PackageDefinition Issues []error } func (e *LoadErrors) Unwrap() []error { return e.Issues } // Transactional patch application func (p *patchProcessor) ApplyPatches(ctx context.Context, resources []Resource, patches []Patch) ([]Resource, error) { // Work on copy for rollback capability working := make([]Resource, len(resources)) copy(working, resources) for i, patch := range patches { select { case \u003c-ctx.Done(): return nil, fmt.Errorf(\"cancelled at patch %d/%d: %w\", i+1, len(patches), ctx.Err()) default: } result, err := p.applyPatch(working, patch) if err != nil { // Fail fast - abort all return nil, fmt.Errorf(\"patch %s failed (aborting all): %w\", patch.Name, err) } working = result } return working, nil } Testing Strategy Table-driven tests: Following Go best practices with subtests Individual component tests: Each component independently testable Benchmarks: Ensure performance matches Helm Mock filesystem: Using afero for loader testing Integration tests: Full package processing flows Fixture packages: Real-world package examples in testdata/ Concurrent testing: Validate thread safety Individual Component Testability // Test resolver independently func TestResolverIndependent(t *testing.T) { opts := \u0026LauncherOptions{MaxDepth: 5} resolver := NewResolver(opts) params := ParameterMap{ \"app\": \"myapp\", \"tag\": \"v1.0\", \"image\": \"${app}:${tag}\", } resolved, err := resolver.Resolve(context.Background(), params, nil, opts) require.NoError(t, err) assert.Equal(t, \"myapp:v1.0\", resolved[\"image\"].Value) } // Test patch processor independently func TestPatchProcessorIndependent(t *testing.T) { opts := DefaultOptions() processor := NewPatchProcessor(opts) // Mock resources def := \u0026PackageDefinition{ Resources: []Resource{ {Kind: \"Deployment\", Metadata: metav1.ObjectMeta{Name: \"app\"}}, }, } patch := Patch{ Name: \"scale\", Content: \"[deployment.app]\\nspec.replicas: 3\", } result, err := processor.ApplyPatches(context.Background(), def, []Patch{patch}, ParameterMap{}) require.NoError(t, err) assert.NotEqual(t, def, result) // Ensure deep copy } // Test validator independently with mock schema func TestValidatorIndependent(t *testing.T) { mockSchema := \u0026MockSchemaGenerator{ schema: \u0026Schema{ Properties: map[string]SchemaProperty{ \"replicas\": {Type: \"integer\", Minimum: 1}, }, }, } opts := DefaultOptions() validator := NewValidator(opts, WithSchemaGenerator(mockSchema)) params := ParameterMap{\"replicas\": 0} result := validator.ValidateParameters(context.Background(), params) assert.Len(t, result.Errors, 1) assert.Contains(t, result.Errors[0].Error(), \"below minimum\") } // Table-driven test example func TestVariableResolver(t *testing.T) { tests := []struct { name string base ParameterMap overrides ParameterMap want ParameterMap wantErr string }{ { name: \"simple_substitution\", base: ParameterMap{\"app\": \"myapp\", \"image\": \"${app}:latest\"}, want: ParameterMap{\"app\": \"myapp\", \"image\": \"myapp:latest\"}, }, { name: \"circular_dependency\", base: ParameterMap{\"a\": \"${b}\", \"b\": \"${a}\"}, wantErr: \"circular dependency\", }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { resolver := NewResolver() ctx := context.Background() got, err := resolver.Resolve(ctx, tt.base, tt.overrides) if tt.wantErr != \"\" { require.Error(t, err) assert.Contains(t, err.Error(), tt.wantErr) return } require.NoError(t, err) assert.Equal(t, tt.want, got) }) } } // Benchmark example func BenchmarkBuildPackage(b *testing.B) { packages := []struct { name string resources int target time.Duration }{ {\"small\", 10, 100 * time.Millisecond}, {\"medium\", 50, 500 * time.Millisecond}, {\"large\", 200, 2 * time.Second}, } for _, pkg := range packages { b.Run(pkg.name, func(b *testing.B) { p := generateTestPackage(pkg.resources) b.ResetTimer() for i := 0; i \u003c b.N; i++ { ctx := context.Background() _, err := Build(ctx, p, BuildOptions{}) require.NoError(b, err) } }) } } Performance Considerations Concurrent processing: Use worker pools for CPU-intensive operations Memory limits: Enforce package size limits (50MB max, 10MB warning) Streaming: Stream large resource sets to avoid memory spikes Caching: Cache schemas with TTL for repeated operations Context cancellation: Support timeouts and early termination Performance targets (matching Helm): Small packages (1-20 resources): \u003c 100ms Medium packages (21-100 resources): \u003c 500ms Large packages (101-500 resources): \u003c 2s X-Large packages (500+ resources): \u003c 5s // Concurrent validation with worker pool func (v *Validator) ValidateConcurrent(ctx context.Context, resources []Resource) []error { workers := runtime.NumCPU() if len(resources) \u003c workers { workers = len(resources) } sem := make(chan struct{}, workers) errChan := make(chan error, len(resources)) var wg sync.WaitGroup for _, r := range resources { wg.Add(1) go func(resource Resource) { defer wg.Done() select { case sem \u003c- struct{}{}: defer func() { \u003c-sem }() if err := v.validate(resource); err != nil { errChan \u003c- err } case \u003c-ctx.Done(): errChan \u003c- ctx.Err() } }(r) } go func() { wg.Wait() close(errChan) }() var errors []error for err := range errChan { errors = append(errors, err) } return errors } Security Considerations Path traversal protection in package loading URL validation for schema URLs Variable injection prevention in resolution Resource validation against schemas No direct secret creation - only references via external-secrets or similar patterns Sensitive data handling - parameters may contain references but not actual secrets Future Extensibility Points Plugin system for custom validators (future consideration) Remote package loading (git, https) Package signing and verification Advanced patch operations (JSONPatch, strategic merge) Dependency resolution between packages Observability and metrics (future consideration) Schema merging validation - Detect type mismatches when patches imply incompatible schemas Interactive debug mode - Step through patch application Package composition - Combine multiple packages safely Design Constraints No templating engines (use patches) No runtime operations (just generate YAML) No cluster connectivity required No package registry dependency Deterministic output (same input = same output) No direct secret creation (use external references) Patches must succeed or error (no silent failures) Local patches can override package patches",
    "description": "Launcher Module - Code Design Overview The launcher module is the core engine for the Kurel package system, implementing a declarative approach to generating Kubernetes manifests with validation and customization capabilities. This document captures all design decisions made during the architecture planning phase.\nDesign Philosophy Core Principle: “Kurel just generates YAML” - The launcher is a declarative system for generating Kubernetes manifests, not a runtime system or orchestrator.\nArchitecture Decisions 1. Core Package Structure Decision: Immutable PackageDefinition with deep copy support",
    "tags": [],
    "title": "Code Design",
    "uri": "/packages/launcher/code-design/index.html"
  },
  {
    "breadcrumb": "Go Kure",
    "content": "Examples Explore practical examples of using Kure to generate Kubernetes configurations.\nPatches - Declarative patching examples Generators - Resource generation examples Kurel Frigate - Building a kurel package Validation - Resource validation examples",
    "description": "Examples Explore practical examples of using Kure to generate Kubernetes configurations.\nPatches - Declarative patching examples Generators - Resource generation examples Kurel Frigate - Building a kurel package Validation - Resource validation examples",
    "tags": [],
    "title": "Examples",
    "uri": "/examples/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Packages",
    "content": "Layout Package The layout package handles manifest organization and directory structure generation. It transforms Kure’s in-memory stack representation into organized directory structures with proper kustomization.yaml files that GitOps tools can consume.\nSee the detailed documentation for the full layout system reference.\nAPI Reference pkg.go.dev/github.com/go-kure/kure/pkg/stack/layout",
    "description": "Layout Package The layout package handles manifest organization and directory structure generation. It transforms Kure’s in-memory stack representation into organized directory structures with proper kustomization.yaml files that GitOps tools can consume.\nSee the detailed documentation for the full layout system reference.\nAPI Reference pkg.go.dev/github.com/go-kure/kure/pkg/stack/layout",
    "tags": [],
    "title": "Layout",
    "uri": "/packages/layout/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Packages \u003e Patch",
    "content": "Advanced Path Resolution and Type Inference This document provides an in-depth technical guide to the sophisticated path resolution and type inference systems in the Kure patch module.\nPath Resolution Architecture PathPart Structure Each patch path is decomposed into structured components:\ntype PathPart struct { Field string // The field name (e.g., \"containers\", \"ports\") MatchType string // \"\", \"index\", or \"key\" MatchValue string // The match criteria (\"0\", \"name=main\") } Path Parsing Process The ParsePatchPath() function converts dot-notation paths into structured segments:\nspec.containers[name=main].image ↓ [ {Field: \"spec\"}, {Field: \"containers\", MatchType: \"key\", MatchValue: \"name=main\"}, {Field: \"image\"} ] Context-Aware Resolution The patch system uses intelligent context-aware resolution:\nResource Type Detection: Different behavior for Deployments vs Services vs ConfigMaps Field Validation: Checks if target fields exist in the resource schema List vs Object Disambiguation: Automatically determines if a path targets a list or object field TOML Header Intelligence Semantic Path Mapping TOML headers are intelligently mapped to Kubernetes paths:\n[deployment.app.containers.name=main] # Automatically resolves to: spec.template.spec.containers[name=main] [service.app.ports.name=http] # Automatically resolves to: spec.ports[name=http] [configmap.config] # Automatically resolves to: data.* Header Resolution Algorithm Resource Identification: Parse kind.name from header Context Mapping: Apply resource-specific path transformations Selector Extraction: Extract list selectors and match criteria Path Validation: Verify the resolved path exists in the target resource Resource-Specific Mappings Deployment Resources containers.* → spec.template.spec.containers.* volumes.* → spec.template.spec.volumes.* env.* → spec.template.spec.containers[].env.* Service Resources ports.* → spec.ports.* selector.* → spec.selector.* ConfigMap/Secret Resources Direct field access to data.* fields Automatic string/binary data type detection Type Inference System Automatic Type Detection The inferValueType() function performs context-aware type inference:\n// Port numbers \"8080\" → int64(8080) // when field contains \"port\" \"80\" → int64(80) // when field is \"containerPort\", \"targetPort\" // Resource quantities \"500m\" → \"500m\" // preserved as string for Kubernetes resource.Quantity \"1Gi\" → \"1Gi\" // preserved as string for memory quantities // Boolean values \"true\" → true // when field suggests boolean context \"enabled\" → true // when field is \"enabled\", \"debug\", etc. // Replica counts \"3\" → int64(3) // when field is \"replicas\", \"instances\" Context-Driven Inference The system considers multiple context clues:\nField Name Patterns: port, replicas, timeout, enabled Parent Path Context: resources.limits.*, spec.ports.* Value Format Recognition: Kubernetes quantity formats, duration strings Resource Type Context: Different inference rules for different Kubernetes kinds Kubernetes Compatibility All inferred types are compatible with unstructured.Unstructured:\nfunc convertValueForUnstructured(value interface{}) interface{} { switch v := value.(type) { case int: return int64(v) // Kubernetes expects int64 case int32: return int64(v) // Convert to standard int64 case float32: return float64(v) // Convert to standard float64 // ... handle maps and slices recursively } } List Resolution Algorithms Index Resolution The resolveListIndex() function handles multiple selector types:\n// Numeric index \"2\" → index 2 // Negative index (from end) \"-1\" → len(list) - 1 // Key-value selector \"name=main\" → find first item where item.name == \"main\" // Multi-key selector \"name=web,port=80\" → find item matching both criteria Insertion Logic Different insertion operations use sophisticated positioning:\n// insertBefore: lst = append(lst[:idx], append([]interface{}{value}, lst[idx:]...)...) // insertAfter: lst = append(lst[:idx+1], append([]interface{}{value}, lst[idx+1:]...)...) // append: lst = append(lst, value) Bounds Checking Comprehensive validation prevents index errors:\nif i \u003c 0 { i = len(list) + i // Handle negative indices } if i \u003c 0 || i \u003e len(list) { return -1, fmt.Errorf(\"index out of bounds: %d\", i) } Error Handling and Debugging Graceful Degradation The patch system follows a “warn, don’t fail” philosophy:\nMissing target resources generate warnings but don’t stop processing Invalid selectors are logged but don’t crash the operation Type inference failures fall back to string values Debug Logging Enable comprehensive debugging with KURE_DEBUG=1:\nDEBUG: Resolving TOML header [deployment.app.containers.name=main] DEBUG: Mapped to path: spec.template.spec.containers[name=main] DEBUG: Found target container at index 0 DEBUG: Applying patch: image.tag = \"v1.2.3\" DEBUG: Type inference: \"8080\" → int64(8080) (field context: containerPort) Validation Pipeline Each patch goes through multiple validation stages:\nSyntax Validation: Parse the patch line syntax Path Validation: Verify the target path exists Type Validation: Check if the value type is appropriate Schema Validation: Ensure compatibility with Kubernetes schemas (future) Performance Considerations Path Caching Frequently used paths are cached to avoid repeated parsing:\nvar pathCache = make(map[string][]PathPart) func cachedParsePath(path string) []PathPart { if cached, ok := pathCache[path]; ok { return cached } // Parse and cache... } Batch Processing Multiple patches are processed efficiently:\nGroup by Resource: Batch patches targeting the same resource Optimize Selectors: Cache list index resolutions Minimize Conversions: Reuse type-converted values Integration Points Variable Substitution Path resolution integrates with variable substitution:\n[deployment.${values.app_name}.containers.name=${values.container_name}] image.tag: \"${values.version}\" Resolution happens after variable substitution:\nSubstitute variables in headers and values Parse the resolved paths Apply type inference to final values Structure Preservation Path resolution works seamlessly with YAML structure preservation:\nParse Original Structure: Maintain comments and formatting Resolve Target Paths: Find exact locations in preserved structure Apply Minimal Changes: Update only the targeted values Preserve Everything Else: Keep original formatting intact This sophisticated system enables the patch module to handle complex Kubernetes resource modifications while maintaining type safety, performance, and user-friendly error handling.",
    "description": "Advanced Path Resolution and Type Inference This document provides an in-depth technical guide to the sophisticated path resolution and type inference systems in the Kure patch module.\nPath Resolution Architecture PathPart Structure Each patch path is decomposed into structured components:\ntype PathPart struct { Field string // The field name (e.g., \"containers\", \"ports\") MatchType string // \"\", \"index\", or \"key\" MatchValue string // The match criteria (\"0\", \"name=main\") } Path Parsing Process The ParsePatchPath() function converts dot-notation paths into structured segments:",
    "tags": [],
    "title": "Path Resolution",
    "uri": "/packages/patch/path-resolution/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Examples",
    "content": "Validation Examples This directory contains examples demonstrating Kure’s built-in validation features.\nBundle Interval Validation File: bundle-intervals.yaml\nDemonstrates proper configuration of time interval fields in Bundle resources:\nValid Examples: Recommended patterns and edge cases Invalid Examples: Common mistakes and validation errors (commented out) Error Messages: Examples of validation error output Key Validation Rules Format: Go time.Duration syntax (1s, 5m, 1h, 1h30m) Range: 1 second minimum, 24 hours maximum Fields: interval, timeout, retryInterval Best Practices Reconciliation: Use 5m to 30m for most applications Timeouts: Set 2-3x longer than expected deployment time Retry Intervals: Use 1m to 5m for faster failure recovery Production: Avoid very short intervals (\u003c1m) to reduce API load Testing Validation To test validation with these examples:\n# This will validate the YAML and show any errors kure validate examples/validation/bundle-intervals.yaml # Or test programmatically go run examples/validation/test-validation.go For more information, see the main README.md validation section.",
    "description": "Validation Examples This directory contains examples demonstrating Kure’s built-in validation features.\nBundle Interval Validation File: bundle-intervals.yaml\nDemonstrates proper configuration of time interval fields in Bundle resources:\nValid Examples: Recommended patterns and edge cases Invalid Examples: Common mistakes and validation errors (commented out) Error Messages: Examples of validation error output Key Validation Rules Format: Go time.Duration syntax (1s, 5m, 1h, 1h30m) Range: 1 second minimum, 24 hours maximum Fields: interval, timeout, retryInterval Best Practices Reconciliation: Use 5m to 30m for most applications Timeouts: Set 2-3x longer than expected deployment time Retry Intervals: Use 1m to 5m for faster failure recovery Production: Avoid very short intervals (\u003c1m) to reduce API load Testing Validation To test validation with these examples:",
    "tags": [],
    "title": "Validation",
    "uri": "/examples/validation/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Packages \u003e Launcher",
    "content": "Launcher Module - Implementation Plan Overview This document provides a detailed implementation plan for the Kurel launcher module, breaking down the work into concrete tasks with specific implementation details.\nPhase 1: Core Foundation (Week 1) Task 1.1: Create Base Types and Interfaces Files to create:\npkg/launcher/types.go - Core data structures pkg/launcher/interfaces.go - Public interfaces pkg/launcher/errors.go - Custom error types Key Considerations:\nResources will handle multi-document YAML (each document becomes separate Resource) Need adapter pattern for existing patch.PatchableAppSet integration Resource type must support conversion to/from unstructured.Unstructured Implementation:\n// types.go package launcher import ( \"sync\" \"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured\" metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" ) type KurelMetadata struct { Name string `yaml:\"name\"` Version string `yaml:\"version\"` AppVersion string `yaml:\"appVersion\"` Description string `yaml:\"description\"` Home string `yaml:\"home\"` Keywords []string `yaml:\"keywords\"` Schemas []string `yaml:\"schemas\"` // CRD schema URLs } type ParameterMap map[string]interface{} // Resource with thread-safe access and unstructured support type Resource struct { APIVersion string `yaml:\"apiVersion\"` Kind string `yaml:\"kind\"` Metadata metav1.ObjectMeta `yaml:\"metadata\"` Raw *unstructured.Unstructured // For patch system compatibility mu sync.RWMutex // Protect concurrent access } func (r *Resource) GetName() string { r.mu.RLock() defer r.mu.RUnlock() return r.Metadata.Name } func (r *Resource) ToUnstructured() (*unstructured.Unstructured, error) { r.mu.RLock() defer r.mu.RUnlock() return r.Raw.DeepCopy(), nil } type Patch struct { Name string Path string Content string // TOML content Metadata *PatchMetadata } type PatchMetadata struct { Enabled string `yaml:\"enabled\"` Description string `yaml:\"description\"` Requires []string `yaml:\"requires\"` Conflicts []string `yaml:\"conflicts\"` } type PackageDefinition struct { Path string Metadata KurelMetadata Parameters ParameterMap Resources []Resource Patches []Patch } type PackageInstance struct { Definition *PackageDefinition UserValues ParameterMap Resolved ParameterMap LocalPath string } // interfaces.go - Small, focused interfaces type DefinitionLoader interface { LoadDefinition(ctx context.Context, path string) (*PackageDefinition, error) } type Resolver interface { Resolve(ctx context.Context, base, overrides ParameterMap) (ParameterMap, error) } type Builder interface { Build(ctx context.Context, inst *PackageInstance, opts BuildOptions) error } type Validator interface { ValidateDefinition(ctx context.Context, def *PackageDefinition) ValidationResult ValidateInstance(ctx context.Context, inst *PackageInstance) ValidationResult } Tests to write:\nType marshaling/unmarshaling tests Interface compliance tests Task 1.2: Implement Package Loader Files to create:\npkg/launcher/loader.go - Package loading logic pkg/launcher/loader_test.go - Loader tests Implementation details:\nconst ( MaxPackageSize = 50 * 1024 * 1024 // 50MB hard limit WarnPackageSize = 10 * 1024 * 1024 // 10MB warning MaxResourceCount = 1000 // Max resources MaxPatchCount = 200 // Max patches ) type defaultLoader struct { fs afero.Fs // For testing with mock filesystem maxSize int64 maxResources int } func (l *defaultLoader) LoadDefinition(ctx context.Context, path string) (*PackageDefinition, error) { // 1. Check package size limits if err := l.validatePackageSize(path); err != nil { return nil, fmt.Errorf(\"package validation: %w\", err) } // 2. Validate package directory structure if err := l.validatePackageStructure(path); err != nil { return nil, err } // 3. Load parameters.yaml (critical) params, err := l.loadParameters(ctx, filepath.Join(path, \"parameters.yaml\")) if err != nil { return nil, fmt.Errorf(\"critical: %w\", err) } // 4. Extract metadata from kurel: key metadata, err := extractMetadata(params) if err != nil { return nil, fmt.Errorf(\"invalid metadata: %w\", err) } // 5. Load resources with context (best effort) resources, resourceErrs := l.loadResources(ctx, filepath.Join(path, \"resources\")) // 6. Discover and load patches (best effort) patches, patchErrs := l.loadPatches(ctx, filepath.Join(path, \"patches\")) // Check limits if len(resources) \u003e MaxResourceCount { return nil, fmt.Errorf(\"too many resources: %d (max %d)\", len(resources), MaxResourceCount) } if len(patches) \u003e MaxPatchCount { return nil, fmt.Errorf(\"too many patches: %d (max %d)\", len(patches), MaxPatchCount) } // 7. Collect non-critical errors var errs []error errs = append(errs, resourceErrs...) errs = append(errs, patchErrs...) def := \u0026PackageDefinition{ Path: path, Metadata: metadata, Parameters: params, Resources: resources, Patches: patches, } if len(errs) \u003e 0 { return def, \u0026LoadErrors{ PartialDefinition: def, Issues: errs, } } return def, nil } func (l *defaultLoader) validatePackageSize(path string) error { var totalSize int64 err := filepath.Walk(path, func(p string, info os.FileInfo, err error) error { if err != nil { return err } totalSize += info.Size() if totalSize \u003e MaxPackageSize { return fmt.Errorf(\"package exceeds %dMB limit\", MaxPackageSize/(1024*1024)) } return nil }) if err != nil { return err } if totalSize \u003e WarnPackageSize { log.Warnf(\"Package size %dMB exceeds recommended %dMB\", totalSize/(1024*1024), WarnPackageSize/(1024*1024)) } return nil } // Load patches with uniqueness enforcement func (l *defaultLoader) loadPatchesWithUniqueness(ctx context.Context, path string) ([]Patch, []error) { var patches []Patch var errors []error seen := make(map[string]string) // name -\u003e file path err := filepath.Walk(path, func(p string, info os.FileInfo, err error) error { if err != nil { errors = append(errors, err) return nil // Continue walking } if !strings.HasSuffix(p, \".kpatch\") { return nil } patch, err := l.loadSinglePatch(ctx, p) if err != nil { errors = append(errors, fmt.Errorf(\"loading %s: %w\", p, err)) return nil } // Check uniqueness if existing, exists := seen[patch.Name]; exists { return fmt.Errorf(\"duplicate patch name '%s' in %s and %s\", patch.Name, existing, p) } seen[patch.Name] = p patches = append(patches, patch) return nil }) if err != nil { errors = append(errors, err) } // Sort by numeric prefix sort.Slice(patches, func(i, j int) bool { return numericPrefixSort(patches[i].Path, patches[j].Path) }) return patches, errors } Key functions to implement:\nvalidatePackageStructure() - Check required directories exist loadParameters() - Parse parameters.yaml with validation extractMetadata() - Extract kurel: key from parameters loadResources() - Discover and parse all resource YAML files (handle multi-document) loadPatches() - Discover .kpatch files and metadata sortPatches() - Sort by numeric prefix and path splitMultiDocument() - Split multi-document YAML into separate Resources Tests:\nValid package loading Multi-document YAML handling Missing parameters.yaml handling Malformed YAML handling Patch discovery and ordering Error collection for non-critical files Phase 2: Variable Resolution (Week 1-2) Task 2.1: Implement Variable Resolver Files to create:\npkg/launcher/variables.go - Variable resolution engine pkg/launcher/variables_test.go - Resolution tests Implementation details:\ntype defaultResolver struct { maxDepth int } type ResolverOption func(*defaultResolver) func WithMaxDepth(depth int) ResolverOption { return func(r *defaultResolver) { r.maxDepth = depth } } func NewResolver(opts ...ResolverOption) Resolver { r := \u0026defaultResolver{maxDepth: 10} for _, opt := range opts { opt(r) } return r } func (r *defaultResolver) Resolve(ctx context.Context, base, overrides ParameterMap) (ParameterMap, error) { // Apply timeout if not already set if _, hasDeadline := ctx.Deadline(); !hasDeadline { var cancel context.CancelFunc ctx, cancel = context.WithTimeout(ctx, 30*time.Second) defer cancel() } // Use goroutine for cancellation support type result struct { params ParameterMap err error } done := make(chan result, 1) go func() { // 1. Deep merge parameters merged := deepMerge(base, overrides) // 2. Extract all variable references refs := extractVariableRefs(merged) // 3. Build dependency graph graph := buildDependencyGraph(refs) // 4. Detect circular dependencies if cycles := detectCycles(graph); len(cycles) \u003e 0 { done \u003c- result{nil, fmt.Errorf(\"circular dependencies: %v\", cycles)} return } // 5. Topological sort for resolution order order := topologicalSort(graph) // 6. Resolve in order resolved := make(ParameterMap) for _, key := range order { value, err := resolveValue(merged, key, resolved, 0, r.maxDepth) if err != nil { done \u003c- result{nil, fmt.Errorf(\"failed to resolve %s: %w\", key, err)} return } resolved[key] = value } done \u003c- result{resolved, nil} }() select { case res := \u003c-done: return res.params, res.err case \u003c-ctx.Done(): return nil, fmt.Errorf(\"variable resolution timeout: %w\", ctx.Err()) } } Key functions:\ndeepMerge() - Merge parameter maps preserving structure extractVariableRefs() - Find all ${…} patterns buildDependencyGraph() - Create variable dependency DAG detectCycles() - Find circular dependencies resolveValue() - Recursively resolve variable value substituteVariables() - Replace ${…} with resolved values Tests:\nSimple variable substitution Nested variable references Circular dependency detection Maximum depth enforcement Missing variable errors Complex parameter merging Phase 3: Patch System (Week 2) Task 3.1: Implement Patch Discovery and Dependencies Files to create:\npkg/launcher/patches.go - Patch processing logic pkg/launcher/patches_test.go - Patch tests Implementation details:\ntype patchProcessor struct { logger Logger } func (p *patchProcessor) DiscoverPatches(patchDir string) ([]Patch, error) { var patches []Patch // 1. Glob for *.kpatch files files, err := filepath.Glob(filepath.Join(patchDir, \"**/*.kpatch\")) if err != nil { return nil, err } // 2. Sort by numeric prefix, then alphabetically sort.Slice(files, func(i, j int) bool { return numericPrefixSort(files[i], files[j]) }) // 3. Load each patch and metadata for _, file := range files { patch, err := p.loadPatch(file) if err != nil { continue // Collect error, continue loading } patches = append(patches, patch) } return patches, nil } func (p *patchProcessor) ResolveDependencies(ctx context.Context, patches []Patch, params ParameterMap) ([]Patch, error) { // Check context select { case \u003c-ctx.Done(): return nil, ctx.Err() default: } // 1. Evaluate enabled conditions enabled := make(map[string]bool) for _, patch := range patches { if patch.Metadata != nil \u0026\u0026 patch.Metadata.Enabled != \"\" { enabled[patch.Name] = evaluateCondition(patch.Metadata.Enabled, params) } else { enabled[patch.Name] = true // Default enabled } } // 2. Build dependency graph graph := buildPatchDependencyGraph(patches) // 3. Auto-enable required patches (with verbose logging) changed := true for changed { changed = false for _, patch := range patches { if enabled[patch.Name] \u0026\u0026 patch.Metadata != nil { for _, req := range patch.Metadata.Requires { if !enabled[req] { if p.verbose { p.logger.Info(\"Auto-enabling %s (required by %s)\", req, patch.Name) } enabled[req] = true changed = true } } } } } // 4. Check for conflicts (fail fast) for _, patch := range patches { if !enabled[patch.Name] { continue } if patch.Metadata != nil { for _, conflict := range patch.Metadata.Conflicts { if enabled[conflict] { return nil, fmt.Errorf(\"conflict: %s and %s cannot both be enabled\", patch.Name, conflict) } } } } // 5. Filter and sort enabled patches var result []Patch for _, patch := range patches { if enabled[patch.Name] { result = append(result, patch) } } // Sort by numeric prefix for consistent ordering sort.Slice(result, func(i, j int) bool { return numericPrefixSort(result[i].Path, result[j].Path) }) return result, nil } Tests:\nPatch discovery with numeric ordering Dependency resolution Auto-enable behavior Conflict detection Conditional enabling Task 3.2: Integrate with Patch Engine Files to modify:\npkg/launcher/patches.go - Add apply functionality with adapter Implementation:\n// Adapter for existing patch system type patchEngineAdapter struct { verbose bool logger Logger } func (p *patchProcessor) ApplyPatches(resources []Resource, patches []Patch, resolved ParameterMap) ([]Resource, error) { adapter := \u0026patchEngineAdapter{verbose: p.verbose, logger: p.logger} // Apply package patches first, then local patches for _, patchDef := range patches { // 1. Substitute variables in patch content substituted := substituteVariables(patchDef.Content, resolved) if p.verbose { p.logger.Debug(\"Applying patch %s\", patchDef.Name) } // 2. Convert resources and apply using existing patch system matched := false for i, resource := range resources { unstructuredRes, err := resource.ToUnstructured() if err != nil { return nil, fmt.Errorf(\"failed to convert resource: %w\", err) } patched, wasPatched, err := adapter.ApplyPatch(unstructuredRes, substituted, patchDef.Name) if err != nil { // Patches MUST succeed or error - no silent failures return nil, fmt.Errorf(\"failed to apply patch %s to %s/%s: %w\", patchDef.Name, resource.Kind, resource.GetName(), err) } if wasPatched { matched = true resources[i] = FromUnstructured(patched) if p.verbose { p.logger.Debug(\"Successfully applied patch %s to %s/%s\", patchDef.Name, resource.Kind, resource.GetName()) } } } if !matched { return nil, fmt.Errorf(\"patch %s matched no resources\", patchDef.Name) } } return resources, nil } // Adapter method to work with existing patch.PatchableAppSet func (a *patchEngineAdapter) ApplyPatch(resource *unstructured.Unstructured, patchContent string, patchName string) (*unstructured.Unstructured, bool, error) { // Implementation to bridge to existing patch system // Will need to study patch.PatchableAppSet usage return resource, false, nil } Phase 4: Validation and Schema (Week 2-3) Task 4.1: Implement Schema Generator Files to create:\npkg/launcher/schema.go - Schema generation logic pkg/launcher/schema_test.go - Schema tests pkg/launcher/schemas/ - Bundled K8s schemas Implementation:\ntype defaultSchemaGenerator struct { bundledSchemas map[string]Schema tracer *pathTracer } func (g *defaultSchemaGenerator) GenerateSchema(def *PackageDefinition) (*Schema, error) { schema := \u0026Schema{ Type: \"object\", Properties: make(map[string]SchemaProperty), } // Phase 1: Type inference from parameter values for key, value := range def.Parameters { schema.Properties[key] = inferType(value) } // Phase 2: Trace patch paths to K8s fields for _, patch := range def.Patches { traces := g.tracer.TracePaths(patch.Content) for _, trace := range traces { if k8sSchema := g.getK8sSchema(trace.ResourceType, trace.Field); k8sSchema != nil { // Enhance parameter schema with K8s constraints enhanceSchema(schema, trace.Variable, k8sSchema) } } } // Phase 3: Load custom CRD schemas from URLs if len(def.Metadata.Schemas) \u003e 0 { for _, url := range def.Metadata.Schemas { crdSchema, err := fetchSchema(url) if err != nil { continue // Log warning, continue } g.bundledSchemas[crdSchema.Kind] = crdSchema } } return schema, nil } Bundled schemas to include:\nCore K8s resources (Deployment, Service, ConfigMap, etc.) Resources from internal/ packages (FluxCD, cert-manager, MetalLB, etc.) Task 4.2: Implement Validator Files to create:\npkg/launcher/validator.go - Validation logic pkg/launcher/validator_test.go - Validation tests Implementation:\ntype defaultValidator struct { schemaGen SchemaGenerator logger Logger } func (v *defaultValidator) ValidateDefinition(ctx context.Context, def *PackageDefinition) ValidationResult { result := ValidationResult{} // 1. Validate package structure if err := validateStructure(def); err != nil { result.Errors = append(result.Errors, err) } // 2. Check patch variable references for _, patch := range def.Patches { refs := extractVariableRefs(patch.Content) for _, ref := range refs { if !parameterExists(ref, def.Parameters) { result.Errors = append(result.Errors, fmt.Errorf(\"patch %s: variable %s not defined\", patch.Name, ref)) } } } // 3. Validate patch dependencies exist patchNames := make(map[string]bool) for _, patch := range def.Patches { patchNames[patch.Name] = true } for _, patch := range def.Patches { if patch.Metadata != nil { for _, req := range patch.Metadata.Requires { if !patchNames[req] { result.Errors = append(result.Errors, fmt.Errorf(\"patch %s requires non-existent patch %s\", patch.Name, req)) } } } } return result } func (v *defaultValidator) ValidateInstance(ctx context.Context, inst *PackageInstance) ValidationResult { result := ValidationResult{} // 1. Validate parameters against schema schema, err := v.schemaGen.GenerateSchema(ctx, inst.Definition) if err != nil { result.Warnings = append(result.Warnings, fmt.Sprintf(\"Could not generate schema: %v\", err)) } else { if errs := validateAgainstSchema(inst.Resolved, schema); len(errs) \u003e 0 { result.Errors = append(result.Errors, errs...) } } // 2. Validate all variables resolve for key, value := range inst.Resolved { if strings.Contains(fmt.Sprint(value), \"${\") { result.Errors = append(result.Errors, fmt.Errorf(\"unresolved variable in %s: %v\", key, value)) } } // 3. Validate K8s resources concurrently for performance errors := v.validateResourcesConcurrent(ctx, inst.Definition.Resources) result.Errors = append(result.Errors, errors...) return result } func (v *defaultValidator) validateResourcesConcurrent(ctx context.Context, resources []Resource) []error { // Use worker pool for concurrent validation numWorkers := runtime.NumCPU() if len(resources) \u003c numWorkers { numWorkers = len(resources) } work := make(chan int, len(resources)) results := make(chan error, len(resources)) // Start workers var wg sync.WaitGroup for i := 0; i \u003c numWorkers; i++ { wg.Add(1) go func() { defer wg.Done() for idx := range work { select { case \u003c-ctx.Done(): results \u003c- ctx.Err() return default: if err := v.validateResource(resources[idx]); err != nil { results \u003c- fmt.Errorf(\"resource %s: %w\", resources[idx].GetName(), err) } } } }() } // Queue work for i := range resources { work \u003c- i } close(work) // Collect results go func() { wg.Wait() close(results) }() var errors []error for err := range results { errors = append(errors, err) } return errors } func (v *defaultValidator) validateResource(resource Resource) error { // Try full validation with schema if schema := getK8sSchema(resource.APIVersion, resource.Kind); schema != nil { return validateWithSchema(resource, schema) } // Fallback to medium validation return validateMedium(resource) } Phase 5: Output and Extensions (Week 3) Task 5.1: Implement Output Builder Files to create:\npkg/launcher/builder.go - Output generation logic pkg/launcher/builder_test.go - Builder tests Implementation:\ntype OutputFormat string const ( OutputFormatSingle OutputFormat = \"single\" OutputFormatByKind OutputFormat = \"by-kind\" OutputFormatByResource OutputFormat = \"by-resource\" ) type OutputType string const ( OutputTypeYAML OutputType = \"yaml\" OutputTypeJSON OutputType = \"json\" ) type defaultBuilder struct { writer FileWriter } const ( DefaultBuildTimeout = 30 * time.Second // Similar to Helm MaxBuildTimeout = 5 * time.Minute ) func (b *defaultBuilder) Build(ctx context.Context, inst *PackageInstance, opts BuildOptions) error { // Apply timeout if not set if _, hasDeadline := ctx.Deadline(); !hasDeadline { var cancel context.CancelFunc ctx, cancel = context.WithTimeout(ctx, DefaultBuildTimeout) defer cancel() } // Progress indicator var progress ProgressReporter if opts.ShowProgress \u0026\u0026 !opts.Quiet { progress = NewProgressBar(\"Building package...\") defer progress.Finish() } // 1. Apply patches to resources (with transactional behavior) processor := newPatchProcessor(WithVerbose(opts.Verbose)) if progress != nil { progress.Update(\"Applying patches...\") } resources, err := processor.ApplyPatches(ctx, inst.Definition.Resources, inst.Definition.Patches, inst.Resolved, ) if err != nil { return fmt.Errorf(\"patch application failed: %w\", err) } // 2. Sort resources by phase annotations if progress != nil { progress.Update(\"Organizing resources...\") } phased := organizeByPhase(resources) // 3. Output based on options if progress != nil { progress.Update(\"Writing output...\") } switch opts.OutputPath { case \"\", \"-\": // Write to stdout (dry-run mode) return b.writeToStdout(resources, opts.OutputType) default: // Write to files return b.writeToFiles(resources, opts.OutputPath, opts.OutputFormat, opts.OutputType) } } func (b *defaultBuilder) writeToFiles(resources []Resource, path string, format OutputFormat, outputType OutputType) error { switch format { case OutputFormatSingle: return b.writeSingleFile(resources, path, outputType) case OutputFormatByKind: return b.writeByKind(resources, path, outputType) case OutputFormatByResource: return b.writeByResource(resources, path, outputType) default: return fmt.Errorf(\"unknown output format: %s\", format) } } Task 5.2: Implement Local Extensions Files to create:\npkg/launcher/extensions.go - Extension handling pkg/launcher/extensions_test.go - Extension tests Implementation:\ntype extensionLoader struct { loader Loader } func (e *extensionLoader) LoadWithExtensions(def *PackageDefinition, localPath string) (*PackageDefinition, error) { // 1. Check if local extension exists if localPath == \"\" { localPath = def.Path + \".local.kurel\" } if !exists(localPath) { return def, nil } // 2. Load local parameters localParams, err := e.loadLocalParameters(localPath) if err != nil { return nil, fmt.Errorf(\"failed to load local parameters: %w\", err) } // 3. Validate parameter compatibility if err := validateParameterCompatibility(def.Parameters, localParams); err != nil { return nil, fmt.Errorf(\"incompatible local parameters: %w\", err) } // 4. Load local patches localPatches, err := e.loadLocalPatches(localPath) if err != nil { return nil, fmt.Errorf(\"failed to load local patches: %w\", err) } // 5. Merge with package extended := \u0026PackageDefinition{ Path: def.Path, Metadata: def.Metadata, Parameters: deepMerge(def.Parameters, localParams), Resources: def.Resources, // Cannot modify resources Patches: append(def.Patches, localPatches...), } return extended, nil } func validateParameterCompatibility(base, override ParameterMap) error { for key, overrideValue := range override { if baseValue, exists := base[key]; exists { if !compatibleTypes(baseValue, overrideValue) { return fmt.Errorf(\"parameter %s: incompatible type change\", key) } } } return nil } Phase 6: CLI Integration (Week 3-4) Task 6.1: Implement CLI Commands Files to modify:\npkg/cmd/kurel/build.go - Build command pkg/cmd/kurel/validate.go - Validate command pkg/cmd/kurel/info.go - Info command pkg/cmd/kurel/schema.go - Schema command Build command implementation:\nfunc newBuildCommand(globalOpts *options.GlobalOptions) *cobra.Command { var ( valuesFile string outputPath string outputFormat string outputType string localPath string verbose bool showPatches bool ) cmd := \u0026cobra.Command{ Use: \"build \u003cpackage\u003e\", Short: \"Build Kubernetes manifests from kurel package\", Args: cobra.ExactArgs(1), RunE: func(cmd *cobra.Command, args []string) error { // Setup logging logger := setupLogger(verbose) // Setup context with timeout ctx, cancel := context.WithTimeout(context.Background(), timeout) defer cancel() // Progress indication var progress launcher.ProgressReporter if showProgress \u0026\u0026 !quiet { progress = launcher.NewProgressBar(\"Loading package...\") defer progress.Finish() } // 1. Load package definition logger.Info(\"Loading package from %s\", args[0]) loader := launcher.NewLoader() def, err := loader.LoadDefinition(ctx, args[0]) if err != nil { if loadErrs, ok := err.(*launcher.LoadErrors); ok { for _, e := range loadErrs.Issues { logger.Warn(\"Load warning: %v\", e) } def = loadErrs.PartialDefinition } else { return fmt.Errorf(\"failed to load package: %w\", err) } } // 2. Load local extensions if localPath != \"\" || exists(args[0]+\".local.kurel\") { logger.Info(\"Loading local extensions\") def, err = launcher.LoadWithExtensions(def, localPath) if err != nil { return fmt.Errorf(\"failed to load extensions: %w\", err) } } // 3. Load user values userValues := make(launcher.ParameterMap) if valuesFile != \"\" { logger.Info(\"Loading values from %s\", valuesFile) userValues, err = loadValuesFile(valuesFile) if err != nil { return fmt.Errorf(\"failed to load values: %w\", err) } } // 4. Create instance instance := \u0026launcher.PackageInstance{ Definition: def, UserValues: userValues, LocalPath: localPath, } // 5. Resolve variables with timeout if progress != nil { progress.Update(\"Resolving variables...\") } logger.Info(\"Resolving variables\") resolver := launcher.NewResolver() instance.Resolved, err = resolver.Resolve(ctx, def.Parameters, userValues) if err != nil { return fmt.Errorf(\"failed to resolve variables: %w\", err) } // 6. Process patches if progress != nil { progress.Update(\"Processing patches...\") } logger.Info(\"Processing patches\") processor := launcher.NewPatchProcessor( launcher.WithLogger(logger), launcher.WithVerbose(verbose), ) enabledPatches, err := processor.ResolveDependencies(ctx, def.Patches, instance.Resolved) if err != nil { return fmt.Errorf(\"failed to resolve patch dependencies: %w\", err) } if showPatches { fmt.Fprintf(os.Stderr, \"Enabled patches:\\n\") for _, p := range enabledPatches { fmt.Fprintf(os.Stderr, \" - %s\\n\", p.Name) } } // 7. Validate with concurrent processing if progress != nil { progress.Update(\"Validating configuration...\") } logger.Info(\"Validating configuration\") validator := launcher.NewValidator() result := validator.ValidateInstance(ctx, instance) if result.HasErrors() { for _, err := range result.Errors { logger.Error(\"Validation error: %v\", err) } return fmt.Errorf(\"validation failed with %d errors\", len(result.Errors)) } for _, warn := range result.Warnings { logger.Warn(\"Validation warning: %v\", warn) } // 8. Build output if progress != nil { progress.Update(\"Building manifests...\") } logger.Info(\"Building manifests\") builder := launcher.NewBuilder() opts := launcher.BuildOptions{ OutputPath: outputPath, OutputFormat: launcher.OutputFormat(outputFormat), OutputType: launcher.OutputType(outputType), Verbose: verbose, ShowProgress: showProgress, Quiet: quiet, } if outputPath == \"\" { opts.OutputPath = \"-\" // stdout (dry-run) } if err := builder.Build(ctx, instance, opts); err != nil { return fmt.Errorf(\"failed to build manifests: %w\", err) } logger.Info(\"Build complete\") return nil }, } cmd.Flags().StringVarP(\u0026valuesFile, \"values\", \"f\", \"\", \"Values file for overrides\") cmd.Flags().StringVarP(\u0026outputPath, \"output\", \"o\", \"\", \"Output path (default: stdout for dry-run)\") cmd.Flags().StringVar(\u0026outputFormat, \"format\", \"single\", \"Output format: single|by-kind|by-resource\") cmd.Flags().StringVar(\u0026outputType, \"output-format\", \"yaml\", \"Output type: yaml|json\") cmd.Flags().StringVar(\u0026localPath, \"local\", \"\", \"Path to .local.kurel directory\") cmd.Flags().DurationVar(\u0026timeout, \"timeout\", DefaultBuildTimeout, \"Build timeout\") cmd.Flags().BoolVarP(\u0026verbose, \"verbose\", \"v\", false, \"Verbose output with patch debugging\") cmd.Flags().BoolVar(\u0026showPatches, \"show-patches\", false, \"Show enabled patches\") cmd.Flags().BoolVar(\u0026showProgress, \"progress\", true, \"Show progress bar\") cmd.Flags().BoolVar(\u0026quiet, \"quiet\", false, \"Suppress non-essential output\") cmd.Flags().BoolVar(\u0026dryRun, \"dry-run\", false, \"Print to stdout without writing files\") // Mark file/directory flags cmd.MarkFlagFilename(\"values\", \"yaml\", \"yml\") cmd.MarkFlagDirname(\"output\") return cmd } Phase 7: Testing (Week 4) Task 7.1: Create Test Fixtures Files to create:\npkg/launcher/testdata/packages/simple/ - Basic package pkg/launcher/testdata/packages/complex/ - Complex package with patches pkg/launcher/testdata/packages/invalid/ - Invalid packages for error testing Task 7.2: Write Comprehensive Tests Test coverage targets:\nPackage loading: 90% Variable resolution: 95% Patch processing: 90% Validation: 85% Schema generation: 80% Output generation: 90% Key test scenarios with Go patterns:\nLoader tests (table-driven): func TestLoader(t *testing.T) { tests := []struct { name string setup func(afero.Fs) wantErr string }{ { name: \"valid_package\", setup: setupValidPackage, }, { name: \"missing_parameters\", setup: setupMissingParams, wantErr: \"critical: parameters.yaml\", }, { name: \"package_too_large\", setup: setupLargePackage, wantErr: \"exceeds 50MB limit\", }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { fs := afero.NewMemMapFs() tt.setup(fs) loader := NewLoader(WithFilesystem(fs)) ctx := context.Background() _, err := loader.LoadDefinition(ctx, \"/test\") if tt.wantErr != \"\" { require.Error(t, err) assert.Contains(t, err.Error(), tt.wantErr) } else { require.NoError(t, err) } }) } } Variable tests:\nSimple substitution Nested references Circular dependencies Missing variables Deep merging Patch tests:\nDependency resolution Auto-enabling Conflict detection Variable substitution in patches Patch application failures (must error, not silent) Package patch → local patch override behavior Verbose mode debugging output Validation tests:\nSchema validation Resource validation Parameter compatibility K8s resource constraints Integration tests with benchmarks:\nfunc TestIntegrationBuild(t *testing.T) { ctx := context.Background() pkg := loadTestPackage(t, \"testdata/packages/complex\") instance := \u0026PackageInstance{ Definition: pkg, UserValues: ParameterMap{\"env\": \"prod\"}, } // Test full pipeline resolver := NewResolver() instance.Resolved, _ = resolver.Resolve(ctx, pkg.Parameters, instance.UserValues) validator := NewValidator() result := validator.ValidateInstance(ctx, instance) require.Empty(t, result.Errors) builder := NewBuilder() err := builder.Build(ctx, instance, BuildOptions{}) require.NoError(t, err) } func BenchmarkBuildPerformance(b *testing.B) { targets := []struct { name string resources int target time.Duration }{ {\"small\", 10, 100 * time.Millisecond}, {\"medium\", 50, 500 * time.Millisecond}, {\"large\", 200, 2 * time.Second}, } for _, tgt := range targets { b.Run(tgt.name, func(b *testing.B) { pkg := generatePackage(tgt.resources) ctx := context.Background() b.ResetTimer() for i := 0; i \u003c b.N; i++ { start := time.Now() err := Build(ctx, pkg, BuildOptions{}) duration := time.Since(start) require.NoError(b, err) if duration \u003e tgt.target { b.Fatalf(\"exceeded target: %v \u003e %v\", duration, tgt.target) } } }) } } Implementation Timeline Week 1 Core types and interfaces Package loader Variable resolver (start) Week 2 Variable resolver (complete) Patch discovery and dependencies Patch application Week 3 Schema generation Validation system Output builder Local extensions Week 4 CLI integration Comprehensive testing Documentation Performance optimization Testing Strategy Unit Tests Each module tested in isolation Mock dependencies using interfaces Table-driven tests for complex logic Use testify/assert for assertions Integration Tests Full package processing flows Real filesystem operations End-to-end CLI testing Performance benchmarks Test Data Create realistic test packages Include edge cases and error conditions Test with actual Kubernetes resources Validate against real K8s schemas Performance Targets Package loading: \u003c 100ms for typical package Variable resolution: \u003c 50ms for 100 variables Patch application: \u003c 200ms for 50 patches Schema generation: \u003c 500ms (with caching) Full build: \u003c 1s for typical package Key Implementation Notes Security Considerations No direct secret creation - only references (e.g., external-secrets) Path traversal protection in package loading URL validation for schema URLs Sensitive parameter handling - avoid logging sensitive values Patch Application Rules Patches MUST succeed or return error (no silent failures) Application order: Package patches first, then local patches Local patches can override package patches Verbose mode provides detailed debugging information Resource Handling Multi-document YAML files are supported Each document becomes a separate Resource object Resources need conversion to/from unstructured.Unstructured for patch engine Output Modes Default stdout output serves as dry-run mode File output requires explicit -o flag Verbose mode shows patch application details Documentation Requirements Code Documentation GoDoc comments for all public types and functions Example usage in documentation Clear error messages with context User Documentation Update README.md with launcher usage Create examples/ directory with sample packages Document all CLI commands and flags Provide troubleshooting guide Success Criteria Functionality: All features from design implemented Quality: \u003e85% test coverage, no critical bugs Performance: Meets performance targets Usability: Clear CLI interface with helpful output Documentation: Complete API and user documentation Integration: Seamless integration with existing Kure codebase",
    "description": "Launcher Module - Implementation Plan Overview This document provides a detailed implementation plan for the Kurel launcher module, breaking down the work into concrete tasks with specific implementation details.\nPhase 1: Core Foundation (Week 1) Task 1.1: Create Base Types and Interfaces Files to create:\npkg/launcher/types.go - Core data structures pkg/launcher/interfaces.go - Public interfaces pkg/launcher/errors.go - Custom error types Key Considerations:\nResources will handle multi-document YAML (each document becomes separate Resource) Need adapter pattern for existing patch.PatchableAppSet integration Resource type must support conversion to/from unstructured.Unstructured Implementation:",
    "tags": [],
    "title": "Implementation Plan",
    "uri": "/packages/launcher/code-implementation-plan/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Packages",
    "content": "IO Package The io package provides utilities for reading, writing, and parsing YAML representations of Kubernetes resources. It acts as a thin wrapper around sigs.k8s.io/yaml and the Kubernetes runtime scheme from client-go.\nYAML Helpers Basic marshalling and unmarshalling is performed through Marshal and Unmarshal functions which operate on the standard io.Reader and io.Writer interfaces. For persisting data to disk, SaveFile and LoadFile helpers wrap file creation and reading.\nFor in-memory operations the Buffer type implements both io.Reader and io.Writer and exposes Marshal and Unmarshal methods.\nParsing Runtime Objects Kubernetes manifests frequently contain multiple YAML documents separated by ---. ParseFile reads such a manifest and decodes each document into a runtime.Object using the client-go scheme. Several additional API schemes from projects like FluxCD, cert-manager and MetalLB are registered so their custom resources can be parsed without further setup.\nResource Printing The package includes comprehensive resource printing capabilities compatible with kubectl output formats:\nResourcePrinter — unified formatting for YAML, JSON, table, wide, and name output modes SimpleTablePrinter — kubectl-style table output without external dependencies Convenience functions — PrintObjectsAsYAML, PrintObjectsAsJSON, PrintObjectsAsTable API Reference pkg.go.dev/github.com/go-kure/kure/pkg/io",
    "description": "IO Package The io package provides utilities for reading, writing, and parsing YAML representations of Kubernetes resources. It acts as a thin wrapper around sigs.k8s.io/yaml and the Kubernetes runtime scheme from client-go.\nYAML Helpers Basic marshalling and unmarshalling is performed through Marshal and Unmarshal functions which operate on the standard io.Reader and io.Writer interfaces. For persisting data to disk, SaveFile and LoadFile helpers wrap file creation and reading.\nFor in-memory operations the Buffer type implements both io.Reader and io.Writer and exposes Marshal and Unmarshal methods.",
    "tags": [],
    "title": "IO",
    "uri": "/packages/io/index.html"
  },
  {
    "breadcrumb": "Go Kure",
    "content": "Reference API Documentation Full Go API documentation is available on pkg.go.dev:\ngithub.com/go-kure/kure — Root module pkg/launcher — Launcher package system pkg/patch — JSONPath patching pkg/stack — Stack data model pkg/stack/layout — Directory layout generation pkg/stack/generators — Resource generators pkg/io — YAML I/O and resource printing pkg/errors — Structured error types pkg/cli — CLI utilities pkg/kubernetes/fluxcd — FluxCD resource builders Additional References Compatibility Matrix - Supported tool and dependency versions",
    "description": "Reference API Documentation Full Go API documentation is available on pkg.go.dev:\ngithub.com/go-kure/kure — Root module pkg/launcher — Launcher package system pkg/patch — JSONPath patching pkg/stack — Stack data model pkg/stack/layout — Directory layout generation pkg/stack/generators — Resource generators pkg/io — YAML I/O and resource printing pkg/errors — Structured error types pkg/cli — CLI utilities pkg/kubernetes/fluxcd — FluxCD resource builders Additional References Compatibility Matrix - Supported tool and dependency versions",
    "tags": [],
    "title": "Reference",
    "uri": "/reference/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Packages \u003e Launcher",
    "content": "Launcher Module Architecture Overview The launcher module is a declarative Kubernetes manifest generation system that processes Kurel packages through a well-defined pipeline. It follows clean architecture principles with clear separation of concerns and interface-driven design.\nCore Design Principles Interface-Driven Design: All major components are defined as interfaces, enabling testability and flexibility Immutability: Core data structures use deep copy patterns to ensure thread safety Hybrid Error Handling: Distinguishes between blocking errors and non-blocking warnings Performance Optimization: Uses caching, memoization, and efficient algorithms Thread Safety: All components are safe for concurrent use Architecture Layers ┌─────────────────────────────────────────┐ │ CLI/API Layer │ ├─────────────────────────────────────────┤ │ Launcher Pipeline │ │ ┌────────────────────────────────┐ │ │ │ Load → Resolve → Patch → Valid │ │ │ └────────────────────────────────┘ │ ├─────────────────────────────────────────┤ │ Core Components │ │ ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐ │ │ │Loader│ │Resolv│ │Patch │ │Valid │ │ │ └──────┘ └──────┘ └──────┘ └──────┘ │ ├─────────────────────────────────────────┤ │ Data Structures │ │ PackageDefinition, Resource, Patch │ ├─────────────────────────────────────────┤ │ Foundation Layer │ │ errors, logger, io utilities │ └─────────────────────────────────────────┘ Component Architecture 1. Package Loader Responsibility: Load and parse Kurel packages from disk\ntype PackageLoader interface { LoadDefinition(ctx context.Context, path string, opts *LauncherOptions) (*PackageDefinition, error) LoadInstance(ctx context.Context, def *PackageDefinition, userValues ParameterMap) (*PackageInstance, error) } Key Features:\nYAML parsing with strict validation Resource discovery and loading Patch file detection Parameter merging with precedence 2. Variable Resolver Responsibility: Resolve variable substitutions in parameters\ntype Resolver interface { ResolveVariables(ctx context.Context, instance *PackageInstance) (*ResolvedPackage, error) ResolveValue(ctx context.Context, value interface{}, params ParameterMap) (interface{}, error) } Key Features:\nRecursive variable substitution Cycle detection using DFS Memoization for performance Configurable depth limits 3. Patch Processor Responsibility: Apply patches to resources with dependency resolution\ntype PatchProcessor interface { ProcessPatches(ctx context.Context, def *PackageDefinition) (*PackageDefinition, error) ApplyPatch(ctx context.Context, resource Resource, patch Patch) (Resource, error) } Key Features:\nTopological sort for dependencies Conflict detection JSONPath-based patching Conditional patch application 4. Schema Generator Responsibility: Generate JSON schemas for validation\ntype SchemaGenerator interface { GeneratePackageSchema(ctx context.Context) (*JSONSchema, error) GenerateResourceSchema(ctx context.Context, gvk schema.GroupVersionKind) (*JSONSchema, error) } Key Features:\nAutomatic schema generation Type inference from values Field usage tracing Schema caching 5. Validator Responsibility: Validate packages against schemas and business rules\ntype Validator interface { ValidatePackage(ctx context.Context, def *PackageDefinition) (*ValidationResult, error) ValidateResource(ctx context.Context, resource Resource) (*ValidationResult, error) } Key Features:\nSchema-based validation Semantic validation Error/warning distinction Strict mode support Data Flow graph LR A[YAML Files] --\u003e B[Loader] B --\u003e C[PackageDefinition] C --\u003e D[Resolver] D --\u003e E[ResolvedPackage] E --\u003e F[PatchProcessor] F --\u003e G[PatchedResources] G --\u003e H[Validator] H --\u003e I[ValidatedPackage] I --\u003e J[OutputBuilder] J --\u003e K[Manifests] Error Handling Strategy The module uses a sophisticated error handling approach:\nError Types: All errors use github.com/go-kure/kure/pkg/errors for stack traces Error Categories: Blocking Errors: Stop processing immediately Warnings: Collected and reported but don’t block Validation Levels: Error: Must be fixed before proceeding Warning: Should be addressed but not critical Info: Informational messages Performance Optimizations Current Optimizations Schema Caching: Schemas are cached to avoid regeneration Variable Memoization: Resolved variables are cached Efficient Algorithms: O(n) cycle detection, topological sort Lazy Evaluation: Resources loaded on-demand Performance Targets Full package build: \u003c 1 second Package validation: \u003c 100ms Variable resolution: \u003c 50ms Patch application: \u003c 200ms Thread Safety All components ensure thread safety through:\nImmutability: Core types use deep copy Mutexes: sync.RWMutex for shared state Stateless Operations: Most operations are stateless Concurrent Processing: Safe for parallel execution Testing Strategy Coverage Goals Target: 80%+ coverage Current: 74.1% coverage Test Categories Unit Tests: Component isolation Integration Tests: Pipeline validation Table-Driven Tests: Comprehensive scenarios Benchmarks: Performance validation Security Considerations No Secret Storage: Secrets referenced, never stored Path Traversal Protection: Validated file paths Input Validation: All inputs sanitized Resource Limits: Memory and depth limits Future Enhancements Short Term Increase test coverage to 80%+ Add context cancellation checks Implement custom error types Add performance benchmarks Medium Term Plugin architecture for validators Metrics and observability Connection pooling Advanced caching strategies Long Term Distributed processing Real-time validation AI-assisted error resolution GitOps integration Code Quality Metrics Metric Current Target Status Test Coverage 74.1% 80% 🟡 Cyclomatic Complexity Low-Med Low ✅ Code/Test Ratio 1.56 \u003c 2.0 ✅ Documentation Good Excellent 🟡 Performance Good Excellent 🟡 Dependencies Core Dependencies k8s.io/apimachinery: Kubernetes API machinery sigs.k8s.io/controller-runtime: Controller utilities gopkg.in/yaml.v3: YAML processing github.com/go-kure/kure/pkg/errors: Error handling github.com/go-kure/kure/pkg/logger: Logging Version Management Go 1.21+ required Kubernetes API v0.28+ Strict dependency versioning Best Practices Always use kure/errors package for error creation Check context cancellation in long operations Use defer for cleanup operations Document all public APIs with examples Maintain backward compatibility in public interfaces Write table-driven tests for comprehensive coverage Use interfaces for testability and flexibility",
    "description": "Launcher Module Architecture Overview The launcher module is a declarative Kubernetes manifest generation system that processes Kurel packages through a well-defined pipeline. It follows clean architecture principles with clear separation of concerns and interface-driven design.\nCore Design Principles Interface-Driven Design: All major components are defined as interfaces, enabling testability and flexibility Immutability: Core data structures use deep copy patterns to ensure thread safety Hybrid Error Handling: Distinguishes between blocking errors and non-blocking warnings Performance Optimization: Uses caching, memoization, and efficient algorithms Thread Safety: All components are safe for concurrent use Architecture Layers ┌─────────────────────────────────────────┐ │ CLI/API Layer │ ├─────────────────────────────────────────┤ │ Launcher Pipeline │ │ ┌────────────────────────────────┐ │ │ │ Load → Resolve → Patch → Valid │ │ │ └────────────────────────────────┘ │ ├─────────────────────────────────────────┤ │ Core Components │ │ ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐ │ │ │Loader│ │Resolv│ │Patch │ │Valid │ │ │ └──────┘ └──────┘ └──────┘ └──────┘ │ ├─────────────────────────────────────────┤ │ Data Structures │ │ PackageDefinition, Resource, Patch │ ├─────────────────────────────────────────┤ │ Foundation Layer │ │ errors, logger, io utilities │ └─────────────────────────────────────────┘ Component Architecture 1. Package Loader Responsibility: Load and parse Kurel packages from disk",
    "tags": [],
    "title": "Architecture",
    "uri": "/packages/launcher/architecture/index.html"
  },
  {
    "breadcrumb": "Go Kure",
    "content": "Development Resources for contributing to and developing Kure.\nDevelopment Guide - Setting up a development environment, running tests, and contributing GitHub Workflows - CI/CD pipeline documentation",
    "description": "Development Resources for contributing to and developing Kure.\nDevelopment Guide - Setting up a development environment, running tests, and contributing GitHub Workflows - CI/CD pipeline documentation",
    "tags": [],
    "title": "Development",
    "uri": "/development/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Packages",
    "content": "Errors Package The errors package provides structured error types and handling utilities for the Kure library and kurel tool.\nError Types The package provides several specialized error constructors:\nResourceValidationError — Validation failures for Kubernetes resources, including kind, name, and field information FileError — File operation failures (read, write, parse) with operation type and path ValidationError — General validation failures with field details ConfigurationError — Configuration-related errors Predefined Errors Common validation errors are predefined for efficiency and consistency:\nErrNilDeployment, ErrNilPod, ErrNilService, ErrNilConfigMap ErrGVKNotFound, ErrGVKNotAllowed Error Wrapping The package provides Wrap and Wrapf functions compatible with Go’s standard errors.Is and errors.As for error unwrapping.\nAPI Reference pkg.go.dev/github.com/go-kure/kure/pkg/errors",
    "description": "Errors Package The errors package provides structured error types and handling utilities for the Kure library and kurel tool.\nError Types The package provides several specialized error constructors:\nResourceValidationError — Validation failures for Kubernetes resources, including kind, name, and field information FileError — File operation failures (read, write, parse) with operation type and path ValidationError — General validation failures with field details ConfigurationError — Configuration-related errors Predefined Errors Common validation errors are predefined for efficiency and consistency:",
    "tags": [],
    "title": "Errors",
    "uri": "/packages/errors/index.html"
  },
  {
    "breadcrumb": "Go Kure",
    "content": "Changelog Track changes across Kure releases.\nReleases - Version history and release notes",
    "description": "Changelog Track changes across Kure releases.\nReleases - Version history and release notes",
    "tags": [],
    "title": "Changelog",
    "uri": "/changelog/index.html"
  },
  {
    "breadcrumb": "Go Kure \u003e Packages",
    "content": "CLI Package The cli package provides shared utilities and abstractions for building command-line interfaces in the Kure and kurel tools.\nComponents Factory — Dependency injection container for CLI commands, making them easier to test and configure IOStreams — Abstraction for stdin/stdout/stderr, enabling testable commands Printer — Output formatting supporting YAML, JSON, table, wide, and name formats (compatible with kubectl conventions) Config — Configuration file handling with merging from defaults, config files, environment variables, and flags API Reference pkg.go.dev/github.com/go-kure/kure/pkg/cli",
    "description": "CLI Package The cli package provides shared utilities and abstractions for building command-line interfaces in the Kure and kurel tools.\nComponents Factory — Dependency injection container for CLI commands, making them easier to test and configure IOStreams — Abstraction for stdin/stdout/stderr, enabling testable commands Printer — Output formatting supporting YAML, JSON, table, wide, and name formats (compatible with kubectl conventions) Config — Configuration file handling with merging from defaults, config files, environment variables, and flags API Reference pkg.go.dev/github.com/go-kure/kure/pkg/cli",
    "tags": [],
    "title": "CLI",
    "uri": "/packages/cli/index.html"
  },
  {
    "breadcrumb": "Go Kure",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "",
    "content": "Kure: Type-Safe Kubernetes Resource Generation Warning Work in Progress: Kure is currently under active development and has not been released yet. APIs and features are subject to change. Kure is a powerful Go library for programmatically building Kubernetes resources, designed specifically for GitOps workflows. Say goodbye to complex templating engines and hello to strongly-typed, composable resource generation.\nWhy Kure? Building Kubernetes manifests for GitOps can be challenging:\nYAML templating is error-prone and hard to maintain Helm charts add complexity with their templating language Raw manifests lead to duplication and inconsistency Kure solves these problems by providing:\nType-safe builders that catch errors at compile time Composable patterns for reusable resource generation Native Go code instead of template syntax GitOps-ready output for Flux and ArgoCD Features Comprehensive Resource Support\nCore Kubernetes resources (Deployments, Services, ConfigMaps, etc.) FluxCD resources (Kustomizations, HelmReleases, Sources) cert-manager integration External Secrets Operator MetalLB configuration Hierarchical Organization\nCluster → Node → Bundle → Application structure Logical grouping of related resources Clean directory layout generation Advanced Capabilities\nJSONPath-based patching system Multi-environment support OCI registry integration Validation and error handling Quick Start import ( \"os\" \"github.com/go-kure/kure/pkg/kubernetes/fluxcd\" \"github.com/go-kure/kure/pkg/io\" kustv1 \"github.com/fluxcd/kustomize-controller/api/v1\" ) // Create a Flux Kustomization ks := fluxcd.Kustomization(\u0026fluxcd.KustomizationConfig{ Name: \"my-app\", Namespace: \"default\", Path: \"./manifests\", Interval: \"5m\", SourceRef: kustv1.CrossNamespaceSourceReference{ Kind: \"GitRepository\", Name: \"my-repo\", }, }) // Output as YAML io.Marshal(os.Stdout, ks) Learn More Overview - Introduction and design philosophy Getting Started - Installation and quickstart guide Architecture - Deep dive into Kure’s design Packages - Core library packages Examples - See Kure in action Get Involved Kure is open source and welcomes contributions!\nGitHub Repository Issue Tracker Discussions",
    "description": "Kure: Type-Safe Kubernetes Resource Generation Warning Work in Progress: Kure is currently under active development and has not been released yet. APIs and features are subject to change. Kure is a powerful Go library for programmatically building Kubernetes resources, designed specifically for GitOps workflows. Say goodbye to complex templating engines and hello to strongly-typed, composable resource generation.\nWhy Kure? Building Kubernetes manifests for GitOps can be challenging:",
    "tags": [],
    "title": "Go Kure",
    "uri": "/index.html"
  },
  {
    "breadcrumb": "Go Kure",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/tags/index.html"
  }
]
